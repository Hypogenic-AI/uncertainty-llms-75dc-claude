{"title": "Towards Mitigating Hallucination in Large Language Models via Self-Reflection", "year": 2023, "authors": "Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, Pascale Fung", "url": "https://www.semanticscholar.org/paper/cd2e04598909158494e556823d9de8baa692cee2", "relevance": 3, "abstract": "Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of\"hallucination\", where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Consequently, we harness the interactivity and multitasking ability of LLMs and produce progressively more precise and accurate answers. Experimental results on both automatic and human evaluation demonstrate the superiority of our approach in hallucination reduction compared to baselines.", "citations": 102}
{"title": "Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine.", "year": 2023, "authors": "Peter Lee, S\u00e9bastien Bubeck, J. Petro", "url": "https://www.semanticscholar.org/paper/72b74bcff8fd76eff6789111a7ce5d0d6c5ac4db", "relevance": 3, "abstract": "", "citations": 1087}
{"title": "Leveraging Generative AI and Large Language Models: A Comprehensive Roadmap for Healthcare Integration", "year": 2023, "authors": "Ping Yu, Hua Xu, Xia Hu, C. Deng", "url": "https://www.semanticscholar.org/paper/d0c48d3b80efb9c170e7100cb9d78d1e7f7710bf", "relevance": 3, "abstract": "Generative artificial intelligence (AI) and large language models (LLMs), exemplified by ChatGPT, are promising for revolutionizing data and information management in healthcare and medicine. However, there is scant literature guiding their integration for non-AI professionals. This study conducts a scoping literature review to address the critical need for guidance on integrating generative AI and LLMs into healthcare and medical practices. It elucidates the distinct mechanisms underpinning these technologies, such as Reinforcement Learning from Human Feedback (RLFH), including few-shot learning and chain-of-thought reasoning, which differentiates them from traditional, rule-based AI systems. It requires an inclusive, collaborative co-design process that engages all pertinent stakeholders, including clinicians and consumers, to achieve these benefits. Although global research is examining both opportunities and challenges, including ethical and legal dimensions, LLMs offer promising advancements in healthcare by enhancing data management, information retrieval, and decision-making processes. Continued innovation in data acquisition, model fine-tuning, prompt strategy development, evaluation, and system implementation is imperative for realizing the full potential of these technologies. Organizations should proactively engage with these technologies to improve healthcare quality, safety, and efficiency, adhering to ethical and legal guidelines for responsible application.", "citations": 209}
{"title": "Med-HALT: Medical Domain Hallucination Test for Large Language Models", "year": 2023, "authors": "Logesh Kumar Umapathi, Ankit Pal, Malaikannan Sankarasubbu", "url": "https://www.semanticscholar.org/paper/3b0792f6d7f6aa6aadd316e73943116afef2979b", "relevance": 3, "abstract": "This research paper focuses on the challenges posed by hallucinations in large language models (LLMs), particularly in the context of the medical domain. Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain Hallucination Test), designed specifically to evaluate and reduce hallucinations. Med-HALT provides a diverse multinational dataset derived from medical examinations across various countries and includes multiple innovative testing modalities. Med-HALT includes two categories of tests reasoning and memory-based hallucination tests, designed to assess LLMs\u2019 problem-solving and information retrieval abilities. Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing significant differences in their performance. The paper provides detailed insights into the dataset, promoting transparency and reproducibility. Through this work, we aim to contribute to the development of safer and more reliable language models in healthcare. Our benchmark can be found at medhalt.github.io", "citations": 221}
{"title": "MedHalu: Hallucinations in Responses to Healthcare Queries by Large Language Models", "year": 2024, "authors": "Vibhor Agarwal, Yiqiao Jin, Mohit Chandra, Munmun De Choudhury, Srijan Kumar, Nishanth R. Sastry", "url": "https://www.semanticscholar.org/paper/d00ed4adc27d33f0a0206fde58225c6b420ce05b", "relevance": 3, "abstract": "Large language models (LLMs) are starting to complement traditional information seeking mechanisms such as web search. LLM-powered chatbots like ChatGPT are gaining prominence among the general public. AI chatbots are also increasingly producing content on social media platforms. However, LLMs are also prone to hallucinations, generating plausible yet factually incorrect or fabricated information. This becomes a critical problem when laypeople start seeking information about sensitive issues such as healthcare. Existing works in LLM hallucinations in the medical domain mainly focus on testing the medical knowledge of LLMs through standardized medical exam questions which are often well-defined and clear-cut with definitive answers. However, these approaches may not fully capture how these LLMs perform during real-world interactions with patients. This work conducts a pioneering study on hallucinations in LLM-generated responses to real-world healthcare queries from patients.We introduce MedHalu, a novel medical hallucination benchmark featuring diverse health-related topics and hallucinated responses from LLMs, with detailed annotation of the hallucination types and text spans. We also propose MedHaluDetect, a comprehensive framework for evaluating LLMs'abilities to detect hallucinations. Furthermore, we study the vulnerability to medical hallucinations among three groups -- medical experts, LLMs, and laypeople. Notably, LLMs significantly underperform human experts and, in some cases, even laypeople in detecting medical hallucinations. To improve hallucination detection, we propose an expert-in-the-loop approach that integrates expert reasoning into LLM inputs, significantly improving hallucination detection for all LLMs, including a 6.3% macro-F1 improvement for GPT-4. Our code and dataset are available at https://netsys.surrey.ac.uk/datasets/medhalu/.", "citations": 22}
{"title": "Large Language Models in Medicine: Applications, Challenges, and Future Directions", "year": 2025, "authors": "Erlan Yu, Xuehong Chu, Wanwan Zhang, Xiangbin Meng, Yaodong Yang, Xunming Ji, Chuanjie Wu", "url": "https://api.semanticscholar.org/CorpusId:279052935", "relevance": 3, "abstract": "In recent years, large language models (LLMs) represented by GPT-4 have developed rapidly and performed well in various natural language processing tasks, showing great potential and transformative impact. The medical field, due to its vast data information as well as complex diagnostic and treatment processes, is undoubtedly one of the most promising areas for the application of LLMs. At present, LLMs has been gradually implemented in clinical practice, medical research, and medical education. However, in practical applications, medical LLMs still face numerous challenges, including the phenomenon of hallucination, interpretability, and ethical concerns. Therefore, in-depth exploration is still needed in areas of standardized evaluation frameworks, multimodal LLMs, and multidisciplinary collaboration in the future, so as to realize the widespread application of medical LLMs and promote the development and transformation in the field of global healthcare. This review offers a comprehensive overview of applications, challenges, and future directions of LLMs in medicine, providing new insights for the sustained development of medical LLMs.", "citations": 37}
{"title": "From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities", "year": 2023, "authors": "Md Farhan Ishmam, Md Sakib Hossain Shovon, M. F. Mridha, Nilanjan Dey", "url": "https://www.semanticscholar.org/paper/88bddfb7d1e0462be8fe99fdbd71c658140cb17b", "relevance": 3, "abstract": "", "citations": 73}
{"title": "Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models", "year": 2024, "authors": "Duy Khoa Pham, Q. Vo", "url": "https://www.semanticscholar.org/paper/9368325ba67c0cac3902273deabebd02f7a1f139", "relevance": 3, "abstract": "The rapid advancement of large language models (LLMs) has significantly impacted various domains, including healthcare and biomedicine. However, the phenomenon of hallucination, where LLMs generate outputs that deviate from factual accuracy or context, poses a critical challenge, especially in high-stakes domains. This paper conducts a scoping study of existing techniques for mitigating hallucinations in knowledge-based task in general and especially for medical domains. Key methods covered in the paper include Retrieval-Augmented Generation (RAG)-based techniques, iterative feedback loops, supervised fine-tuning, and prompt engineering. These techniques, while promising in general contexts, require further adaptation and optimization for the medical domain due to its unique demands for up-to-date, specialized knowledge and strict adherence to medical guidelines. Addressing these challenges is crucial for developing trustworthy AI systems that enhance clinical decision-making and patient safety as well as accuracy of biomedical scientific research.", "citations": 24}
{"title": "HALO: Hallucination Analysis and Learning Optimization to Empower LLMs with Retrieval-Augmented Context for Guided Clinical Decision Making", "year": 2024, "authors": "Sumera Anjum, Hanzhi Zhang, Wenjun Zhou, E. Paek, Xiaopeng Zhao, Yunhe Feng", "url": "https://www.semanticscholar.org/paper/e461e9d183724e86e481ee382084b7d86dec45f6", "relevance": 3, "abstract": "Large language models (LLMs) have significantly advanced natural language processing tasks, yet they are susceptible to generating inaccurate or unreliable responses, a phenomenon known as hallucination. In critical domains such as health and medicine, these hallucinations can pose serious risks. This paper introduces HALO, a novel framework designed to enhance the accuracy and reliability of medical question-answering (QA) systems by focusing on the detection and mitigation of hallucinations. Our approach generates multiple variations of a given query using LLMs and retrieves relevant information from external open knowledge bases to enrich the context. We utilize maximum marginal relevance scoring to prioritize the retrieved context, which is then provided to LLMs for answer generation, thereby reducing the risk of hallucinations. The integration of LangChain further streamlines this process, resulting in a notable and robust increase in the accuracy of both open-source and commercial LLMs, such as Llama-3.1 (from 44% to 66%) and ChatGPT (from 56% to 70%). This framework underscores the critical importance of addressing hallucinations in medical QA systems, ultimately improving clinical decision-making and patient care. The open-source HALO is available at: https://github.com/ResponsibleAILab/HALO.", "citations": 7}
{"title": "Benchmarking ChatGPT-4 on a radiation oncology in-training exam and Red Journal Gray Zone cases: potentials and challenges for ai-assisted medical education and decision making in radiation oncology", "year": 2023, "authors": "Yixing Huang, A. Gomaa, S. Semrau, M. Haderlein, S. Lettmaier, T. Weissmann, J. Grigo, Hassen Ben Tkhayat, Benjamin Frey, U. Gaipl, L. Distel, Andreas Maier, R. Fietkau, Christoph Bert, F. Putz", "url": "https://www.semanticscholar.org/paper/bcea96cdff75b703c366c8bb8f42e30d4f8a9f3b", "relevance": 3, "abstract": "Purpose The potential of large language models in medicine for education and decision-making purposes has been demonstrated as they have achieved decent scores on medical exams such as the United States Medical Licensing Exam (USMLE) and the MedQA exam. This work aims to evaluate the performance of ChatGPT-4 in the specialized field of radiation oncology. Methods The 38th American College of Radiology (ACR) radiation oncology in-training (TXIT) exam and the 2022 Red Journal Gray Zone cases are used to benchmark the performance of ChatGPT-4. The TXIT exam contains 300 questions covering various topics of radiation oncology. The 2022 Gray Zone collection contains 15 complex clinical cases. Results For the TXIT exam, ChatGPT-3.5 and ChatGPT-4 have achieved the scores of 62.05% and 78.77%, respectively, highlighting the advantage of the latest ChatGPT-4 model. Based on the TXIT exam, ChatGPT-4\u2019s strong and weak areas in radiation oncology are identified to some extent. Specifically, ChatGPT-4 demonstrates better knowledge of statistics, CNS & eye, pediatrics, biology, and physics than knowledge of bone & soft tissue and gynecology, as per the ACR knowledge domain. Regarding clinical care paths, ChatGPT-4 performs better in diagnosis, prognosis, and toxicity than brachytherapy and dosimetry. It lacks proficiency in in-depth details of clinical trials. For the Gray Zone cases, ChatGPT-4 is able to suggest a personalized treatment approach to each case with high correctness and comprehensiveness. Importantly, it provides novel treatment aspects for many cases, which are not suggested by any human experts. Conclusion Both evaluations demonstrate the potential of ChatGPT-4 in medical education for the general public and cancer patients, as well as the potential to aid clinical decision-making, while acknowledging its limitations in certain domains. Owing to the risk of hallucinations, it is essential to verify the content generated by models such as ChatGPT for accuracy.", "citations": 89}
{"title": "MedGPTEval: A Dataset and Benchmark to Evaluate Responses of Large Language Models in Medicine", "year": 2023, "authors": "Jie Xu, Lu Lu, Sen Yang, Bilin Liang, Xinwei Peng, Jiali Pang, Jinru Ding, Xiaoming Shi, Lingrui Yang, Huan-Zhi Song, Kang Li, Xin Sun, Shaoting Zhang", "url": "https://api.semanticscholar.org/CorpusId:258676639", "relevance": 3, "abstract": "METHODS: First, a set of evaluation criteria is designed based on a comprehensive literature review. Second, existing candidate criteria are optimized for using a Delphi method by five experts in medicine and engineering. Third, three clinical experts design a set of medical datasets to interact with LLMs. Finally, benchmarking experiments are conducted on the datasets. The responses generated by chatbots based on LLMs are recorded for blind evaluations by five licensed medical experts. RESULTS: The obtained evaluation criteria cover medical professional capabilities, social comprehensive capabilities, contextual capabilities, and computational robustness, with sixteen detailed indicators. The medical datasets include twenty-seven medical dialogues and seven case reports in Chinese. Three chatbots are evaluated, ChatGPT by OpenAI, ERNIE Bot by Baidu Inc., and Doctor PuJiang (Dr. PJ) by Shanghai Artificial Intelligence Laboratory. Experimental results show that Dr. PJ outperforms ChatGPT and ERNIE Bot in both multiple-turn medical dialogue and case report scenarios.", "citations": 9}
{"title": "Beyond Multiple-Choice Accuracy: Real-World Challenges of Implementing Large Language Models in Healthcare", "year": 2024, "authors": "Yifan Yang, Qiao Jin, Qingqing Zhu, Zhizheng Wang, Francisco Erramuspe 'Alvarez, Nicholas Wan, Benjamin Hou, Zhiyong Lu", "url": "https://www.semanticscholar.org/paper/830e4503ddcf59fd19bd1824c1cb656ab36c7195", "relevance": 3, "abstract": "Large Language Models (LLMs) have gained significant attention in the medical domain for their human-level capabilities, leading to increased efforts to explore their potential in various healthcare applications. However, despite such a promising future, there are multiple challenges and obstacles that remain for their real-world uses in practical settings. This work discusses key challenges for LLMs in medical applications from four unique aspects: operational vulnerabilities, ethical and social considerations, performance and assessment difficulties, and legal and regulatory compliance. Addressing these challenges is crucial for leveraging LLMs to their full potential and ensuring their responsible integration into healthcare.", "citations": 6}
{"title": "Enhancing Large Language Models for Improved Accuracy and Safety in Medical Question Answering: Comparative Study", "year": 2024, "authors": "Dingqiao Wang, Jinguo Ye, Jingni Li, Jiangbo Liang, Qikai Zhang, Qiuling Hu, Caineng Pan, Dongliang Wang, Zhong Liu, Wen Shi, Mengxiang Guo, Fei Li, Wei Du, Yingfeng Zheng", "url": "https://www.semanticscholar.org/paper/e0de08bd2381783a883c73955c1f9a26bc0d5f26", "relevance": 3, "abstract": "Background Large language models (LLMs) offer the potential to improve virtual patient-physician communication and reduce health care professionals\u2019 workload. However, limitations in accuracy, outdated knowledge, and safety issues restrict their effective use in real clinical settings. Addressing these challenges is crucial for making LLMs a reliable health care tool. Objective This study aimed to evaluate the efficacy of Med-RISE, an information retrieval and augmentation tool, in comparison with baseline LLMs, focusing on enhancing accuracy and safety in medical question answering across diverse clinical domains. Methods This comparative study introduces Med-RISE, an enhanced version of a retrieval-augmented generation framework specifically designed to improve question-answering performance across wide-ranging medical domains and diverse disciplines. Med-RISE consists of 4 key steps: query rewriting, information retrieval (providing local and real-time retrieval), summarization, and execution (a fact and safety filter before output). This study integrated Med-RISE with 4 LLMs (GPT-3.5, GPT-4, Vicuna-13B, and ChatGLM-6B) and assessed their performance on 4 multiple-choice medical question datasets: MedQA (US Medical Licensing Examination), PubMedQA (original and revised versions), MedMCQA, and EYE500. Primary outcome measures included answer accuracy and hallucination rates, with hallucinations categorized into factuality (inaccurate information) or faithfulness (inconsistency with instructions) types. This study was conducted between March 2024 and August 2024. Results The integration of Med-RISE with each LLM led to a substantial increase in accuracy, with improvements ranging from 9.8% to 16.3% (mean 13%, SD 2.3%) across the 4 datasets. The enhanced accuracy rates were 16.3%, 12.9%, 13%, and 9.8% for GPT-3.5, GPT-4, Vicuna-13B, and ChatGLM-6B, respectively. In addition, Med-RISE effectively reduced hallucinations, with reductions ranging from 11.8% to 18% (mean 15.1%, SD 2.8%), factuality hallucinations decreasing by 13.5%, and faithfulness hallucinations decreasing by 5.8%. The hallucination rate reductions were 17.7%, 12.8%, 18%, and 11.8% for GPT-3.5, GPT-4, Vicuna-13B, and ChatGLM-6B, respectively. Conclusions The Med-RISE framework significantly improves the accuracy and reduces the hallucinations of LLMs in medical question answering across benchmark datasets. By providing local and real-time information retrieval and fact and safety filtering, Med-RISE enhances the reliability and interpretability of LLMs in the medical domain, offering a promising tool for clinical practice and decision support.", "citations": 1}
{"title": "Trustworthy Medical Question Answering: An Evaluation-Centric Survey", "year": 2025, "authors": "Yinuo Wang, Robert E. Mercer, Frank Rudzicz, Sudipta Singha Roy, Pengjie Ren, Zhumin Chen, Xindi Wang", "url": "https://api.semanticscholar.org/CorpusId:279154653", "relevance": 3, "abstract": "Trustworthiness in healthcare question-answering (QA) systems is important for ensuring patient safety, clinical effectiveness, and user confidence. As large language models (LLMs) become increasingly integrated into medical settings, the reliability of their responses directly influences clinical decision-making and patient outcomes. However, achieving comprehensive trustworthiness in medical QA poses significant challenges due to the inherent complexity of healthcare data, the critical nature of clinical scenarios, and the multifaceted dimensions of trustworthy AI. In this survey, we systematically examine six key dimensions of trustworthiness in medical QA, i.e., Factuality, Robustness, Fairness, Safety, Explainability, and Calibration. We review how each dimension is evaluated in existing LLM-based medical QA systems. We compile and compare major benchmarks designed to assess these dimensions and analyze evaluation-guided techniques that drive model improvements, such as retrieval-augmented grounding, adversarial fine-tuning, and safety alignment. Finally, we identify open challenges-such as scalable expert evaluation, integrated multi-dimensional metrics, and real-world deployment studies-and propose future research directions to advance the safe, reliable, and transparent deployment of LLM-powered medical QA.", "citations": 5}
{"title": "Demo: Statistically Significant Results On Biases and Errors of LLMs Do Not Guarantee Generalizable Results", "year": 2025, "authors": "Jonathan Liu, Haoling Qiu, Jonathan Lasko, Damianos G. Karakos, Mahsa Yarmohammadi, Mark Dredze", "url": "https://www.semanticscholar.org/paper/4cafc240af61879709d35eab897cc03829e1a5ed", "relevance": 3, "abstract": "Recent research has shown that hallucinations, omissions, and biases are prevalent in everyday use-cases of LLMs. However, chatbots used in medical contexts must provide consistent advice in situations where non-medical factors are involved, such as when demographic information is present. In order to understand the conditions under which medical chatbots fail to perform as expected, we develop an infrastructure that 1) automatically generates queries to probe LLMs and 2) evaluates answers to these queries using multiple LLM-as-a-judge setups and prompts. For 1), our prompt creation pipeline samples the space of patient demographics, histories, disorders, and writing styles to create realistic questions that we subsequently use to prompt LLMs. In 2), our evaluation pipeline provides hallucination and omission detection using LLM-as-a-judge as well as agentic workflows, in addition to LLM-as-a-judge treatment category detectors. As a baseline study, we perform two case studies on inter-LLM agreement and the impact of varying the answering and evaluation LLMs. We find that LLM annotators exhibit low agreement scores (average Cohen's Kappa $\\kappa=0.118$), and only specific (answering, evaluation) LLM pairs yield statistically significant differences across writing styles, genders, and races. We recommend that studies using LLM evaluation use multiple LLMs as evaluators in order to avoid arriving at statistically significant but non-generalizable results, particularly in the absence of ground-truth data. We also suggest publishing inter-LLM agreement metrics for transparency. Our code and dataset are available here: https://github.com/BBN-E/medic-neurips-2025-demo.", "citations": 0}
{"title": "MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations in Large Language Models", "year": 2025, "authors": "Shrey Pandit, Jiawei Xu, Junyuan Hong, Zhangyang Wang, Tianlong Chen, Kaidi Xu, Ying Ding", "url": "https://www.semanticscholar.org/paper/910d26adcd83c4ec36f365198f8b2224b14ad6c9", "relevance": 3, "abstract": "Advancements in Large Language Models (LLMs) and their increasing use in medical question-answering necessitate rigorous evaluation of their reliability. A critical challenge lies in hallucination, where models generate plausible yet factually incorrect outputs. In the medical domain, this poses serious risks to patient safety and clinical decision-making. To address this, we introduce MedHallu, the first benchmark specifically designed for medical hallucination detection. MedHallu comprises 10,000 high-quality question-answer pairs derived from PubMedQA, with hallucinated answers systematically generated through a controlled pipeline. Our experiments show that state-of-the-art LLMs, including GPT-4o, Llama-3.1, and the medically fine-tuned UltraMedical, struggle with this binary hallucination detection task, with the best model achieving an F1 score as low as 0.625 for detecting\"hard\"category hallucinations. Using bidirectional entailment clustering, we show that harder-to-detect hallucinations are semantically closer to ground truth. Through experiments, we also show incorporating domain-specific knowledge and introducing a\"not sure\"category as one of the answer categories improves the precision and F1 scores by up to 38% relative to baselines.", "citations": 18}
{"title": "Medical Hallucination in Foundation Models and Their Impact on Healthcare", "year": 2025, "authors": "Y. Kim, H. Jeong, S. Chen, S. S. Li, M. Lu, K. Alhamoud, J. Mun, C. Grau, M. Jung, R. Gameiro, L. Fan, Eugene W Park, T. Lin, J. Yoon, W. Yoon, M. Sap, Y. Tsvetkov, P. Liang, X. Xu, X. Liu, D. McDuff, H. Lee, H. W. Park, S. Tulebaev, C. Breazeal", "url": "https://api.semanticscholar.org/CorpusId:276737445", "relevance": 3, "abstract": "Foundation Models that are capable of processing and generating multi-modal data have transformed AI's role in medicine. However, a key limitation of their reliability is hallucination, where inaccurate or fabricated information can impact clinical decisions and patient safety. We define medical hallucination as any instance in which a model generates misleading medical content. This paper examines the unique characteristics, causes, and implications of medical hallucinations, with a particular focus on how these errors manifest themselves in real-world clinical scenarios. Our contributions include (1) a taxonomy for understanding and addressing medical hallucinations, (2) benchmarking models using medical hallucination dataset and physician-annotated LLM responses to real medical cases, providing direct insight into the clinical impact of hallucinations, and (3) a multi-national clinician survey on their experiences with medical hallucinations. Our results reveal that inference techniques such as Chain-of-Thought (CoT) and Search Augmented Generation can effectively reduce hallucination rates. However, despite these improvements, non-trivial levels of hallucination persist. These findings underscore the ethical and practical imperative for robust detection and mitigation strategies, establishing a foundation for regulatory policies that prioritize patient safety and maintain clinical integrity as AI becomes more integrated into healthcare. The feedback from clinicians highlights the urgent need for not only technical advances but also for clearer ethical and regulatory guidelines to ensure patient safety. A repository organizing the paper resources, summaries, and additional information is available at https://github.com/mitmedialab/medical_hallucination.", "citations": 46}
{"title": "ReflecTool: Towards Reflection-Aware Tool-Augmented Clinical Agents", "year": 2024, "authors": "Yusheng Liao, Shuyang Jiang, Yanfeng Wang, Yu Wang", "url": "https://www.semanticscholar.org/paper/09cdade9f905c5b93b170bb5eb0d8a0231c95c8a", "relevance": 3, "abstract": "Large Language Models (LLMs) have shown promising potential in the medical domain, assisting with tasks like clinical note generation and patient communication. However, current LLMs are limited to text-based communication, hindering their ability to interact with diverse forms of information in clinical environments. Despite clinical agents succeeding in diverse signal interaction, they are oriented to a single clinical scenario and hence fail for broader applications. To evaluate clinical agents holistically, we propose ClinicalAgent Bench~(CAB), a comprehensive medical agent benchmark consisting of 18 tasks across five key realistic clinical dimensions. Building on this, we introduce ReflecTool, a novel framework that excels at utilizing domain-specific tools within two stages. The first optimization stage progressively enlarges a long-term memory by saving successful solving processes and tool-wise experience of agents in a tiny pre-defined training set. In the following inference stage, ReflecTool can search for supportive successful demonstrations from already built long-term memory to guide the tool selection strategy, and a verifier improves the tool usage according to the tool-wise experience with two verification methods--iterative refinement and candidate selection. Extensive experiments on ClinicalAgent Benchmark demonstrate that ReflecTool surpasses the pure LLMs with more than 10 points and the well-established agent-based methods with 3 points, highlighting its adaptability and effectiveness in solving complex clinical tasks.", "citations": 11}
{"title": "UniVRSE: Unified Vision-conditioned Response Semantic Entropy for Hallucination Detection in Medical Vision-Language Models", "year": 2025, "authors": "Zehui Liao, Shishuai Hu, Ke Zou, Huazhu Fu, Liangli Zhen, Yong Xia", "url": "https://www.semanticscholar.org/paper/e4a736d4934e11b1352d5763f9208d1214ef9bd4", "relevance": 3, "abstract": "Vision-language models (VLMs) have great potential for medical image understanding, particularly in Visual Report Generation (VRG) and Visual Question Answering (VQA), but they may generate hallucinated responses that contradict visual evidence, limiting clinical deployment. Although uncertainty-based hallucination detection methods are intuitive and effective, they are limited in medical VLMs. Specifically, Semantic Entropy (SE), effective in text-only LLMs, becomes less reliable in medical VLMs due to their overconfidence from strong language priors. To address this challenge, we propose UniVRSE, a Unified Vision-conditioned Response Semantic Entropy framework for hallucination detection in medical VLMs. UniVRSE strengthens visual guidance during uncertainty estimation by contrasting the semantic predictive distributions derived from an original image-text pair and a visually distorted counterpart, with higher entropy indicating hallucination risk. For VQA, UniVRSE works on the image-question pair, while for VRG, it decomposes the report into claims, generates verification questions, and applies vision-conditioned entropy estimation at the claim level. To evaluate hallucination detection, we propose a unified pipeline that generates responses on medical datasets and derives hallucination labels via factual consistency assessment. However, current evaluation methods rely on subjective criteria or modality-specific rules. To improve reliability, we introduce Alignment Ratio of Atomic Facts (ALFA), a novel method that quantifies fine-grained factual consistency. ALFA-derived labels provide ground truth for robust benchmarking. Experiments on six medical VQA/VRG datasets and three VLMs show UniVRSE significantly outperforms existing methods with strong cross-modal generalization.", "citations": 4}
{"title": "From RAG to Agentic: Validating Islamic-Medicine Responses with LLM Agents", "year": 2025, "authors": "Mohammad Amaan Sayeed, Mohammed Talha Alam, Raza Imam, S. Sohail, Amir Hussain", "url": "https://www.semanticscholar.org/paper/fba124f2a93d14d2909b6c6f1a358218cd946925", "relevance": 3, "abstract": "Centuries-old Islamic medical texts like Avicenna's Canon of Medicine and the Prophetic Tibb-e-Nabawi encode a wealth of preventive care, nutrition, and holistic therapies, yet remain inaccessible to many and underutilized in modern AI systems. Existing language-model benchmarks focus narrowly on factual recall or user preference, leaving a gap in validating culturally grounded medical guidance at scale. We propose a unified evaluation pipeline, Tibbe-AG, that aligns 30 carefully curated Prophetic-medicine questions with human-verified remedies and compares three LLMs (LLaMA-3, Mistral-7B, Qwen2-7B) under three configurations: direct generation, retrieval-augmented generation, and a scientific self-critique filter. Each answer is then assessed by a secondary LLM serving as an agentic judge, yielding a single 3C3H quality score. Retrieval improves factual accuracy by 13%, while the agentic prompt adds another 10% improvement through deeper mechanistic insight and safety considerations. Our results demonstrate that blending classical Islamic texts with retrieval and self-evaluation enables reliable, culturally sensitive medical question-answering.", "citations": 3}
{"title": "MedCite: Can Language Models Generate Verifiable Text for Medicine?", "year": 2025, "authors": "Xiao Wang, Mengjue Tan, Qiao Jin, Guangzhi Xiong, Yu Hu, Aidong Zhang, Zhiyong Lu, Minjia Zhang", "url": "https://www.semanticscholar.org/paper/4082c099c95320a1938fda1e4bc195e30b45c6d2", "relevance": 3, "abstract": "Existing LLM-based medical question-answering systems lack citation generation and evaluation capabilities, raising concerns about their adoption in practice. In this work, we introduce \\name, the first end-to-end framework that facilitates the design and evaluation of citation generation with LLMs for medical tasks. Meanwhile, we introduce a novel multi-pass retrieval-citation method that generates high-quality citations. Our evaluation highlights the challenges and opportunities of citation generation for medical tasks, while identifying important design choices that have a significant impact on the final citation quality. Our proposed method achieves superior citation precision and recall improvements compared to strong baseline methods, and we show that evaluation results correlate well with annotation results from professional experts.", "citations": 6}
{"title": "Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals", "year": 2025, "authors": "Lucky Susanto, Anasta Pranawijayana, C. Sukotjo, Soni Prasad, Derry Wijaya", "url": "https://www.semanticscholar.org/paper/956af17e446ae8d6937f041dbf2b02dbcc20790b", "relevance": 3, "abstract": "Large language models (LLMs) are increasingly adopted in high-stakes domains such as healthcare and medical education, where the risk of generating factually incorrect (i.e., hallucinated) information is a major concern. While significant efforts have been made to detect and mitigate such hallucinations, predicting whether an LLM's response is correct remains a critical yet underexplored problem. This study investigates the feasibility of predicting correctness by analyzing a general-purpose model (GPT-4o) and a reasoning-centric model (OSS-120B) on a multiple-choice prosthodontics exam. We utilize metadata and hallucination signals across three distinct prompting strategies to build a correctness predictor for each (model, prompting) pair. Our findings demonstrate that this metadata-based approach can improve accuracy by up to +7.14% and achieve a precision of 83.12% over a baseline that assumes all answers are correct. We further show that while actual hallucination is a strong indicator of incorrectness, metadata signals alone are not reliable predictors of hallucination. Finally, we reveal that prompting strategies, despite not affecting overall accuracy, significantly alter the models'internal behaviors and the predictive utility of their metadata. These results present a promising direction for developing reliability signals in LLMs but also highlight that the methods explored in this paper are not yet robust enough for critical, high-stakes deployment.", "citations": 0}
{"title": "HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways", "year": 2025, "authors": "Cristian Cosentino, Anna M DeFilippo, Marco Dossena, Christopher Irwin, Sara Joubbi, Pietro Li\u00f2", "url": "https://www.semanticscholar.org/paper/253e8f6c798caa4e15590c6ee039de4abe843f2d", "relevance": 3, "abstract": "HealthBranches is a novel benchmark dataset for medical Question-Answering (Q&A), specifically designed to evaluate complex reasoning in Large Language Models (LLMs). This dataset is generated through a semi-automated pipeline that transforms explicit decision pathways from medical source into realistic patient cases with associated questions and answers. Covering 4,063 case studies across 17 healthcare topics, each data point is based on clinically validated reasoning chains. HealthBranches supports both open-ended and multiple-choice question formats and uniquely includes the full reasoning path for each Q&A. Its structured design enables robust evaluation of LLMs'multi-step inference capabilities, including their performance in structured Retrieval-Augmented Generation (RAG) contexts. HealthBranches establishes a foundation for the development of more trustworthy, interpretable, and clinically reliable LLMs in high-stakes domains while also serving as a valuable resource for educational purposes.", "citations": 1}
{"title": "From Retrieval to Generation: Unifying External and Parametric Knowledge for Medical Question Answering", "year": 2025, "authors": "Lei Li, Xiao Zhou, Yingying Zhang, Xian Wu", "url": "https://www.semanticscholar.org/paper/7f26158f36dca008e8df806698888a559362effa", "relevance": 3, "abstract": "Medical question answering (QA) requires extensive access to domain-specific knowledge. A promising direction is to enhance large language models (LLMs) with external knowledge retrieved from medical corpora or parametric knowledge stored in model parameters. Existing approaches typically fall into two categories: Retrieval-Augmented Generation (RAG), which grounds model reasoning on externally retrieved evidence, and Generation-Augmented Generation (GAG), which depends solely on the models internal knowledge to generate contextual documents. However, RAG often suffers from noisy or incomplete retrieval, while GAG is vulnerable to hallucinated or inaccurate information due to unconstrained generation. Both issues can mislead reasoning and undermine answer reliability. To address these challenges, we propose MedRGAG, a unified retrieval-generation augmented framework that seamlessly integrates external and parametric knowledge for medical QA. MedRGAG comprises two key modules: Knowledge-Guided Context Completion (KGCC), which directs the generator to produce background documents that complement the missing knowledge revealed by retrieval; and Knowledge-Aware Document Selection (KADS), which adaptively selects an optimal combination of retrieved and generated documents to form concise yet comprehensive evidence for answer generation. Extensive experiments on five medical QA benchmarks demonstrate that MedRGAG achieves a 12.5% improvement over MedRAG and a 4.5% gain over MedGENIE, highlighting the effectiveness of unifying retrieval and generation for knowledge-intensive reasoning. Our code and data are publicly available at https://anonymous.4open.science/r/MedRGAG", "citations": 0}
{"title": "Vision Language Models in Medicine", "year": 2025, "authors": "Beria Chingnabe Kalpelbe, Angel Gabriel Adaambiik, Wei Peng", "url": "https://www.semanticscholar.org/paper/6e2ef61b9f81e1cdff0869b25ef208d3d148be0d", "relevance": 3, "abstract": "With the advent of Vision-Language Models (VLMs), medical artificial intelligence (AI) has experienced significant technological progress and paradigm shifts. This survey provides an extensive review of recent advancements in Medical Vision-Language Models (Med-VLMs), which integrate visual and textual data to enhance healthcare outcomes. We discuss the foundational technology behind Med-VLMs, illustrating how general models are adapted for complex medical tasks, and examine their applications in healthcare. The transformative impact of Med-VLMs on clinical practice, education, and patient care is highlighted, alongside challenges such as data scarcity, narrow task generalization, interpretability issues, and ethical concerns like fairness, accountability, and privacy. These limitations are exacerbated by uneven dataset distribution, computational demands, and regulatory hurdles. Rigorous evaluation methods and robust regulatory frameworks are essential for safe integration into healthcare workflows. Future directions include leveraging large-scale, diverse datasets, improving cross-modal generalization, and enhancing interpretability. Innovations like federated learning, lightweight architectures, and Electronic Health Record (EHR) integration are explored as pathways to democratize access and improve clinical relevance. This review aims to provide a comprehensive understanding of Med-VLMs' strengths and limitations, fostering their ethical and balanced adoption in healthcare.", "citations": 6}
{"title": "Beyond Accuracy: Risk-Sensitive Evaluation of Hallucinated Medical Advice", "year": 2026, "authors": "Savan Doshi", "url": "https://www.semanticscholar.org/paper/72b02564a0dc23bddfb78693fefed75a7b67413b", "relevance": 3, "abstract": "Large language models are increasingly being used in patient-facing medical question answering, where hallucinated outputs can vary widely in potential harm. However, existing hallucination standards and evaluation metrics focus primarily on factual correctness, treating all errors as equally severe. This obscures clinically relevant failure modes, particularly when models generate unsupported but actionable medical language. We propose a risk-sensitive evaluation framework that quantifies hallucinations through the presence of risk-bearing language, including treatment directives, contraindications, urgency cues, and mentions of high-risk medications. Rather than assessing clinical correctness, our approach evaluates the potential impact of hallucinated content if acted upon. We further combine risk scoring with a relevance measure to identify high-risk, low-grounding failures. We apply this framework to three instruction-tuned language models using controlled patient-facing prompts designed as safety stress tests. Our results show that models with similar surface-level behavior exhibit substantially different risk profiles and that standard evaluation metrics fail to capture these distinctions. These findings highlight the importance of incorporating risk sensitivity into hallucination evaluation and suggest that evaluation validity is critically dependent on task and prompt design.", "citations": 0}
{"title": "Towards Mitigating LLM Hallucination via Self Reflection", "year": 2023, "authors": "Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, Pascale Fung", "url": "https://www.semanticscholar.org/paper/aa7e3b2b88ae3b434e3d5a343c7e3109c9d48ea8", "relevance": 2, "abstract": "", "citations": 389}
{"title": "MedHallBench: A New Benchmark for Assessing Hallucination in Medical Large Language Models", "year": 2024, "authors": "Kaiwen Zuo, Yirui Jiang", "url": "https://www.semanticscholar.org/paper/7112d0d36b960b590f55569bc294b2190d288860", "relevance": 2, "abstract": "Medical Large Language Models (MLLMs) have demonstrated potential in healthcare applications, yet their propensity for hallucinations -- generating medically implausible or inaccurate information -- presents substantial risks to patient care. This paper introduces MedHallBench, a comprehensive benchmark framework for evaluating and mitigating hallucinations in MLLMs. Our methodology integrates expert-validated medical case scenarios with established medical databases to create a robust evaluation dataset. The framework employs a sophisticated measurement system that combines automated ACHMI (Automatic Caption Hallucination Measurement in Medical Imaging) scoring with rigorous clinical expert evaluations and utilizes reinforcement learning methods to achieve automatic annotation. Through an optimized reinforcement learning from human feedback (RLHF) training pipeline specifically designed for medical applications, MedHallBench enables thorough evaluation of MLLMs across diverse clinical contexts while maintaining stringent accuracy standards. We conducted comparative experiments involving various models, utilizing the benchmark to establish a baseline for widely adopted large language models (LLMs). Our findings indicate that ACHMI provides a more nuanced understanding of the effects of hallucinations compared to traditional metrics, thereby highlighting its advantages in hallucination assessment. This research establishes a foundational framework for enhancing MLLMs' reliability in healthcare settings and presents actionable strategies for addressing the critical challenge of AI hallucinations in medical applications.", "citations": 18}
{"title": "Evaluating large language models in medical applications: a survey", "year": 2024, "authors": "Xiaolan Chen, Jiayang Xiang, Shanfu Lu, Yexin Liu, M. He, Danli Shi", "url": "https://www.semanticscholar.org/paper/905be3b9ecef14dd825feba7bdf9a9854a089d0f", "relevance": 2, "abstract": "Large language models (LLMs) have emerged as powerful tools with transformative potential across numerous domains, including healthcare and medicine. In the medical domain, LLMs hold promise for tasks ranging from clinical decision support to patient education. However, evaluating the performance of LLMs in medical contexts presents unique challenges due to the complex and critical nature of medical information. This paper provides a comprehensive overview of the landscape of medical LLM evaluation, synthesizing insights from existing studies and highlighting evaluation data sources, task scenarios, and evaluation methods. Additionally, it identifies key challenges and opportunities in medical LLM evaluation, emphasizing the need for continued research and innovation to ensure the responsible integration of LLMs into clinical practice.", "citations": 16}
{"title": "Uncertainty Estimation of Large Language Models in Medical Question Answering", "year": 2024, "authors": "Jiaxin Wu, Yizhou Yu, Hong-Yu Zhou", "url": "https://www.semanticscholar.org/paper/b7998f4af46561000e379e45d796370155fe99c1", "relevance": 2, "abstract": "Large Language Models (LLMs) show promise for natural language generation in healthcare, but risk hallucinating factually incorrect information. Deploying LLMs for medical question answering necessitates reliable uncertainty estimation (UE) methods to detect hallucinations. In this work, we benchmark popular UE methods with different model sizes on medical question-answering datasets. Our results show that current approaches generally perform poorly in this domain, highlighting the challenge of UE for medical applications. We also observe that larger models tend to yield better results, suggesting a correlation between model size and the reliability of UE. To address these challenges, we propose Two-phase Verification, a probability-free Uncertainty Estimation approach. First, an LLM generates a step-by-step explanation alongside its initial answer, followed by formulating verification questions to check the factual claims in the explanation. The model then answers these questions twice: first independently, and then referencing the explanation. Inconsistencies between the two sets of answers measure the uncertainty in the original response. We evaluate our approach on three biomedical question-answering datasets using Llama 2 Chat models and compare it against the benchmarked baseline methods. The results show that our Two-phase Verification method achieves the best overall accuracy and stability across various datasets and model sizes, and its performance scales as the model size increases.", "citations": 10}
{"title": "A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information: Errors, Misinformation, and Hallucination", "year": 2025, "authors": "Zhaoyi Sun, Wen-wai Yim, Ozlem Uzuner, Fei Xia, Meliha Yetisgen", "url": "https://www.semanticscholar.org/paper/d23f4eb4f710aeed0b4524df429c4ac481b071b3", "relevance": 2, "abstract": "", "citations": 3}
{"title": "MIRIAD: Augmenting LLMs with millions of medical query-response pairs", "year": 2025, "authors": "Qinyue Zheng, Salman Abdullah, Sam Rawal, Cyril Zakka, Sophie Ostmeier, Maximilian Purk, Eduardo Reis, E.J. Topol, J. Leskovec, Michael Moor", "url": "https://www.semanticscholar.org/paper/b1afad9875b76d2746c9c1bcad2c693110920987", "relevance": 2, "abstract": "LLMs are bound to transform healthcare with advanced decision support and flexible chat assistants. However, LLMs are prone to generate inaccurate medical content. To ground LLMs in high-quality medical knowledge, LLMs have been equipped with external knowledge via RAG, where unstructured medical knowledge is split into small text chunks that can be selectively retrieved and integrated into the LLMs context. Yet, existing RAG pipelines rely on raw, unstructured medical text, which can be noisy, uncurated and difficult for LLMs to effectively leverage. Systematic approaches to organize medical knowledge to best surface it to LLMs are generally lacking. To address these challenges, we introduce MIRIAD, a large-scale, curated corpus of 5,821,948 medical QA pairs, each rephrased from and grounded in a passage from peer-reviewed medical literature using a semi-automated pipeline combining LLM generation, filtering, grounding, and human annotation. Unlike prior medical corpora, which rely on unstructured text, MIRIAD encapsulates web-scale medical knowledge in an operationalized query-response format, which enables more targeted retrieval. Experiments on challenging medical QA benchmarks show that augmenting LLMs with MIRIAD improves accuracy up to 6.7% compared to unstructured RAG baselines with the same source corpus and with the same amount of retrieved text. Moreover, MIRIAD improved the ability of LLMs to detect medical hallucinations by 22.5 to 37% (increase in F1 score). We further introduce MIRIAD-Atlas, an interactive map of MIRIAD spanning 56 medical disciplines, enabling clinical users to visually explore, search, and refine medical knowledge. MIRIAD promises to unlock a wealth of down-stream applications, including medical information retrievers, enhanced RAG applications, and knowledge-grounded chat interfaces, which ultimately enables more reliable LLM applications in healthcare.", "citations": 6}
{"title": "Artificial intelligence in perioperative medicine education: A feasibility test of case-based learning", "year": 2025, "authors": "Timothy Trewren, Nicholas Fitzgerald, Sarah Jaensch, Olivia Nguyen, Alexander Tsymbal, Christina Gao, Brandon Stretton, Stewart Anderson, D-Yin Lin, Dario Winterton, Galina Gheihman, Guy Ludbrook, Kelly Bratkovic, Stephen Bacchi", "url": "https://www.semanticscholar.org/paper/b63bd28070685c2c0cb28fe5b3029cbfee2bc679", "relevance": 2, "abstract": "The use of artificial intelligence in medicine is rapidly expanding. Large language models, such as ChatGPT, have the potential to enhance perioperative medicine through education and clinical practice. However, concerns remain regarding the accuracy of these models, particularly the risk of hallucinations, generating factually incorrect outputs. This feasibility test explores the use of a large language model\u2013enabled platform to assist in case-based education in perioperative clinical cases. Methods: Five perioperative cases addressing core topics were developed and uploaded to a custom large language model platform. The large language model platform allows free-text questions to be asked to the artificial intelligence, which then uses the derived cases to provide answers. Anaesthetic trainees engaged with the artificial intelligence, asking questions to obtain information regarding history, examination, and investigations. Artificial intelligence question-and-answer pairs were then evaluated independently in duplicate for the presence of inappropriate responses, including hallucinations. Results: The large language model responded appropriately to nearly all questions, with no hallucinations observed. The proportion of questions that were answered appropriately was 99.3% (543/547). In the four instances of inappropriate responses, the large language model declined to provide information in the case description rather than hallucinate. Conclusion: The large language model appears capable of supporting the delivery of case-based perioperative medicine content with a high degree of accuracy.", "citations": 1}
{"title": "Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form Medical Question Answering Applications and Beyond", "year": 2024, "authors": "Zhiyuan Wang, Jinhao Duan, Chenxi Yuan, Qingyu Chen, Tianlong Chen, Huaxiu Yao, Yue Zhang, Ren Wang, Kaidi Xu, Xiaoshuang Shi", "url": "https://api.semanticscholar.org/CorpusId:267782499", "relevance": 2, "abstract": "Uncertainty estimation is crucial for the reliability of safety-critical human and artificial intelligence (AI) interaction systems, particularly in the domain of healthcare engineering. However, a robust and general uncertainty measure for free-form answers has not been well-established in open-ended medical question-answering (QA) tasks, where generative inequality introduces a large number of irrelevant words and sequences within the generated set for uncertainty quantification (UQ), which can lead to biases. This paper introduces Word-Sequence Entropy (WSE), a method that calibrates uncertainty at both the word and sequence levels, considering semantic relevance. WSE quantifies uncertainty in a way that is more closely aligned with the reliability of LLMs during uncertainty quantification (UQ). We compare WSE with six baseline methods on five free-form medical QA datasets, utilizing seven popular large language models (LLMs). Experimental results demonstrate that WSE exhibits superior performance in UQ under two standard criteria for correctness evaluation. Additionally, in terms of real-world medical QA applications, the performance of LLMs is significantly enhanced (e.g., a 6.36% improvement in model accuracy on the COVID-QA dataset) by employing responses with lower uncertainty that are identified by WSE as final answers, without any additional task-specific fine-tuning or architectural modifications.", "citations": 25}
{"title": "JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability", "year": 2024, "authors": "Junda Wang, Zhichao Yang, Zonghai Yao, Hong Yu", "url": "https://www.semanticscholar.org/paper/0c57cb87fcfb345ddc13bab3f20e9c203bac52e9", "relevance": 2, "abstract": "Large Language Models (LLMs) have demonstrated a remarkable potential in medical knowledge acquisition and question-answering. However, LLMs can potentially hallucinate and yield factually incorrect outcomes, even with domain-specific pretraining. Previously, retrieval augmented generation (RAG) has limited success in addressing hallucinations. Unlike previous methods in RAG where the retrieval model was trained separately from the LLM, we introduce JMLR (for Jointly trains LLM and information Retrieval) during the fine-tuning phase. The synchronized training mechanism enhances JMLR's ability to retrieve clinical guidelines and leverage medical knowledge to reason and answer questions and reduces the demand for computational resources. We evaluated JMLR on the important medical question-answering application. Our experimental results demonstrate that JMLR-13B (70.5%) outperforms a previous state-of-the-art open-source model using conventional pre-training and fine-tuning Meditron-70B (68.9%) and Llama2-13B with RAG (67.7%) on a medical question-answering dataset. Comprehensive evaluations reveal JMLR-13B enhances reasoning quality and reduces hallucinations better than Claude3-Opus. Additionally, JMLR-13B (148 GPU hours) also trains much faster than Meditron-70B (42630 GPU hours). Through this work, we provide a new and efficient knowledge enhancement method for healthcare, demonstrating the potential of integrating retrieval and LLM training for medical question-answering systems.", "citations": 54}
{"title": "M-Eval: A Heterogeneity-Based Framework for Multi-evidence Validation in Medical RAG Systems", "year": 2025, "authors": "Mengzhou Sun, Sendong Zhao, Jianyu Chen, Hao Wang, Bin Qin", "url": "https://www.semanticscholar.org/paper/e86e4fb91a02d05e097ef0e0875070bcec584cc4", "relevance": 2, "abstract": "Retrieval-augmented Generation (RAG) has demonstrated potential in enhancing medical question-answering systems through the integration of large language models (LLMs) with external medical literature. LLMs can retrieve relevant medical articles to generate more professional responses efficiently. However, current RAG applications still face problems. They generate incorrect information, such as hallucinations, and they fail to use external knowledge correctly. To solve these issues, we propose a new method named M-Eval. This method is inspired by the heterogeneity analysis approach used in Evidence-Based Medicine (EBM). Our approach can check for factual errors in RAG responses using evidence from multiple sources. First, we extract additional medical literature from external knowledge bases. Then, we retrieve the evidence documents generated by the RAG system. We use heterogeneity analysis to check whether the evidence supports different viewpoints in the response. In addition to verifying the accuracy of the response, we also assess the reliability of the evidence provided by the RAG system. Our method shows an improvement of up to 23.31% accuracy across various LLMs. This work can help detect errors in current RAG-based medical systems. It also makes the applications of LLMs more reliable and reduces diagnostic errors.", "citations": 0}
{"title": "Prompting Medical Large Vision-Language Models to Diagnose Pathologies by Visual Question Answering", "year": 2024, "authors": "Danfeng Guo, D. Terzopoulos", "url": "https://www.semanticscholar.org/paper/ed02abfecbd713d4ab7600a711f43721e8b3574f", "relevance": 2, "abstract": "Large Vision-Language Models (LVLMs) have achieved significant success in recent years, and they have been extended to the medical domain. Although demonstrating satisfactory performance on medical Visual Question Answering (VQA) tasks, Medical LVLMs (MLVLMs) suffer from the hallucination problem, which makes them fail to diagnose complex pathologies. Moreover, they readily fail to learn minority pathologies due to imbalanced training data. We propose two prompting strategies for MLVLMs that reduce hallucination and improve VQA performance. In the first strategy, we provide a detailed explanation of the queried pathology. In the second strategy, we fine-tune a cheap, weak learner to achieve high performance on a specific metric, and textually provide its judgment to the MLVLM. Tested on the MIMIC-CXR-JPG and Chexpert datasets, our methods significantly improve the diagnostic F1 score, with the highest increase being 0.27. We also demonstrate that our prompting strategies can be extended to general LVLM domains. Based on POPE metrics, it effectively suppresses the false negative predictions of existing LVLMs and improves Recall by approximately 0.07.", "citations": 8}
{"title": "Localizing Before Answering: A Hallucination Evaluation Benchmark for Grounded Medical Multimodal LLMs", "year": 2025, "authors": "Dung Nguyen, Minh Khoi Ho, Huy Ta, Thanh Tam Nguyen, Qi Chen, Kumar Rav, Quy Duong Dang, Satwik Ramchandre, S. L. Phung, Zhibin Liao, Minh-Son To, Johan W. Verjans, Phi Le Nguyen, Vu Minh Hieu Phan", "url": "https://www.semanticscholar.org/paper/219723d959e0014ea5543bcd93affb4f8cc744b9", "relevance": 2, "abstract": "Medical Large Multi-modal Models (LMMs) have demonstrated remarkable capabilities in medical data interpretation. However, these models frequently generate hallucinations contradicting source evidence, particularly due to inadequate localization reasoning. This work reveals a critical limitation in current medical LMMs: instead of analyzing relevant pathological regions, they often rely on linguistic patterns or attend to irrelevant image areas when responding to disease-related queries. To address this, we introduce HEAL-MedVQA (Hallucination Evaluation via Localization MedVQA), a comprehensive benchmark designed to evaluate LMMs'localization abilities and hallucination robustness. HEAL-MedVQA features (i) two innovative evaluation protocols to assess visual and textual shortcut learning, and (ii) a dataset of 67K VQA pairs, with doctor-annotated anatomical segmentation masks for pathological regions. To improve visual reasoning, we propose the Localize-before-Answer (LobA) framework, which trains LMMs to localize target regions of interest and self-prompt to emphasize segmented pathological areas, generating grounded and reliable answers. Experimental results demonstrate that our approach significantly outperforms state-of-the-art biomedical LMMs on the challenging HEAL-MedVQA benchmark, advancing robustness in medical VQA.", "citations": 4}
{"title": "Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology", "year": 2023, "authors": "Yixing Huang, A. Gomaa, S. Semrau, M. Haderlein, S. Lettmaier, T. Weissmann, J. Grigo, Hassen Ben Tkhayat, B. Frey, U. Gaipl, L. Distel, Andreas Maier, R. Fietkau, C. Bert, F. Putz", "url": "https://www.semanticscholar.org/paper/1a45858d84a3baece50bb690235323ed8579b4c7", "relevance": 2, "abstract": "The potential of large language models in medicine for education and decision making purposes has been demonstrated as they achieve decent scores on medical exams such as the United States Medical Licensing Exam (USMLE) and the MedQA exam. In this work, we evaluate the performance of ChatGPT-4 in the specialized field of radiation oncology using the 38th American College of Radiology (ACR) radiation oncology in-training (TXIT) exam and the 2022 Red Journal Gray Zone cases. For the TXIT exam, ChatGPT-3.5 and ChatGPT-4 have achieved the scores of 63.65% and 74.57%, respectively, highlighting the advantage of the latest ChatGPT-4 model. Based on the TXIT exam, ChatGPT-4's strong and weak areas in radiation oncology are identified to some extent. Specifically, ChatGPT-4 demonstrates better knowledge of statistics, CNS&eye, pediatrics, biology, and physics than knowledge of bone&soft tissue and gynecology, as per the ACR knowledge domain. Regarding clinical care paths, ChatGPT-4 performs better in diagnosis, prognosis, and toxicity than brachytherapy and dosimetry. It lacks proficiency in in-depth details of clinical trials. For the Gray Zone cases, ChatGPT-4 is able to suggest a personalized treatment approach to each case with high correctness and comprehensiveness. Importantly, it provides novel treatment aspects for many cases, which are not suggested by any human experts. Both evaluations demonstrate the potential of ChatGPT-4 in medical education for the general public and cancer patients, as well as the potential to aid clinical decision-making, while acknowledging its limitations in certain domains. Because of the risk of hallucination, facts provided by ChatGPT always need to be verified.", "citations": 12}
{"title": "MEDIC: Comprehensive Evaluation of Leading Indicators for LLM Safety and Utility in Clinical Applications", "year": 2024, "authors": "P. Kanithi, Cl'ement Christophe, Marco A. F. Pimentel, Tathagata Raha, Nada Saadi, Hamza A Javed, Svetlana Maslenkova, Nasir Hayat, Ronnie Rajan, Shadab Khan", "url": "https://www.semanticscholar.org/paper/38ca73bbaa08295f97ef0b64354ac6a759016cbd", "relevance": 2, "abstract": "While Large Language Models (LLMs) achieve superhuman performance on standardized medical licensing exams, these static benchmarks have become saturated and increasingly disconnected from the functional requirements of clinical workflows. To bridge the gap between theoretical capability and verified utility, we introduce MEDIC, a comprehensive evaluation framework establishing leading indicators across various clinical dimensions. Beyond standard question-answering, we assess operational capabilities using deterministic execution protocols and a novel Cross-Examination Framework (CEF), which quantifies information fidelity and hallucination rates without reliance on reference texts. Our evaluation across a heterogeneous task suite exposes critical performance trade-offs: we identify a significant knowledge-execution gap, where proficiency in static retrieval does not predict success in operational tasks such as clinical calculation or SQL generation. Furthermore, we observe a divergence between passive safety (refusal) and active safety (error detection), revealing that models fine-tuned for high refusal rates often fail to reliably audit clinical documentation for factual accuracy. These findings demonstrate that no single architecture dominates across all dimensions, highlighting the necessity of a portfolio approach to clinical model deployment. As part of this investigation, we released a public leaderboard on Hugging Face.\\footnote{https://huggingface.co/spaces/m42-health/MEDIC-Benchmark}", "citations": 30}
{"title": "OLAPH: Improving Factuality in Biomedical Long-form Question Answering", "year": 2024, "authors": "Minbyul Jeong, Hyeon Hwang, Chanwoong Yoon, Taewhoo Lee, Jaewoo Kang", "url": "https://www.semanticscholar.org/paper/01971e59136c8adafd66888a7bcb58c967efcd42", "relevance": 2, "abstract": "In the medical domain, numerous scenarios necessitate the long-form generation ability of large language models (LLMs). Specifically, when addressing patients' questions, it is essential that the model's response conveys factual claims, highlighting the need for an automated method to evaluate those claims. Thus, we introduce MedLFQA, a benchmark dataset reconstructed using long-form question-answering datasets related to the biomedical domain. We use MedLFQA to facilitate a cost-effective automatic evaluations of factuality. We also propose OLAPH, a simple and novel framework that utilizes cost-effective and multifaceted automatic evaluation to construct a synthetic preference set and answers questions in our preferred manner. Our framework leads us to train LLMs step-by-step to reduce hallucinations and include crucial medical claims. We highlight that, even on evaluation metrics not used during training, LLMs trained with our OLAPH framework demonstrate significant performance improvement in factuality. Our findings reveal that a 7B LLM trained with our OLAPH framework can provide long answers comparable to the medical experts' answers in terms of factuality. We believe that our work could shed light on gauging the long-text generation ability of LLMs in the medical domain. Our code and datasets are available.", "citations": 20}
{"title": "Worse than Random? An Embarrassingly Simple Probing Evaluation of Large Multimodal Models in Medical VQA", "year": 2024, "authors": "Qianqi Yan, Xuehai He, Xiang Yue, Xin Eric Wang", "url": "https://www.semanticscholar.org/paper/0911b452394eeabf46114df37d942d2048b96f83", "relevance": 2, "abstract": "Large Multimodal Models (LMMs) have shown remarkable progress in medical Visual Question Answering (Med-VQA), achieving high accuracy on existing benchmarks. However, their reliability under robust evaluation is questionable. This study reveals that when subjected to simple probing evaluation, state-of-the-art models perform worse than random guessing on medical diagnosis questions. To address this critical evaluation problem, we introduce the Probing Evaluation for Medical Diagnosis (ProbMed) dataset to rigorously assess LMM performance in medical imaging through probing evaluation and procedural diagnosis. Particularly, probing evaluation features pairing original questions with negation questions with hallucinated attributes, while procedural diagnosis requires reasoning across various diagnostic dimensions for each image, including modality recognition, organ identification, clinical findings, abnormalities, and positional grounding. Our evaluation reveals that top-performing models like GPT-4o, GPT-4V, and Gemini Pro perform worse than random guessing on specialized diagnostic questions, indicating significant limitations in handling fine-grained medical inquiries. Besides, models like LLaVA-Med struggle even with more general questions, and results from CheXagent demonstrate the transferability of expertise across different modalities of the same organ, showing that specialized domain knowledge is still crucial for improving performance. This study underscores the urgent need for more robust evaluation to ensure the reliability of LMMs in critical fields like medical diagnosis, and current LMMs are still far from applicable to those fields.", "citations": 19}
{"title": "Retrieval-Augmented Question Answering System Based on Large Language Models and Knowledge Graphs", "year": 2025, "authors": "Zhuojin Wang", "url": "https://www.semanticscholar.org/paper/31e3a04572ebd776f3d1be25675bab8ddb2a921b", "relevance": 2, "abstract": "With the increasing public awareness of health and safety, there is a growing demand for intelligent systems capable of providing precise answers to medical and health-related questions. However, Large Language Models (LLMs) are prone to \"hallucination\" phenomena in medical domain responses, generating seemingly plausible but actually inaccurate content, which could lead to serious consequences in medical scenarios. To address this challenge, this study proposes a knowledge graph-based retrieval-augmented question answering framework, with a specific focus on the dermatology domain. First, this study constructed a medical knowledge graph ontology for dermatology, encompassing key dimensions such as disease definitions, symptoms, diagnoses, and treatments. Second, a method was designed utilizing the large language model GLM-4 (General Language Model-4) for automated knowledge extraction from medical guidelines to construct a domain-specific knowledge graph. Third, this study introduced fuzzy entity recognition and knowledge graph enhancement mechanisms, which can identify key entities in questions and retrieve relevant knowledge from the graph to augment the original queries. Furthermore, experiments demonstrate that our approach effectively reduces hallucinations in LLM-generated medical responses.On the test set, a comparison with the baseline method reveals that our proposed framework achieves superior performance, with the average BLEU score increasing from 0.0102 to 0.0157, and the average BERT_SCORE_P, R, and F1 scores improving from 0.5037, 0.7120, and 0.5897 to 0.5273, 0.7346, and 0.6135, respectively.These results indicate significant accuracy improvements, particularly in diagnostic recommendations and treatment plans.This methodology provides a new paradigm for building safer and more reliable medical intelligent systems and can be extended to other specialized medical domains.", "citations": 0}
{"title": "ECG-LLM - training and evaluation of domain-specific large language models for electrocardiography", "year": 2025, "authors": "Lara Ahrens, Wilhelm Haverkamp, N. Strodthoff", "url": "https://www.semanticscholar.org/paper/bf4325c105f390af0812e94f97775c40f9e220b3", "relevance": 2, "abstract": "Domain-adapted open-weight large language models (LLMs) offer promising healthcare applications, from queryable knowledge bases to multimodal assistants, with the crucial advantage of local deployment for privacy preservation. However, optimal adaptation strategies, evaluation methodologies, and performance relative to general-purpose LLMs remain poorly characterized. We investigated these questions in electrocardiography, an important area of cardiovascular medicine, by finetuning open-weight models on domain-specific literature and implementing a multi-layered evaluation framework comparing finetuned models, retrieval-augmented generation (RAG), and Claude Sonnet 3.7 as a representative general-purpose model. Finetuned Llama 3.1 70B achieved superior performance on multiple-choice evaluations and automatic text metrics, ranking second to Claude 3.7 in LLM-as-a-judge assessments. Human expert evaluation favored Claude 3.7 and RAG approaches for complex queries. Finetuned models significantly outperformed their base counterparts across nearly all evaluation modes. Our findings reveal substantial performance heterogeneity across evaluation methodologies, underscoring assessment complexity. Nevertheless, domain-specific adaptation through finetuning and RAG achieves competitive performance with proprietary models, supporting the viability of privacy-preserving, locally deployable clinical solutions.", "citations": 0}
{"title": "Bridging Rationales and Relations: The Graph-Rationale-Guided Retrieval-Augmented Generation in Medical QA", "year": 2026, "authors": "Zhenyu Zhu", "url": "https://www.semanticscholar.org/paper/c1d3b3aab92d4cc2b9bc53491839625423fa44b1", "relevance": 2, "abstract": "Large language models (LLMs) face challenges of hallucination and knowledge obsolescence in medical question answering. Existing retrieval-augmented generation (RAG) frameworks can improve retrieval reliability through rational guiding; however, their neglect of structured knowledge leads to insufficient relational reasoning. This paper proposes the Graph-Rationale-Guided Retrieval-Augmented Generation (GRAG) framework, which introduces a knowledge graph layer based on Rationality-Guided Retrieval-Augmented Generation (RAG\u00b2) to support dynamic graph query expansion, evidence fusion, hallucination detection, and quantized low-rank adaptation (QLoRA). GRAG's core mechanisms include rational generation, entity extraction, graph construction based on Unified Medical Language System (UMLS)-Neo4j, and similarity-driven multi-source evidence fusion. Experiments on medical question answering dataset (MedQA), medical muti-choice question answering (MedMCQA), and a self-constructed RareDisease-MedQuAD subset show that GRAG outperforms baseline models by approximately 10-12% in accuracy, reduces hallucination rate by around 20%, and achieves graph fidelity exceeding 80%. Ablation experiments further confirm that the knowledge graph (KG) and QLoRA modules each contribute approximately 5-8% to performance improvements. Overall, GRAG bridges the gap between rational guidance and structured retrieval, providing a more interpretable and reliable solution for MedQA systems.", "citations": 0}
{"title": "Med-HVL: Automatic Medical Domain Hallucination Evaluation for Large Vision-Language Models", "year": null, "authors": "Qianqi Yan, Xuehai He, Xin Eric, Wang", "url": "https://www.semanticscholar.org/paper/147b8902e33dbfb212385a5bc0c525b07fb5594c", "relevance": 2, "abstract": "", "citations": 4}
{"title": "Rationale-Guided Retrieval Augmented Generation for Medical Question Answering", "year": 2024, "authors": "Jiwoong Sohn, Yein Park, Chanwoong Yoon, Sihyeon Park, Hyeon Hwang, Mujeen Sung, Hyunjae Kim, Jaewoo Kang", "url": "https://www.semanticscholar.org/paper/b03cfca7672e60cbe7999853e9c6f833ab20e012", "relevance": 2, "abstract": "Large language models (LLM) hold significant potential for applications in biomedicine, but they struggle with hallucinations and outdated knowledge. While retrieval-augmented generation (RAG) is generally employed to address these issues, it also has its own set of challenges: (1) LLMs are vulnerable to irrelevant or incorrect context, (2) medical queries are often not well-targeted for helpful information, and (3) retrievers are prone to bias toward the specific source corpus they were trained on. In this study, we present RAG$^2$ (RAtionale-Guided RAG), a new framework for enhancing the reliability of RAG in biomedical contexts. RAG$^2$ incorporates three key innovations: a small filtering model trained on perplexity-based labels of rationales, which selectively augments informative snippets of documents while filtering out distractors; LLM-generated rationales as queries to improve the utility of retrieved snippets; a structure designed to retrieve snippets evenly from a comprehensive set of four biomedical corpora, effectively mitigating retriever bias. Our experiments demonstrate that RAG$^2$ improves the state-of-the-art LLMs of varying sizes, with improvements of up to 6.1\\%, and it outperforms the previous best medical RAG model by up to 5.6\\% across three medical question-answering benchmarks. Our code is available at https://github.com/dmis-lab/RAG2.", "citations": 35}
{"title": "Introduction to Large Language Models (LLMs) for dementia care and research", "year": 2024, "authors": "M. Treder, Sojin Lee, K. Tsvetanov", "url": "https://www.semanticscholar.org/paper/29c5ce12847362bfd28bf43fdf7f71f09bcb757b", "relevance": 2, "abstract": "Introduction Dementia is a progressive neurodegenerative disorder that affects cognitive abilities including memory, reasoning, and communication skills, leading to gradual decline in daily activities and social engagement. In light of the recent advent of Large Language Models (LLMs) such as ChatGPT, this paper aims to thoroughly analyse their potential applications and usefulness in dementia care and research. Method To this end, we offer an introduction into LLMs, outlining the key features, capabilities, limitations, potential risks, and practical considerations for deployment as easy-to-use software (e.g., smartphone apps). We then explore various domains related to dementia, identifying opportunities for LLMs to enhance understanding, diagnostics, and treatment, with a broader emphasis on improving patient care. For each domain, the specific contributions of LLMs are examined, such as their ability to engage users in meaningful conversations, deliver personalized support, and offer cognitive enrichment. Potential benefits encompass improved social interaction, enhanced cognitive functioning, increased emotional well-being, and reduced caregiver burden. The deployment of LLMs in caregiving frameworks also raises a number of concerns and considerations. These include privacy and safety concerns, the need for empirical validation, user-centered design, adaptation to the user's unique needs, and the integration of multimodal inputs to create more immersive and personalized experiences. Additionally, ethical guidelines and privacy protocols must be established to ensure responsible and ethical deployment of LLMs. Results We report the results on a questionnaire filled in by people with dementia (PwD) and their supporters wherein we surveyed the usefulness of different application scenarios of LLMs as well as the features that LLM-powered apps should have. Both PwD and supporters were largely positive regarding the prospect of LLMs in care, although concerns were raised regarding bias, data privacy and transparency. Discussion Overall, this review corroborates the promising utilization of LLMs to positively impact dementia care by boosting cognitive abilities, enriching social interaction, and supporting caregivers. The findings underscore the importance of further research and development in this field to fully harness the benefits of LLMs and maximize their potential for improving the lives of individuals living with dementia.", "citations": 31}
{"title": "Multiple large language models versus experienced physicians in diagnosing challenging cases with gastrointestinal symptoms", "year": 2025, "authors": "Xintian Yang, Tongxin Li, Han Wang, Rongchun Zhang, Zhi Ni, Na Liu, Huihong Zhai, Jianghai Zhao, Fandong Meng, Zhongyin Zhou, Shanhong Tang, Limei Wang, Xiangping Wang, Hui Luo, Gui Ren, Linhui Zhang, Xiaoyu Kang, Jun Wang, Ning Bo, Xiaoning Yang, Weijie Xue, Xiaoying Zhang, Ning Chen, Rui Guo, Baiwen Li, Yajun Li, Yaling Liu, Tiantian Zhang, Shuhui Liang, Yong Lv, Yongzhan Nie, Daiming Fan, Lina Zhao, Yanglin Pan", "url": "https://www.semanticscholar.org/paper/9cb2138b954ebfd2b43134c34de0571f8fd3a448", "relevance": 2, "abstract": "Faced with challenging cases, doctors are increasingly seeking diagnostic advice from large language models (LLMs). This study aims to compare the ability of LLMs and human physicians to diagnose challenging cases. An offline dataset of 67 challenging cases with primary gastrointestinal symptoms was used to solicit possible diagnoses from seven LLMs and 22 gastroenterologists. The diagnoses by Claude 3.5 Sonnet covered the highest proportion (95% confidence interval [CI]) of instructive diagnoses (76.1%, [70.6%\u201380.9%]), significantly surpassing all the gastroenterologists (p\u2009<\u20090.05 for all). Claude 3.5 Sonnet achieved a significantly higher coverage rate (95% CI) than that of the gastroenterologists using search engines or other traditional resource (76.1% [70.6%\u201380.9%] vs. 45.5% [40.7%-50.4%], p\u2009<\u20090.001). The study highlights that advanced LLMs may assist gastroenterologists with instructive, time-saving, and cost-effective diagnostic scopes in challenging cases.", "citations": 17}
{"title": "Boosting LLM-assisted diagnosis: 10-minute LLM tutorial elevates radiology residents\u2019 performance in brain MRI interpretation", "year": 2024, "authors": "Su Hwan Kim, S. Schramm, Jonas Wihl, Philipp Raffler, Marlene Tahedl, Julian Canisius, I. Luiken, Lukas Endr\u00f6s, Stefan Reischl, A. Marka, Robert Walter, Mathias Schillmaier, Claus Zimmer, B. Wiestler, D. Hedderich", "url": "https://www.semanticscholar.org/paper/9f3234ac657be8c84be703e098c3f70b8d17e0bf", "relevance": 2, "abstract": "To evaluate the impact of a structured tutorial on the use of a large language model (LLM)-based search engine on radiology residents\u2019 performance in brain MRI differential diagnosis. In this study, nine radiology residents determined the three most likely differential diagnoses for three sets of ten brain MRI cases with a challenging yet definite diagnosis. Each set was assessed (1) with the support of conventional internet search, (2) using an LLM-based search engine (\u00a9 Perplexity AI) without prior tutorial, or (3) using the LLM-based search engine after a structured 10-minute tutorial. Reader responses were rated using a binary and numeric scoring system. Reading times and confidence levels (measured on a 5-point Likert scale) were recorded for each case. Search engine logs were examined to quantify user interaction metrics, and to identify hallucinations and misinterpretations in LLM responses. Radiology residents achieved the highest accuracy when employing the LLM-based search engine following the tutorial, indicating the correct diagnosis among the top three differential diagnoses in 62.5% of cases (55/88). This was followed by the LLM-assisted workflow before the tutorial (44.8%; 39/87) and the conventional internet search workflow (32.2%; 28/87). The LLM tutorial led to significantly higher performance (binary scores: p\u2009=\u20090.042, numeric scores: p\u2009=\u20090.016) and confidence (p\u2009=\u20090.006) but resulted in no relevant differences in reading times. Hallucinations were found in 5.1% of LLM queries. Our findings demonstrate the considerable benefits that even low-effort educational interventions on LLMs can provide, highlighting their potential role in radiology training programs.", "citations": 3}
{"title": "Rethinking Retrieval-Augmented Generation for Medicine: A Large-Scale, Systematic Expert Evaluation and Practical Insights", "year": 2025, "authors": "Hyunjae Kim, Jiwoong Sohn, Aidan Gilson, Nicholas C Cochran-Caggiano, Serina S Applebaum, Heeju Jin, Seihee Park, Yujin Park, Jiyeong Park, Seoyoung Choi, Brittany Alexandra Herrera Contreras, Thomas Huang, J. Yun, Ethan F. Wei, Roy Jiang, Leah Colucci, Eric Lai, Amisha D. Dave, Tuo Guo, Maxwell B. Singer, Yonghoe Koo, Ron A. Adelman, James Zou, Andrew Taylor, Arman Cohan, Hua Xu, Qingyu Chen", "url": "https://www.semanticscholar.org/paper/dc578d45039a5d06eeab4e49d1af29366107e7cd", "relevance": 2, "abstract": "Large language models (LLMs) are transforming the landscape of medicine, yet two fundamental challenges persist: keeping up with rapidly evolving medical knowledge and providing verifiable, evidence-grounded reasoning. Retrieval-augmented generation (RAG) has been widely adopted to address these limitations by supplementing model outputs with retrieved evidence. However, whether RAG reliably achieves these goals remains unclear. Here, we present the most comprehensive expert evaluation of RAG in medicine to date. Eighteen medical experts contributed a total of 80,502 annotations, assessing 800 model outputs generated by GPT-4o and Llama-3.1-8B across 200 real-world patient and USMLE-style queries. We systematically decomposed the RAG pipeline into three components: (i) evidence retrieval (relevance of retrieved passages), (ii) evidence selection (accuracy of evidence usage), and (iii) response generation (factuality and completeness of outputs). Contrary to expectation, standard RAG often degraded performance: only 22% of top-16 passages were relevant, evidence selection remained weak (precision 41-43%, recall 27-49%), and factuality and completeness dropped by up to 6% and 5%, respectively, compared with non-RAG variants. Retrieval and evidence selection remain key failure points for the model, contributing to the overall performance drop. We further show that simple yet effective strategies, including evidence filtering and query reformulation, substantially mitigate these issues, improving performance on MedMCQA and MedXpertQA by up to 12% and 8.2%, respectively. These findings call for re-examining RAG's role in medicine and highlight the importance of stage-aware evaluation and deliberate system design for reliable medical LLM applications.", "citations": 4}
{"title": "Agentic AI and Large Language Models in Radiology: Opportunities and Hallucination Challenges", "year": 2025, "authors": "Sara Salehi, Yashbir Singh, K. Horst, Quincy A. Hathaway, BJ Erickson", "url": "https://www.semanticscholar.org/paper/00fedd3da96a37948585e1a586133760142506cb", "relevance": 2, "abstract": "The field of radiology is experiencing rapid adoption of large language models (LLMs), yet their tendency to generate hallucinations (plausible but incorrect information) remains a significant barrier to trust. This comprehensive review evaluates emerging agentic artificial intelligence (AI) approaches, including multi-agent role-based systems, retrieval-augmented generation (RAG), and uncertainty quantification, to assess their potential for reducing hallucinations in radiology workflows. Evidence from 2024 to 2025 demonstrates that agentic AI can improve diagnostic accuracy and reduce error rates, though these methods remain computationally demanding and lack comprehensive clinical validation. Multi-agent frameworks enable cross-validation through role-based specialization and systematic workflow orchestration, while RAG strategies enhance accuracy by grounding responses in verified medical literature. Within multi-agent systems, uncertainty quantification enables agents to communicate confidence levels to one another, allowing them to appropriately weigh each other\u2019s contributions during collaborative analysis. While multi-agent frameworks and RAG strategies show significant promise, practical deployment will require careful integration with human oversight, robust evaluation metrics tailored to medical imaging tasks, and regulatory adaptation to ensure safe clinical use in diverse patient populations and imaging modalities.", "citations": 2}
{"title": "How valuable are the questions and answers generated by large language models in oral and maxillofacial surgery?", "year": 2025, "authors": "Kyuhyung Kim, Sae Byeol Mun, Young Jae Kim, Bong Chul Kim, K. Kim", "url": "https://www.semanticscholar.org/paper/8c82cd1fe2e67ee67f2fe0c2af68b74476bc75cd", "relevance": 2, "abstract": "Introduction In this study, we aim to evaluate the ability of large language models (LLM) to generate questions and answers in oral and maxillofacial surgery. Methods ChatGPT4, ChatGPT4o, and Claude3-Opus were evaluated in this study. Each LLM was instructed to generate 50 questions about oral and maxillofacial surgery. Three LLMs were asked to answer the generated 150 questions. Results All 150 questions generated by the three LLMs were related to oral and maxillofacial surgery. Each model exhibited a correct answer rate of over 90%. None of the three models were able to answer correctly all the questions they generated themselves. The correct answer rate was 97.0% for questions with figures, significantly higher than the 88.9% rate for questions without figures. The analysis of problem-solving by the three LLMs showed that each model generally inferred answers with high accuracy, and there were few logical errors that could be considered controversial. Additionally, all three scored above 88% for the fidelity of their explanations. Conclusion This study demonstrates that while LLMs like ChatGPT4, ChatGPT4o, and Claude3-Opus exhibit robust capabilities in generating and solving oral and maxillofacial surgery questions, their performance is not without limitations. None of the models were able to answer correctly all the questions they generated themselves, highlighting persistent challenges such as AI hallucinations and contextual understanding gaps. The results also emphasize the importance of multimodal inputs, as questions with annotated images achieved higher accuracy rates compared to text-only prompts. Despite these shortcomings, the LLMs showed significant promise in problem-solving, logical consistency, and response fidelity, particularly in structured or numerical contexts.", "citations": 1}
{"title": "Applications and Challenges of Retrieval-Augmented Generation (RAG) in Maternal Health: A Multi-Axial Review of the State of the Art in Biomedical QA with LLMs", "year": 2025, "authors": "Adriana Noguera, Andr\u00e9s L. Mogoll\u00f3n-Benavides, Manuel D. Ni\u00f1o-Mojica, Santiago R\u00faa, Daniel San\u00edn-Villa, Juan C. Tejada", "url": "https://www.semanticscholar.org/paper/366208141d0269b804eb6698c3e68d99d6772ea8", "relevance": 2, "abstract": "The emergence of large language models (LLMs) has redefined the potential of artificial intelligence in clinical domains. In this context, retrieval-augmented generation (RAG) systems provide a promising approach to enhance traceability, timeliness, and accuracy in tasks such as biomedical question answering (QA). This article presents a narrative and thematic review of the evolution of these technologies in maternal health, structured across five axes: technical foundations of RAG, advancements in biomedical LLMs, conversational agents in healthcare, clinical validation frameworks, and specific applications in obstetric telehealth. Through a systematic search in scientific databases covering the period from 2022 to 2025, 148 relevant studies were identified. Notable developments include architectures such as BiomedRAG and MedGraphRAG, which integrate semantic retrieval with controlled generation, achieving up to 18% improvement in accuracy compared to pure generative models. The review also highlights domain-specific models like PMC-LLaMA and Med-PaLM 2, while addressing persistent challenges in bias mitigation, hallucination reduction, and clinical validation. In the maternal care context, the review outlines applications in prenatal monitoring, the automatic generation of clinically validated QA pairs, and low-resource deployment using techniques such as QLoRA. The article concludes with a proposed research agenda emphasizing federated evaluation, participatory co-design with patients and healthcare professionals, and the ethical design of adaptable systems for diverse clinical settings.", "citations": 0}
{"title": "Can We Trust AI Doctors? A Survey of Medical Hallucination in Large Language and Large Vision-Language Models", "year": 2025, "authors": "Zhihong Zhu, Yunyan Zhang, Xianwei Zhuang, Fan Zhang, Zhongwei Wan, Yuyan Chen, Qingqing Long, Yefeng Zheng, Xian Wu", "url": "https://www.semanticscholar.org/paper/7f948e5b5effbc220ea558620979122665882737", "relevance": 2, "abstract": ",", "citations": 4}
{"title": "Evaluating Retrieval-Augmented Generation Variants for Clinical Decision Support: Hallucination Mitigation and Secure On-Premises Deployment", "year": 2025, "authors": "Krzysztof Wo\u0142k", "url": "https://www.semanticscholar.org/paper/740fa1e47c1fb22c364e51e1bd651606cd270fca", "relevance": 2, "abstract": "For clinical decision support to work, medical knowledge needs to be easy to find quickly and accurately. Retrieval-Augmented Generation (RAG) systems use big language models and document retrieval to help with diagnostic reasoning, but they could cause hallucinations and have strict privacy rules in healthcare. We tested twelve different types of RAG, such as dense, sparse, hybrid, graph-based, multimodal, self-reflective, adaptive, and security-focused pipelines, on 250 de-identified patient vignettes. We used Precision@5, Mean Reciprocal Rank, nDCG@10, hallucination rate, and latency to see how well the system worked. The best retrieval accuracy (P@5 \u2265 0.68, nDCG@10 \u2265 0.67) was achieved by a Haystack pipeline (DPR + BM25 + cross-encoder) and hybrid fusion (RRF). Self-reflective RAG, on the other hand, lowered hallucinations to 5.8%. Sparse retrieval gave the fastest response (120 ms), but it was not as accurate. We also suggest a single framework for reducing hallucinations that includes retrieval confidence thresholds, chain-of-thought verification, and outside fact-checking. Our findings emphasize pragmatic protocols for the secure implementation of RAG on premises, incorporating encryption, provenance tagging, and audit trails. Future directions encompass the incorporation of clinician feedback and the expansion of multimodal inputs to genomics and proteomics for precision medicine.", "citations": 1}
{"title": "Should we be going MAD? A Look at Multi-Agent Debate Strategies for LLMs", "year": 2023, "authors": "Andries P. Smit, Paul Duckworth, Nathan Grinsztajn, Kale-ab Tessera, Thomas D. Barrett, Arnu Pretorius", "url": "https://www.semanticscholar.org/paper/fce6367bb0a97efe1baded2ff311947e696caab2", "relevance": 2, "abstract": "Recent advancements in large language models (LLMs) underscore their potential for responding to inquiries in various domains. However, ensuring that generative agents provide accurate and reliable answers remains an ongoing challenge. In this context, multi-agent debate (MAD) has emerged as a promising strategy for enhancing the truthfulness of LLMs. We benchmark a range of debating and prompting strategies to explore the trade-offs between cost, time, and accuracy. Importantly, we find that multi-agent debating systems, in their current form, do not reliably outperform other proposed prompting strategies, such as self-consistency and ensembling using multiple reasoning paths. However, when performing hyperparameter tuning, several MAD systems, such as Multi-Persona, perform better. This suggests that MAD protocols might not be inherently worse than other approaches, but that they are more sensitive to different hyperparameter settings and difficult to optimize. We build on these results to offer insights into improving debating strategies, such as adjusting agent agreement levels, which can significantly enhance performance and even surpass all other non-debate protocols we evaluated. We provide an open-source repository to the community with several state-of-the-art protocols together with evaluation scripts to benchmark across popular research datasets.", "citations": 64}
{"title": "Leveraging long context in retrieval augmented language models for medical question answering", "year": 2025, "authors": "Gongbo Zhang, Zihan Xu, Qiao Jin, Fangyi Chen, Yilu Fang, Yi Liu, Justin F. Rousseau, Ziyang Xu, Zhiyong Lu, Chunhua Weng, Yifan Peng", "url": "https://api.semanticscholar.org/CorpusId:278284698", "relevance": 1, "abstract": "While holding great promise for improving and facilitating healthcare through applications of medical literature summarization, large language models (LLMs) struggle to produce up-to-date responses on evolving topics due to outdated knowledge or hallucination. Retrieval-augmented generation (RAG) is a pivotal innovation that improves the accuracy and relevance of LLM responses by integrating LLMs with a search engine and external sources of knowledge. However, the quality of RAG responses can be largely impacted by the rank and density of key information in the retrieval results, such as the \u201clost-in-the-middle\u201d problem. In this work, we aim to improve the robustness and reliability of the RAG workflow in the medical domain. Specifically, we propose a map-reduce strategy, BriefContext, to combat the \u201clost-in-the-middle\u201d issue without modifying the model weights. We demonstrated the advantage of the workflow with various LLM backbones and on multiple QA datasets. This method promises to improve the safety and reliability of LLMs deployed in healthcare domains by reducing the risk of misinformation, ensuring critical clinical content is retained in generated responses, and enabling more trustworthy use of LLMs in critical tasks such as medical question answering, clinical decision support, and patient-facing applications.", "citations": 35}
{"title": "MRAG: Benchmarking Retrieval-Augmented Generation for Bio-medicine", "year": 2026, "authors": "Wei Zhu", "url": "https://www.semanticscholar.org/paper/d98419f1e3d2530837c753450ebec25eabb96f1e", "relevance": 1, "abstract": "While Retrieval-Augmented Generation (RAG) has been swiftly adopted in scientific and clinical QA systems, a comprehensive evaluation benchmark in the medical domain is lacking. To address this gap, we introduce the Medical Retrieval-Augmented Generation (MRAG) benchmark, covering various tasks in English and Chinese languages, and building a corpus with Wikipedia and Pubmed. Additionally, we develop the MRAG-Toolkit, facilitating systematic exploration of different RAG components. Our experiments reveal that: (a) RAG enhances LLM reliability across MRAG tasks. (b) the performance of RAG systems is influenced by retrieval approaches, model sizes, and prompting strategies. (c) While RAG improves usefulness and reasoning quality, LLM responses may become slightly less readable for long-form questions. We will release the MRAG-Bench's dataset and toolkit with CCBY-4.0 license upon acceptance, to facilitate applications from both academia and industry.", "citations": 4}
{"title": "Large language models encode clinical knowledge", "year": 2022, "authors": "K. Singhal, Shekoofeh Azizi, T. Tu, S. Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl, P. Payne, Martin G. Seneviratne, P. Gamble, C. Kelly, Nathaneal Scharli, A. Chowdhery, P. A. Mansfield, B. A. Y. Arcas, D. Webster, Greg S. Corrado, Yossi Matias, K. Chou, Juraj Gottweis, Nenad Toma\u0161ev, Yun Liu, A. Rajkomar, J. Barral, Christopher Semturs, A. Karthikesalingam, Vivek Natarajan", "url": "https://api.semanticscholar.org/CorpusId:255124952", "relevance": 1, "abstract": "Med-PaLM, a\u00a0state-of-the-art large language model for medicine, is introduced and evaluated across several\u00a0medical question answering tasks, demonstrating the promise of these models\u00a0in this domain. Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and\u00a0a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension,\u00a0reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model^ 1 (PaLM,\u00a0a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM^ 2 on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA^ 3 , MedMCQA^ 4 , PubMedQA^ 5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics^ 6 ), including 67.6% accuracy on MedQA\u00a0(US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today\u2019s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications.", "citations": 3559}
{"title": "Correctness Coverage Evaluation for Medical Multiple-Choice Question Answering Based on the Enhanced Conformal Prediction Framework", "year": 2025, "authors": "Yusong Ke, Hongru Lin, Yuting Ruan, Junya Tang, Li Li", "url": "https://api.semanticscholar.org/CorpusId:276884664", "relevance": 1, "abstract": "Large language models (LLMs) are increasingly adopted in medical question answering (QA) scenarios. However, LLMs have been proven to generate hallucinations and nonfactual information, undermining their trustworthiness in high-stakes medical tasks. Conformal Prediction (CP) is now recognized as a robust framework within the broader domain of machine learning, offering statistically rigorous guarantees of marginal (average) coverage for prediction sets. However, the applicability of CP in medical QA remains to be explored. To address this limitation, this study proposes an enhanced CP framework for medical multiple-choice question answering (MCQA) tasks. The enhanced CP framework associates the non-conformance score with the frequency score of the correct option. The framework generates multiple outputs for the same medical query by leveraging self-consistency theory. The proposed framework calculates the frequency score of each option to address the issue of limited access to the model\u2019s internal information. Furthermore, a risk control framework is incorporated into the enhanced CP framework to manage task-specific metrics through a monotonically decreasing loss function. The enhanced CP framework is evaluated on three popular MCQA datasets using off-the-shelf LLMs. Empirical results demonstrate that the enhanced CP framework achieves user-specified average (or marginal) error rates on the test set. Moreover, the results show that the test set\u2019s average prediction set size (APSS) decreases as the risk level increases. It is concluded that it is a promising evaluation metric for the uncertainty of LLMs.", "citations": 1}
{"title": "Comparative performance evaluation of large language models in answering esophageal cancer-related questions: a multi-model assessment study", "year": 2025, "authors": "Zijie He, Lilan Zhao, Genglin Li, Jintao Wang, Songyu Cai, Pengjie Tu, Jingbo Chen, Jianman Wu, Juan Zhang, Ruiqi Chen, Yangyun Huang, Xiaojie Pan, Wenshu Chen", "url": "https://www.semanticscholar.org/paper/21ef2ca61eecd148464fa3fb8154f2f1a7fe0e40", "relevance": 1, "abstract": "Background Esophageal cancer has high incidence and mortality rates, leading to increased public demand for accurate information. However, the reliability of online medical information is often questionable. This study systematically compared the accuracy, completeness, and comprehensibility of mainstream large language models (LLMs) in answering esophageal cancer-related questions. Methods In total, 65 questions covering fundamental knowledge, preoperative preparation, surgical treatment, and postoperative management were selected. Each model, namely, ChatGPT 5, Claude Sonnet 4.0, DeepSeek-R1, Gemini 2.5 Pro, and Grok-4, was queried independently using standardized prompts. Five senior clinical experts, including three thoracic surgeons, one radiologist, and one medical oncologist, evaluated the responses using a five-point Likert scale. A retesting mechanism was applied for the low-scoring responses, and intraclass correlation coefficients were used to assess the rating consistency. The statistical analyses were conducted using the Friedman test, the Wilcoxon signed-rank test, and the Bonferroni correction. Results All the models performed well, with average scores exceeding 4.0. However, the following significant differences emerged: Gemini excelled in accuracy, while ChatGPT led in completeness, particularly in surgical and postoperative contexts. Minor differences appeared in fundamental knowledge, but notable disparities were found in complex areas. Retesting showed improvements in overall quality, yet some responses showed decreased completeness and relevance. Conclusion Large language models have considerable potential in answering questions about esophageal cancer, with significant differences in completeness. ChatGPT is more comprehensive in complex scenarios, while Gemini excels in accuracy. This study offers guidance for selecting artificial intelligence tools in clinical settings, advocating for a tiered application strategy tailored to specific scenarios and highlighting the importance of user education to understand the limitations and applicability of LLMs.", "citations": 1}
{"title": "Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty", "year": 2026, "authors": "Sravanthi Machcha, Sushrita Yerra, Sahil Gupta, Aishwarya Sahoo, Sharmin Sultana, Hong Yu, Zonghai Yao", "url": "https://api.semanticscholar.org/CorpusId:284911000", "relevance": 1, "abstract": "Current evaluation of large language models (LLMs) overwhelmingly prioritizes accuracy; however, in real-world and safety-critical applications, the ability to abstain when uncertain is equally vital for trustworthy deployment. We introduce MedAbstain, a unified benchmark and evaluation protocol for abstention in medical multiple-choice question answering (MCQA) -- a discrete-choice setting that generalizes to agentic action selection -- integrating conformal prediction, adversarial question perturbations, and explicit abstention options. Our systematic evaluation of both open- and closed-source LLMs reveals that even state-of-the-art, high-accuracy models often fail to abstain with uncertain. Notably, providing explicit abstention options consistently increases model uncertainty and safer abstention, far more than input perturbations, while scaling model size or advanced prompting brings little improvement. These findings highlight the central role of abstention mechanisms for trustworthy LLM deployment and offer practical guidance for improving safety in high-stakes applications.", "citations": 0}
{"title": "Toward expert-level medical question answering with large language models", "year": 2025, "authors": "Karan Singhal, Tao Tu, Juraj Gottweis, R. Sayres, Ellery Wulczyn, Mohamed Amin, Le Hou, Kevin Clark, Stephen R. Pfohl, Heather Cole-Lewis, Darlene Neal, Q. Rashid, Mike Schaekermann, Amy Wang, Dev Dash, Jonathan H. Chen, Nigam H. Shah, Sami Lachgar, P. Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Ag\u00fcera y Arcas, Nenad Toma\u0161ev, Yun Liu, Renee Wong, Christopher Semturs, S. Mahdavi, Joelle K. Barral, Dale R. Webster, G. Corrado, Yossi Matias, Shekoofeh Azizi, A. Karthikesalingam, Vivek Natarajan", "url": "https://www.semanticscholar.org/paper/b34c0b0169925a6c7f14e3de9764ed9505f30de3", "relevance": 1, "abstract": "Large language models (LLMs) have shown promise in medical question answering, with Med-PaLM being the first to exceed a \u2018passing\u2019 score in United States Medical Licensing Examination style questions. However, challenges remain in long-form medical question answering and handling real-world workflows. Here, we present Med-PaLM 2, which bridges these gaps with a combination of base LLM improvements, medical domain fine-tuning and new strategies for improving reasoning and grounding through ensemble refinement and chain of retrieval. Med-PaLM 2 scores up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19%, and demonstrates dramatic performance increases across MedMCQA, PubMedQA and MMLU clinical topics datasets. Our detailed human evaluations framework shows that physicians prefer Med-PaLM 2 answers to those from other physicians on eight of nine clinical axes. Med-PaLM 2 also demonstrates significant improvements over its predecessor across all evaluation metrics, particularly on new adversarial datasets designed to probe LLM limitations (P\u2009<\u20090.001). In a pilot study using real-world medical questions, specialists preferred Med-PaLM 2 answers to generalist physician answers 65% of the time. While specialist answers were still preferred overall, both specialists and generalists rated Med-PaLM 2 to be as safe as physician answers, demonstrating its growing potential in real-world medical applications. With an improved framework for model development and evaluation, a large language model is shown to provide answers to medical questions that are comparable or preferred with respect to those provided by human physicians.", "citations": 593}
{"title": "O87: Stratified Evaluation of Large Language Model GPT-4\u2019s Question-Answering In Surgery reveals AI Knowledge Gaps", "year": 2024, "authors": "R. M. Lonergan, Jake Curry, Kallpana Dhas, Benno I Simmons", "url": "https://www.semanticscholar.org/paper/b9c07a8aba5cb2d9b0248c9bef18369705253936", "relevance": 1, "abstract": "\n \n \n Large Language Models (LLMs), such as GPT, are artificial intelligence models designed to analyse vast data and generate coherent outputs, changing the way healthcare professionals access knowledge. Critically, LLMs also present incorrect information confidently, a phenomenon known as hallucination, which is particularly dangerous in the safety-critical field of medicine. The validity of LLM responses to medical queries are being explored generally, however, responses to surgical questions remain poorly quantified. Variations between specialties are important to identify to support strategic LLM improvements.\n \n \n \n We assessed accuracy of GPT-3 and GPT-4 in answering surgical multi-choice questions from the MedMCQA post-graduate question bank. We calculated the percentage accuracy of GPT-4 on all surgical questions (n=23025) and compared this to published GPT-4 performance across the whole MedMCQA dataset. We also analysed variations in performance by topic on a randomised sample of questions manually sorted by surgical specialty (n=1000).\n \n \n \n Accuracy rates for GPT-3 and GPT-4 were 53% and 64% respectively, demonstrating significant superiority of GPT-4, however GPT-4's surgical performance remained weaker than its overall MedMCQA performance. Notably, accuracy varied significantly by specialty, with strong performances in anatomy, vascular and paediatric surgery but below-average performances in orthopaedics, ENT and neurosurgery.\n \n \n \n This study holds significant implications for the expanding use of LLMs in surgery, especially education. GPT has improved accuracy with sequential developments, however, its performance requires further scrutiny. We recommend ongoing attention on factors underpinning subject-variation in performance to aid strategic LLM innovation.\n", "citations": 0}
{"title": "Detecting and Evaluating Medical Hallucinations in Large Vision Language Models", "year": 2024, "authors": "Jiawei Chen, Dingkang Yang, Tong Wu, Yue Jiang, Xiaolu Hou, Mingcheng Li, Shunli Wang, Dongling Xiao, Ke Li, Lihua Zhang", "url": "https://www.semanticscholar.org/paper/c910c8f715d8231ed824caff13952d6946de1e59", "relevance": 1, "abstract": "Large Vision Language Models (LVLMs) are increasingly integral to healthcare applications, including medical visual question answering and imaging report generation. While these models inherit the robust capabilities of foundational Large Language Models (LLMs), they also inherit susceptibility to hallucinations-a significant concern in high-stakes medical contexts where the margin for error is minimal. However, currently, there are no dedicated methods or benchmarks for hallucination detection and evaluation in the medical field. To bridge this gap, we introduce Med-HallMark, the first benchmark specifically designed for hallucination detection and evaluation within the medical multimodal domain. This benchmark provides multi-tasking hallucination support, multifaceted hallucination data, and hierarchical hallucination categorization. Furthermore, we propose the MediHall Score, a new medical evaluative metric designed to assess LVLMs' hallucinations through a hierarchical scoring system that considers the severity and type of hallucination, thereby enabling a granular assessment of potential clinical impacts. We also present MediHallDetector, a novel Medical LVLM engineered for precise hallucination detection, which employs multitask training for hallucination detection. Through extensive experimental evaluations, we establish baselines for popular LVLMs using our benchmark. The findings indicate that MediHall Score provides a more nuanced understanding of hallucination impacts compared to traditional metrics and demonstrate the enhanced performance of MediHallDetector. We hope this work can significantly improve the reliability of LVLMs in medical applications. All resources of this work will be released soon.", "citations": 41}
{"title": "Medical large language models are vulnerable to data-poisoning attacks", "year": 2025, "authors": "D. Alber, Zihao Yang, Anton Alyakin, Eunice Yang, Sumedha Rai, Aly A. Valliani, Jeff Zhang, Gabriel R. Rosenbaum, Ashley K. Amend-Thomas, David B. Kurland, Caroline M. Kremer, Alexander N Eremiev, Bruck Negash, D. Wiggan, M. A. Nakatsuka, K. Sangwon, Sean Neifert, Hammad A Khan, Akshay Save, Adhith Palla, Eric A. Grin, Monika Hedman, Mustafa Nasir-Moin, Xujin Chris Liu, L. Jiang, Michal Mankowski, D. Segev, Yindalon Aphinyanaphongs, H. Riina, J. Golfinos, D. Orringer, Douglas Kondziolka, E. Oermann", "url": "https://api.semanticscholar.org/CorpusId:275426106", "relevance": 1, "abstract": "The adoption of large language models (LLMs) in healthcare demands a careful analysis of their potential to spread false medical knowledge. Because LLMs ingest massive volumes of data from the open Internet during training, they are potentially exposed to unverified medical knowledge that may include deliberately planted misinformation. Here, we perform a threat assessment that simulates a data-poisoning attack against The Pile, a popular dataset used for LLM development. We find that replacement of just 0.001% of training tokens with medical misinformation results in harmful models more likely to propagate medical errors. Furthermore, we discover that corrupted models match the performance of their corruption-free counterparts on open-source benchmarks routinely used to evaluate medical LLMs. Using biomedical knowledge graphs to screen medical LLM outputs, we propose a harm mitigation strategy that captures 91.9% of harmful content (F1\u2009=\u200985.7%). Our algorithm provides a unique method to validate stochastically generated LLM outputs against hard-coded relationships in knowledge graphs. In view of current calls for improved data provenance and transparent LLM development, we hope to raise awareness of emergent risks from LLMs trained indiscriminately on web-scraped data, particularly in healthcare where misinformation can potentially compromise patient safety. Large language models can be manipulated to generate misinformation by poisoning of a very small percentage of the data on which they are trained, but a harm mitigation strategy using biomedical knowledge graphs can offer a method for addressing this vulnerability.", "citations": 124}
{"title": "MedOmni-45\u00b0: A Safety-Performance Benchmark for Reasoning-Oriented LLMs in Medicine", "year": 2025, "authors": "Kaiyuan Ji, Yijin Guo, Zicheng Zhang, Xiangyang Zhu, Yuan Tian, Ning Liu, Guangtao Zhai", "url": "https://api.semanticscholar.org/CorpusId:280708621", "relevance": 1, "abstract": "With the increasing use of large language models (LLMs) in medical decision-support, it is essential to evaluate not only their final answers but also the reliability of their reasoning. Two key risks are Chain-of-Thought (CoT) faithfulness -- whether reasoning aligns with responses and medical facts -- and sycophancy, where models follow misleading cues over correctness. Existing benchmarks often collapse such vulnerabilities into single accuracy scores. To address this, we introduce MedOmni-45 Degrees, a benchmark and workflow designed to quantify safety-performance trade-offs under manipulative hint conditions. It contains 1,804 reasoning-focused medical questions across six specialties and three task types, including 500 from MedMCQA. Each question is paired with seven manipulative hint types and a no-hint baseline, producing about 27K inputs. We evaluate seven LLMs spanning open- vs. closed-source, general-purpose vs. medical, and base vs. reasoning-enhanced models, totaling over 189K inferences. Three metrics -- Accuracy, CoT-Faithfulness, and Anti-Sycophancy -- are combined into a composite score visualized with a 45 Degrees plot. Results show a consistent safety-performance trade-off, with no model surpassing the diagonal. The open-source QwQ-32B performs closest (43.81 Degrees), balancing safety and accuracy but not leading in both. MedOmni-45 Degrees thus provides a focused benchmark for exposing reasoning vulnerabilities in medical LLMs and guiding safer model development.", "citations": 3}
{"title": "Med-CoDE: Medical Critique based Disagreement Evaluation Framework", "year": 2025, "authors": "Mohit Gupta, Akiko Aizawa, R. Shah", "url": "https://api.semanticscholar.org/CorpusId:277994091", "relevance": 1, "abstract": "The emergence of large language models (LLMs) has significantly influenced numerous fields, including healthcare, by enhancing the capabilities of automated systems to process and generate human-like text. However, despite their advancements, the reliability and accuracy of LLMs in medical contexts remain critical concerns. Current evaluation methods often lack robustness and fail to provide a comprehensive assessment of LLM performance, leading to potential risks in clinical settings. In this work, we propose Med-CoDE, a specifically designed evaluation framework for medical LLMs to address these challenges. The framework leverages a critique-based approach to quantitatively measure the degree of disagreement between model-generated responses and established medical ground truths. This framework captures both accuracy and reliability in medical settings. The proposed evaluation framework aims to fill the existing gap in LLM assessment by offering a systematic method to evaluate the quality and trustworthiness of medical LLMs. Through extensive experiments and case studies, we illustrate the practicality of our framework in providing a comprehensive and reliable evaluation of medical LLMs.", "citations": 2}
{"title": "LLM Robustness Against Misinformation in Biomedical Question Answering", "year": 2024, "authors": "Alexander Bondarenko, Adrian Viehweger", "url": "https://www.semanticscholar.org/paper/fb7e9dd52a88ec4e6b1b898a9a18ea23962d26b0", "relevance": 1, "abstract": "The retrieval-augmented generation (RAG) approach is used to reduce the confabulation of large language models (LLMs) for question answering by retrieving and providing additional context coming from external knowledge sources (e.g., by adding the context to the prompt). However, injecting incorrect information can mislead the LLM to generate an incorrect answer. In this paper, we evaluate the effectiveness and robustness of four LLMs against misinformation - Gemma 2, GPT-4o-mini, Llama~3.1, and Mixtral - in answering biomedical questions. We assess the answer accuracy on yes-no and free-form questions in three scenarios: vanilla LLM answers (no context is provided),\"perfect\"augmented generation (correct context is provided), and prompt-injection attacks (incorrect context is provided). Our results show that Llama 3.1 (70B parameters) achieves the highest accuracy in both vanilla (0.651) and\"perfect\"RAG (0.802) scenarios. However, the accuracy gap between the models almost disappears with\"perfect\"RAG, suggesting its potential to mitigate the LLM's size-related effectiveness differences. We further evaluate the ability of the LLMs to generate malicious context on one hand and the LLM's robustness against prompt-injection attacks on the other hand, using metrics such as attack success rate (ASR), accuracy under attack, and accuracy drop. As adversaries, we use the same four LLMs (Gemma 2, GPT-4o-mini, Llama 3.1, and Mixtral) to generate incorrect context that is injected in the target model's prompt. Interestingly, Llama is shown to be the most effective adversary, causing accuracy drops of up to 0.48 for vanilla answers and 0.63 for\"perfect\"RAG across target models. Our analysis reveals that robustness rankings vary depending on the evaluation measure, highlighting the complexity of assessing LLM resilience to adversarial attacks.", "citations": 2}
{"title": "AutoHall: Automated Factuality Hallucination Dataset Generation for Large Language Models", "year": 2023, "authors": "Zouying Cao, Yifei Yang, Hai Zhao", "url": "https://www.semanticscholar.org/paper/607bf4fd07ec93f7187ab22b5d14bd0194088420", "relevance": 1, "abstract": "Large language models (LLMs) have gained broad applications across various domains but still struggle with hallucinations. Currently, hallucinations occur frequently in the generation of factual content and pose a great challenge to trustworthy LLMs. However, hallucination detection is hindered by the laborious and expensive manual annotation of hallucinatory content. Meanwhile, as different LLMs exhibit distinct types and rates of hallucination, the collection of hallucination datasets is inherently model-specific, which also increases the cost. To address this issue, this paper proposes a method called AutoHall for Automatically constructing model-specific Hallucination datasets based on existing fact-checking datasets. The empirical results reveal variations in hallucination proportions and types among different models. Moreover, we introduce a zero-resource and black-box hallucination detection method based on self-contradiction to recognize the hallucination in our constructed dataset, achieving superior detection performance compared to baselines. Further analysis on our dataset provides insight into factors that may contribute to LLM hallucinations.", "citations": 12}
{"title": "Do Large Language Models have Shared Weaknesses in Medical Question Answering?", "year": 2023, "authors": "Andrew M. Bean, Karolina Korgul, F. Krones, Robert McCraith, Adam Mahdi", "url": "https://www.semanticscholar.org/paper/651a8eb6ea2180ae58bd4b4ce2c34e07c1d52c8b", "relevance": 1, "abstract": "Large language models (LLMs) have made rapid improvement on medical benchmarks, but their unreliability remains a persistent challenge for safe real-world uses. To design for the use LLMs as a category, rather than for specific models, requires developing an understanding of shared strengths and weaknesses which appear across models. To address this challenge, we benchmark a range of top LLMs and identify consistent patterns across models. We test $16$ well-known LLMs on $874$ newly collected questions from Polish medical licensing exams. For each question, we score each model on the top-1 accuracy and the distribution of probabilities assigned. We then compare these results with factors such as question difficulty for humans, question length, and the scores of the other models. LLM accuracies were positively correlated pairwise ($0.39$ to $0.58$). Model performance was also correlated with human performance ($0.09$ to $0.13$), but negatively correlated to the difference between the question-level accuracy of top-scoring and bottom-scoring humans ($-0.09$ to $-0.14$). The top output probability and question length were positive and negative predictors of accuracy respectively (p$<0.05$). The top scoring LLM, GPT-4o Turbo, scored $84\\%$, with Claude Opus, Gemini 1.5 Pro and Llama 3/3.1 between $74\\%$ and $79\\%$. We found evidence of similarities between models in which questions they answer correctly, as well as similarities with human test takers. Larger models typically performed better, but differences in training, architecture, and data were also highly impactful. Model accuracy was positively correlated with confidence, but negatively correlated with question length. We find similar results with older models, and argue that these patterns are likely to persist across future models using similar training methods.", "citations": 2}
{"title": "Almanac: Retrieval-Augmented Language Models for Clinical Medicine", "year": 2023, "authors": "W. Hiesinger, Cyril Zakka, Akash Chaurasia, R. Shad, Alex R. Dalal, Jennifer L. Kim, Michael Moor, Kevin Alexander, Euan A. Ashley, Jack Boyd, Kathleen Boyd, Karen Hirsch, C. Langlotz, Joanna Nelson", "url": "https://api.semanticscholar.org/CorpusId:258740478", "relevance": 1, "abstract": "Large-language models have recently demonstrated impressive zero-shot capabilities in a variety of natural language tasks such as summarization, dialogue generation, and question-answering. Despite many promising applications in clinical medicine, adoption of these models in real-world settings has been largely limited by their tendency to generate incorrect and sometimes even toxic statements. In this study, we develop Almanac, a large language model framework augmented with retrieval capabilities for medical guideline and treatment recommendations. Performance on a novel dataset of clinical scenarios (n= 130) evaluated by a panel of 5 board-certified and resident physicians demonstrates significant increases in factuality (mean of 18% at p-value < 0.05) across all specialties, with improvements in completeness and safety. Our results demonstrate the potential for large language models to be effective tools in the clinical decision-making process, while also emphasizing the importance of careful testing and deployment to mitigate their shortcomings.", "citations": 225}
{"title": "A Survey of Large Language Models in Medicine: Progress, Application, and Challenge", "year": 2023, "authors": "Hongjian Zhou, Boyang Gu, Xinyu Zou, Jinfa Huang, Yiru Li, Sam S. Chen, Peilin Zhou, Junling Liu, Y. Hua, Chengfeng Mao, Xian Wu, Zheng Li, Fenglin Liu", "url": "https://www.semanticscholar.org/paper/bca0bbd01ea917b7a9fe369288ea3ba03d3b1ff3", "relevance": 1, "abstract": "Large language models (LLMs), such as ChatGPT, have received substantial attention due to their capabilities for understanding and generating human language. While there has been a burgeoning trend in research focusing on the employment of LLMs in supporting different medical tasks (e.g., enhancing clinical diagnostics and providing medical education), a review of these efforts, particularly their development, practical applications, and outcomes in medicine, remains scarce. Therefore, this review aims to provide a detailed overview of the development and deployment of LLMs in medicine, including the challenges and opportunities they face. In terms of development, we provide a detailed introduction to the principles of existing medical LLMs, including their basic model structures, number of parameters, and sources and scales of data used for model development. It serves as a guide for practitioners in developing medical LLMs tailored to their specific needs. In terms of deployment, we offer a comparison of the performance of different LLMs across various medical tasks, and further compare them with state-of-the-art lightweight models, aiming to provide an understanding of the advantages and limitations of LLMs in medicine. Overall, in this review, we address the following questions: 1) What are the practices for developing medical LLMs 2) How to measure the medical task performance of LLMs in a medical setting? 3) How have medical LLMs been employed in real-world practice? 4) What challenges arise from the use of medical LLMs? and 5) How to more effectively develop and deploy medical LLMs? By answering these questions, this review aims to provide insights into the opportunities for LLMs in medicine and serve as a practical resource. We also maintain a regularly updated list of practical guides on medical LLMs at https://github.com/AI-in-Health/MedLLMsPracticalGuide", "citations": 195}
{"title": "MedFuzz: Exploring the Robustness of Large Language Models in Medical Question Answering", "year": 2024, "authors": "Robert Osazuwa Ness, Katie Matton, Hayden S. Helm, Sheng Zhang, Junaid Bajwa, Carey E. Priebe, Eric Horvitz", "url": "https://api.semanticscholar.org/CorpusId:270380308", "relevance": 1, "abstract": "Large language models (LLM) have achieved impressive performance on medical question-answering benchmarks. However, high benchmark accuracy does not imply that the performance generalizes to real-world clinical settings. Medical question-answering benchmarks rely on assumptions consistent with quantifying LLM performance but that may not hold in the open world of the clinic. Yet LLMs learn broad knowledge that can help the LLM generalize to practical conditions regardless of unrealistic assumptions in celebrated benchmarks. We seek to quantify how well LLM medical question-answering benchmark performance generalizes when benchmark assumptions are violated. Specifically, we present an adversarial method that we call MedFuzz (for medical fuzzing). MedFuzz attempts to modify benchmark questions in ways aimed at confounding the LLM. We demonstrate the approach by targeting strong assumptions about patient characteristics presented in the MedQA benchmark. Successful\"attacks\"modify a benchmark item in ways that would be unlikely to fool a medical expert but nonetheless\"trick\"the LLM into changing from a correct to an incorrect answer. Further, we present a permutation test technique that can ensure a successful attack is statistically significant. We show how to use performance on a\"MedFuzzed\"benchmark, as well as individual successful attacks. The methods show promise at providing insights into the ability of an LLM to operate robustly in more realistic settings.", "citations": 23}
{"title": "Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees", "year": 2025, "authors": "Guang Yang, Xinyang Liu", "url": "https://www.semanticscholar.org/paper/3574fa083a3bf76470d754a851cfec1ec6b99dff", "relevance": 1, "abstract": "Large Language Models (LLMs) have shown remarkable progress in multiple-choice question answering (MCQA), but their inherent unreliability, such as hallucination and overconfidence, limits their application in high-risk domains. To address this, we propose a frequency-based uncertainty quantification method under black-box settings, leveraging conformal prediction (CP) to ensure provable coverage guarantees. Our approach involves multiple independent samplings of the model's output distribution for each input, with the most frequent sample serving as a reference to calculate predictive entropy (PE). Experimental evaluations across six LLMs and four datasets (MedMCQA, MedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms logit-based PE in distinguishing between correct and incorrect predictions, as measured by AUROC. Furthermore, the method effectively controls the empirical miscoverage rate under user-specified risk levels, validating that sampling frequency can serve as a viable substitute for logit-based probabilities in black-box scenarios. This work provides a distribution-free model-agnostic framework for reliable uncertainty quantification in MCQA with guaranteed coverage, enhancing the trustworthiness of LLMs in practical applications.", "citations": 0}
{"title": "Bridging the Semantic Gaps: Improving Medical VQA Consistency with LLM-Augmented Question Sets", "year": 2025, "authors": "Yongpei Ma, Pengyu Wang, Adam G. Dunn, Usman Naseem, Jinman Kim", "url": "https://www.semanticscholar.org/paper/3353071ae316f2886d5663bc460d7f4490cba15a", "relevance": 1, "abstract": "Medical Visual Question Answering (MVQA) systems can interpret medical images in response to natural language queries. However, linguistic variability in question phrasing often undermines the consistency of these systems. To address this challenge, we propose a Semantically Equivalent Question Augmentation (SEQA) framework, which leverages large language models (LLMs) to generate diverse yet semantically equivalent rephrasings of questions. Specifically, this approach enriches linguistic diversity while preserving semantic meaning. We further introduce an evaluation metric, Total Agreement Rate with Semantically Equivalent Input and Correct Answer (TAR-SC), which assesses a model's capability to generate consistent and correct responses to semantically equivalent linguistic variations. In addition, we also propose three other diversity metrics - average number of QA items per image (ANQI), average number of questions per image with the same answer (ANQA), and average number of open-ended questions per image with the same semantics (ANQS). Using the SEQA framework, we augmented the benchmarked MVQA public datasets of SLAKE, VQA-RAD, and PathVQA. As a result, all three datasets achieved significant improvements by incorporating more semantically equivalent questions: ANQI increased by an average of 86.1, ANQA by 85.1, and ANQS by 46. Subsequent experiments evaluate three MVQA models (M2I2, MUMC, and BiomedGPT) under both zero-shot and fine-tuning settings on the enhanced datasets. Experimental results in MVQA datasets show that fine-tuned models achieve an average accuracy improvement of 19.35%, while our proposed TAR-SC metric shows an average improvement of 11. 61%, indicating a substantial enhancement in model consistency.", "citations": 0}
{"title": "ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge", "year": 2023, "authors": "Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steven Jiang, You Zhang", "url": "https://www.semanticscholar.org/paper/4a7f6c4e71e20311ade4e76e8d0945d499c31fcd", "relevance": 1, "abstract": "Objective The primary aim of this research was to address the limitations observed in the medical knowledge of prevalent large language models (LLMs) such as ChatGPT, by creating a specialized language model with enhanced accuracy in medical advice. Methods We achieved this by adapting and refining the large language model meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialogues sourced from a widely used online medical consultation platform. These conversations were cleaned and anonymized to respect privacy concerns. In addition to the model refinement, we incorporated a self-directed information retrieval mechanism, allowing the model to access and utilize real-time information from online sources like Wikipedia and data from curated offline medical databases. Results The fine-tuning of the model with real-world patient-doctor interactions significantly improved the model's ability to understand patient needs and provide informed advice. By equipping the model with self-directed information retrieval from reliable online and offline sources, we observed substantial improvements in the accuracy of its responses. Conclusion Our proposed ChatDoctor, represents a significant advancement in medical LLMs, demonstrating a significant improvement in understanding patient inquiries and providing accurate advice. Given the high stakes and low error tolerance in the medical field, such enhancements in providing accurate and reliable information are not only beneficial but essential.", "citations": 617}
{"title": "Evaluating interactions of patients with large language models for medical information", "year": 2025, "authors": "Nicolas Carl, Sarah Haggenm\u00fcller, C. Wies, Lisa Nguyen, J. Winterstein, M. J. Hetz, Maurin Mangold, Friedrich Otto Hartung, B. Gr\u00fcne, Tim Holland-Letz, Maurice Stephan Michel, T. Brinker, F. Wessels", "url": "https://www.semanticscholar.org/paper/c08b4e70f28790d1b6f5697e8b48117b5f8302c7", "relevance": 1, "abstract": "To explore the interaction of real\u2010world patients with a chatbot in a clinical setting, investigating key aspects of medical information provided by large language models (LLMs).", "citations": 8}
{"title": "Shallow Robustness, Deep Vulnerabilities: Multi-Turn Evaluation of Medical LLMs", "year": 2025, "authors": "Blazej Manczak, Eric Lin, Francisco Eiras, James O' Neill, Vaikkunth Mugunthan", "url": "https://api.semanticscholar.org/CorpusId:282064783", "relevance": 1, "abstract": "Large language models (LLMs) are rapidly transitioning into medical clinical use, yet their reliability under realistic, multi-turn interactions remains poorly understood. Existing evaluation frameworks typically assess single-turn question answering under idealized conditions, overlooking the complexities of medical consultations where conflicting input, misleading context, and authority influence are common. We introduce MedQA-Followup, a framework for systematically evaluating multi-turn robustness in medical question answering. Our approach distinguishes between shallow robustness (resisting misleading initial context) and deep robustness (maintaining accuracy when answers are challenged across turns), while also introducing an indirect-direct axis that separates contextual framing (indirect) from explicit suggestion (direct). Using controlled interventions on the MedQA dataset, we evaluate five state-of-the-art LLMs and find that while models perform reasonably well under shallow perturbations, they exhibit severe vulnerabilities in multi-turn settings, with accuracy dropping from 91.2% to as low as 13.5% for Claude Sonnet 4. Counterintuitively, indirect, context-based interventions are often more harmful than direct suggestions, yielding larger accuracy drops across models and exposing a significant vulnerability for clinical deployment. Further compounding analyses reveal model differences, with some showing additional performance drops under repeated interventions while others partially recovering or even improving. These findings highlight multi-turn robustness as a critical but underexplored dimension for safe and reliable deployment of medical LLMs.", "citations": 0}
{"title": "A Survey on Medical Large Language Models: Technology, Application, Trustworthiness, and Future Directions", "year": 2024, "authors": "Lei Liu, Xiaoyan Yang, Junchi Lei, Xiaoyang Liu, Yue Shen, Zhiqiang Zhang, Peng Wei, Jinjie Gu, Zhixuan Chu, Zhan Qin, Kui Ren", "url": "https://api.semanticscholar.org/CorpusId:270285974", "relevance": 1, "abstract": "With the advent of Large Language Models (LLMs), medical artificial intelligence (AI) has experienced substantial technological progress and paradigm shifts, highlighting the potential of LLMs to streamline healthcare delivery and improve patient outcomes. Considering this rapid technical progress, in this survey, we trace the recent advances of Medical Large Language Models (Med-LLMs), including the background, key findings, and mainstream techniques, especially for the evolution from general-purpose models to medical-specialized applications. Firstly, we delve into the foundational technology of Med-LLMs, indicating how general models can be progressively adapted and refined for the complicated medical tasks. Secondly, the wide-ranging applications of Med-LLMs are investigated across various healthcare domains, as well as an up-to-date review of existing Med-LLMs. The transformative impact of these models on daily medical practice is evident through their ability to assist clinicians, educators, and patients. Recognizing the importance of responsible innovation, we discuss the challenges associated with ensuring fairness, accountability, privacy, and robustness. Ethical considerations, rigorous evaluation methodologies, and the establishment of regulatory frameworks are crucial for building trustworthiness in the real-world system. We emphasize the need for ongoing scrutiny and development to maintain high standards of safety and reliability. Finally, we anticipate possible future trajectories for Med-LLMs, identifying key avenues for prudent expansion. By consolidating these insights, our review aims to provide professionals and researchers with a thorough understanding of the strengths and limitations of Med-LLMs, fostering a balanced and ethical approach to their integration into the healthcare ecosystem.", "citations": 43}
{"title": "Can large language models reason about medical questions?", "year": 2022, "authors": "Valentin Li'evin, C. Hother, A. Motzfeldt, O. Winther", "url": "https://www.semanticscholar.org/paper/d697b440dd0e65a05fe027e4c0ea85f62dcba033", "relevance": 1, "abstract": "", "citations": 399}
{"title": "Applications of Large Models in Medicine", "year": 2025, "authors": "YunHe Su, Zhengyang Lu, Junhui Liu, Ke Pang, Haoran Dai, Sa Liu, Yuxin Jia, Lujia Ge, Jing-min Yang", "url": "https://www.semanticscholar.org/paper/1d7fc640b5f11ca3c4946c78b0104dcce1050508", "relevance": 1, "abstract": "The rapid advancement of artificial intelligence (AI) in healthcare has significantly enhanced diagnostic accuracy and clinical decision-making processes. This review examines four pivotal studies that highlight the integration of large language models (LLMs) and multimodal systems in medical diagnostics. BioBERT demonstrates the efficacy of domain-specific pretraining on biomedical texts, improving performance in tasks such as named entity recognition, relation extraction, and question answering. Med-PaLM, a large-scale language model tailored for clinical question answering, leverages instruction prompt tuning to enhance accuracy and reduce harmful outputs, validated through the MultiMedQA benchmark. DR.KNOWS integrates medical knowledge graphs with LLMs, enhancing diagnostic reasoning and interpretability by grounding model predictions in structured medical knowledge. Medical Multimodal Foundation Models (MMFMs) combine textual and imaging data to improve tasks like segmentation, lesion detection, and automated report generation. These studies demonstrate the importance of domain adaptation, structured knowledge integration, and multimodal data fusion in developing robust and interpretable AI-driven diagnostic tools.", "citations": 7}
{"title": "Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answering", "year": 2024, "authors": "Nghia Trung Ngo, C. Nguyen, Franck Dernoncourt, T. Nguyen", "url": "https://www.semanticscholar.org/paper/006168b63d1d4358bce840b55b76c1cacfb35121", "relevance": 1, "abstract": "Retrieval-augmented generation (RAG) has emerged as a promising approach to enhance the performance of large language models (LLMs) in knowledge-intensive tasks such as those from medical domain. However, the sensitive nature of the medical domain necessitates a completely accurate and trustworthy system. While existing RAG benchmarks primarily focus on the standard retrieve-answer setting, they overlook many practical scenarios that measure crucial aspects of a reliable medical system. This paper addresses this gap by providing a comprehensive evaluation framework for medical question-answering (QA) systems in a RAG setting for these situations, including sufficiency, integration, and robustness. We introduce Medical Retrieval-Augmented Generation Benchmark (MedRGB) that provides various supplementary elements to four medical QA datasets for testing LLMs' ability to handle these specific scenarios. Utilizing MedRGB, we conduct extensive evaluations of both state-of-the-art commercial LLMs and open-source models across multiple retrieval conditions. Our experimental results reveals current models' limited ability to handle noise and misinformation in the retrieved documents. We further analyze the LLMs' reasoning processes to provides valuable insights and future directions for developing RAG systems in this critical medical domain.", "citations": 9}
{"title": "Retrieval augmented large language model system for comprehensive drug contraindications", "year": 2025, "authors": "Byeonghun Bang, Jongsuk Yoon, Dong-Jin Chang, Seho Park, Yong Oh Lee", "url": "https://www.semanticscholar.org/paper/9ca477d1918f4f659031829d92db461f947fb0e4", "relevance": 1, "abstract": "The versatility of large language models (LLMs) has been explored across various sectors, but their application in healthcare poses challenges, particularly in the domain of pharmaceutical contraindications where accurate and reliable information is required. This study enhances the capability of LLMs to address contraindications effectively by implementing a Retrieval Augmented Generation (RAG) pipeline. Utilizing OpenAI\u2019s GPT-4o-mini as the base model, and the text-embedding-3-small model for embeddings, our approach integrates LangChain to orchestrate a hybrid retrieval system with re-ranking. This system leverages Drug Utilization Review (DUR) data from public databases, focusing on contraindications for specific age groups, pregnancy, and concomitant drug use. The dataset includes 300 question\u2013answer pairs across three categories, with baseline model accuracy ranging from 0.49 to 0.57. Post-integration of the RAG pipeline, we observed a significant improvement in model accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications related to age groups, pregnancy, and concomitant drug use, respectively. The results indicate that augmenting LLMs with a RAG framework can substantially reduce uncertainty in prescription and drug intake decisions by providing more precise and reliable drug contraindication information.", "citations": 0}
{"title": "IvyGPT: InteractiVe Chinese pathwaY language model in medical domain", "year": 2023, "authors": "Rongsheng Wang, Yaofei Duan, C. Lam, Jing Chen, Jiangsheng Xu, Hao Chen, Xiaohong Liu, P. Pang, Tao Tan", "url": "https://www.semanticscholar.org/paper/972b2c4dcb9712d5cd1c5d9f06a5ac0c5e084350", "relevance": 1, "abstract": "General large language models (LLMs) such as ChatGPT have shown remarkable success. However, such LLMs have not been widely adopted for medical purposes, due to poor accuracy and inability to provide medical advice. We propose IvyGPT, an LLM based on LLaMA that is trained and fine-tuned with high-quality medical question-answer (QA) instances and Reinforcement Learning from Human Feedback (RLHF). After supervised fine-tuning, IvyGPT has good multi-turn conversation capabilities, but it cannot perform like a doctor in other aspects, such as comprehensive diagnosis. Through RLHF, IvyGPT can output richer diagnosis and treatment answers that are closer to human. In the training, we used QLoRA to train 33 billion parameters on a small number of NVIDIA A100 (80GB) GPUs. Experimental results show that IvyGPT has outperformed other medical GPT models.", "citations": 22}
{"title": "Popular large language model chatbots\u2019 accuracy, comprehensiveness, and self-awareness in answering ocular symptom queries", "year": 2023, "authors": "Krithi Pushpanathan, Zhi Wei Lim, Samantha Min Er Yew, D. Chen, Hazel Anne Hui'En Lin, Jocelyn Hui Lin Goh, W. Wong, Xiaofei Wang, Marcus Chun Jin Tan, V. T. Chang Koh, Y. Tham", "url": "https://www.semanticscholar.org/paper/4d51fe3bb284f8ec9c5c22a6d4c8fc04b4995940", "relevance": 1, "abstract": "", "citations": 67}
{"title": "Using ChatGPT for Clinical Practice and Medical Education: Cross-Sectional Survey of Medical Students\u2019 and Physicians\u2019 Perceptions", "year": 2023, "authors": "Pasin Tangadulrat, Supinya Sono, B. Tangtrakulwanich", "url": "https://www.semanticscholar.org/paper/cb3237e9eeb60be01b22ba124405669c1cd86ff3", "relevance": 1, "abstract": "Background ChatGPT is a well-known large language model\u2013based chatbot. It could be used in the medical field in many aspects. However, some physicians are still unfamiliar with ChatGPT and are concerned about its benefits and risks. Objective We aim to evaluate the perception of physicians and medical students toward using ChatGPT in the medical field. Methods A web-based questionnaire was sent to medical students, interns, residents, and attending staff with questions regarding their perception toward using ChatGPT in clinical practice and medical education. Participants were also asked to rate their perception of ChatGPT\u2019s generated response about knee osteoarthritis. Results Participants included 124 medical students, 46 interns, 37 residents, and 32 attending staff. After reading ChatGPT\u2019s response, 132 of the 239 (55.2%) participants had a positive rating about using ChatGPT for clinical practice. The proportion of positive answers was significantly lower in graduated physicians (48/115, 42%) compared with medical students (84/124, 68%; P<.001). Participants listed a lack of a patient-specific treatment plan, updated evidence, and a language barrier as ChatGPT\u2019s pitfalls. Regarding using ChatGPT for medical education, the proportion of positive responses was also significantly lower in graduate physicians (71/115, 62%) compared to medical students (103/124, 83.1%; P<.001). Participants were concerned that ChatGPT\u2019s response was too superficial, might lack scientific evidence, and might need expert verification. Conclusions Medical students generally had a positive perception of using ChatGPT for guiding treatment and medical education, whereas graduated doctors were more cautious in this regard. Nonetheless, both medical students and graduated doctors positively perceived using ChatGPT for creating patient educational materials.", "citations": 64}
{"title": "A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making", "year": 2025, "authors": "Xiao Wu, Ting-Zhu Huang, Liang-Jian Deng, Yanyuan Qiao, Imran Razzak, Yutong Xie", "url": "https://www.semanticscholar.org/paper/bfcb3998ad89e61699fc45a5f099e916eaceb2c7", "relevance": 1, "abstract": "Medical decision-making often involves integrating knowledge from multiple clinical specialties, typically achieved through multidisciplinary teams. Inspired by this collaborative process, recent work has leveraged large language models (LLMs) in multi-agent collaboration frameworks to emulate expert teamwork. While these approaches improve reasoning through agent interaction, they are limited by static, pre-assigned roles, which hinder adaptability and dynamic knowledge integration. To address these limitations, we propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration framework that enables LLM agents to dynamically form and expand expert teams based on the evolving diagnostic context. KAMAC begins with one or more expert agents and then conducts a knowledge-driven discussion to identify and fill knowledge gaps by recruiting additional specialists as needed. This supports flexible, scalable collaboration in complex clinical scenarios, with decisions finalized through reviewing updated agent comments. Experiments on two real-world medical benchmarks demonstrate that KAMAC significantly outperforms both single-agent and advanced multi-agent methods, particularly in complex clinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty expertise. Our code is publicly available at: https://github.com/XiaoXiao-Woo/KAMAC.", "citations": 1}
{"title": "Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement Learning", "year": 2025, "authors": "Chi Liu, Derek Li, Yan Shu, Robin Chen, Derek Duan, Teng Fang, Bryan Dai", "url": "https://www.semanticscholar.org/paper/38ef9907ac3680fe0fba70f1305379c99e225349", "relevance": 1, "abstract": "While large language models show promise in medical applications, achieving expert-level clinical reasoning remains challenging due to the need for both accurate answers and transparent reasoning processes. To address this challenge, we introduce Fleming-R1, a model designed for verifiable medical reasoning through three complementary innovations. First, our Reasoning-Oriented Data Strategy (RODS) combines curated medical QA datasets with knowledge-graph-guided synthesis to improve coverage of underrepresented diseases, drugs, and multi-hop reasoning chains. Second, we employ Chain-of-Thought (CoT) cold start to distill high-quality reasoning trajectories from teacher models, establishing robust inference priors. Third, we implement a two-stage Reinforcement Learning from Verifiable Rewards (RLVR) framework using Group Relative Policy Optimization, which consolidates core reasoning skills while targeting persistent failure modes through adaptive hard-sample mining. Across diverse medical benchmarks, Fleming-R1 delivers substantial parameter-efficient improvements: the 7B variant surpasses much larger baselines, while the 32B model achieves near-parity with GPT-4o and consistently outperforms strong open-source alternatives. These results demonstrate that structured data design, reasoning-oriented initialization, and verifiable reinforcement learning can advance clinical reasoning beyond simple accuracy optimization. We release Fleming-R1 publicly to promote transparent, reproducible, and auditable progress in medical AI, enabling safer deployment in high-stakes clinical environments.", "citations": 2}
{"title": "Benchmarking Retrieval-Augmented Generation for Medicine", "year": 2024, "authors": "Guangzhi Xiong, Qiao Jin, Zhiyong Lu, Aidong Zhang", "url": "https://www.semanticscholar.org/paper/b798cf6af813638fab09a8af6ad0f3df6c241485", "relevance": 1, "abstract": "While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MedRAG toolkit introduced in this work. Overall, MedRAG improves the accuracy of six different LLMs by up to 18% over chain-of-thought prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our results show that the combination of various medical corpora and retrievers achieves the best performance. In addition, we discovered a log-linear scaling property and the\"lost-in-the-middle\"effects in medical RAG. We believe our comprehensive evaluations can serve as practical guidelines for implementing RAG systems for medicine.", "citations": 392}
{"title": "Comparative Evaluation of a Medical Large Language Model in Answering Real-World Radiation Oncology Questions: Multicenter Observational Study", "year": 2024, "authors": "Fabio Dennst\u00e4dt, Max Schmerder, Elena Riggenbach, Lucas Mose, Katarina Bryjova, Nicolas Bachmann, P. Mackeprang, M. Ahmadsei, Dubravko Sinovcic, Paul Windisch, Daniel R. Zwahlen, S. Rogers, O. Riesterer, Martin Maffei, E. Gkika, H. Haddad, Jan Peeken, P. Putora, M. Glatzer, Florian Putz, D. Hoefler, S. Christ, I. Filchenko, Janna Hastings, R. Gaio, Lawrence Chiang, Daniel Aebersold, N. Cihoric", "url": "https://www.semanticscholar.org/paper/3505cf18fc68181978576c4841a58cd83580b383", "relevance": 1, "abstract": "Background Large language models (LLMs) hold promise for supporting clinical tasks, particularly in data-driven and technical disciplines such as radiation oncology. While prior evaluation studies have focused on examination-style settings for evaluating LLMs, their performance in real-life clinical scenarios remains unclear. In the future, LLMs might be used as general AI assistants to answer questions arising in clinical practice. It is unclear how well a modern LLM, locally executed within the infrastructure of a hospital, would answer such questions compared with clinical experts. Objective This study aimed to assess the performance of a locally deployed, state-of-the-art medical LLM in answering real-world clinical questions in radiation oncology compared with clinical experts. The aim was to evaluate the overall quality of answers, as well as the potential harmfulness of the answers if used for clinical decision-making. Methods Physicians from 10 departments of European hospitals collected questions arising in the clinical practice of radiation oncology. Fifty of these questions were answered by 3 senior radiation oncology experts with at least 10 years of work experience, as well as the LLM Llama3-OpenBioLLM-70B (Ankit Pal and Malaikannan Sankarasubbu). In a blinded review, physicians rated the overall answer quality on a 5-point Likert scale (quality), assessed whether an answer might be potentially harmful if used for clinical decision-making (harmfulness), and determined if responses were from an expert or the LLM (recognizability). Comparisons between clinical experts and LLMs were then made for quality, harmfulness, and recognizability. Results There were no significant differences between the quality of the answers between LLM and clinical experts (mean scores of 3.38 vs 3.63; median 4.00, IQR 3.00-4.00 vs median 3.67, IQR 3.33-4.00; P=.26; Wilcoxon signed rank test). The answers were deemed potentially harmful in 13% of cases for the clinical experts compared with 16% of cases for the LLM (P=.63; Fisher exact test). Physicians correctly identified whether an answer was given by a clinical expert or an LLM in 78% and 72% of cases, respectively. Conclusions A state-of-the-art medical LLM can answer real-life questions from the clinical practice of radiation oncology similarly well as clinical experts regarding overall quality and potential harmfulness. Such LLMs can already be deployed within the local hospital environment at an affordable cost. While LLMs may not yet be ready for clinical implementation as general AI assistants, the technology continues to improve at a rapid pace. Evaluation studies based on real-life situations are important to better understand the weaknesses and limitations of LLMs in clinical practice. Such studies are also crucial to define when the technology is ready for clinical implementation. Furthermore, education for health care professionals on generative AI is needed to ensure responsible clinical implementation of this transforming technology.", "citations": 1}
{"title": "HIVMedQA: Benchmarking large language models for HIV medical decision support", "year": 2025, "authors": "Gonzalo Cardenal-Antolin, J. Fellay, Bashkim Jaha, R. Kouyos, N. Beerenwinkel, Diane Duroux", "url": "https://www.semanticscholar.org/paper/14cc521817abf23142822f880c9569498ccf129a", "relevance": 1, "abstract": "Large language models (LLMs) are emerging as valuable tools to support clinicians in routine decision-making. HIV management is a compelling use case due to its complexity, including diverse treatment options, comorbidities, and adherence challenges. However, integrating LLMs into clinical practice raises concerns about accuracy, potential harm, and clinician acceptance. Despite their promise, AI applications in HIV care remain underexplored, and LLM benchmarking studies are scarce. This study evaluates the current capabilities of LLMs in HIV management, highlighting their strengths and limitations. We introduce HIVMedQA, a benchmark designed to assess open-ended medical question answering in HIV care. The dataset consists of curated, clinically relevant questions developed with input from an infectious disease physician. We evaluated seven general-purpose and three medically specialized LLMs, applying prompt engineering to enhance performance. Our evaluation framework incorporates both lexical similarity and an LLM-as-a-judge approach, extended to better reflect clinical relevance. We assessed performance across key dimensions: question comprehension, reasoning, knowledge recall, bias, potential harm, and factual accuracy. Results show that Gemini 2.5 Pro consistently outperformed other models across most dimensions. Notably, two of the top three models were proprietary. Performance declined as question complexity increased. Medically fine-tuned models did not always outperform general-purpose ones, and larger model size was not a reliable predictor of performance. Reasoning and comprehension were more challenging than factual recall, and cognitive biases such as recency and status quo were observed. These findings underscore the need for targeted development and evaluation to ensure safe, effective LLM integration in clinical care.", "citations": 0}
{"title": "A Benchmark for Long-Form Medical Question Answering", "year": 2024, "authors": "Pedram Hosseini, Jessica M. Sin, Bing Ren, Bryceton G. Thomas, Elnaz Nouri, Ali Farahanchi, Saeed Hassanpour", "url": "https://www.semanticscholar.org/paper/699cf8cce0053dca8c70ccf78caf092d1cabb6e2", "relevance": 1, "abstract": "There is a lack of benchmarks for evaluating large language models (LLMs) in long-form medical question answering (QA). Most existing medical QA evaluation benchmarks focus on automatic metrics and multiple-choice questions. While valuable, these benchmarks fail to fully capture or assess the complexities of real-world clinical applications where LLMs are being deployed. Furthermore, existing studies on evaluating long-form answer generation in medical QA are primarily closed-source, lacking access to human medical expert annotations, which makes it difficult to reproduce results and enhance existing baselines. In this work, we introduce a new publicly available benchmark featuring real-world consumer medical questions with long-form answer evaluations annotated by medical doctors. We performed pairwise comparisons of responses from various open and closed-source medical and general-purpose LLMs based on criteria such as correctness, helpfulness, harmfulness, and bias. Additionally, we performed a comprehensive LLM-as-a-judge analysis to study the alignment between human judgments and LLMs. Our preliminary results highlight the strong potential of open LLMs in medical QA compared to leading closed models. Code&Data: https://github.com/lavita-ai/medical-eval-sphere", "citations": 17}
{"title": "Applications and Future Prospects of Medical LLMs: A Survey Based on the M-KAT Conceptual Framework", "year": 2024, "authors": "Ying Chang, Jian-Ming Yin, Jianmin Li, Chang Liu, Ling-yong Cao, Shuyuan Lin", "url": "https://www.semanticscholar.org/paper/52bfd1205745b0339799bc5b42f7171b9e1e79a6", "relevance": 1, "abstract": "", "citations": 11}
{"title": "Exploring the Capabilities and Limitations of Large Language Models for Radiation Oncology Decision Support.", "year": 2024, "authors": "F. Putz, M. Haderlein, S. Lettmaier, S. Semrau, R. Fietkau, Yixing Huang", "url": "https://www.semanticscholar.org/paper/3df4cb1b0328f33147a6a2313ecbd70457399197", "relevance": 1, "abstract": "", "citations": 11}
{"title": "Evaluation of DeepSeek-R1 and contemporary large language models on the radiology board examination: A milestone achieved as open-source model matches performance with closed-source model.", "year": 2025, "authors": "Takeshi Nakaura, Naoki Kobayashi, T. Masuda, Y. Nagayama, H. Uetani, M. Kidoh, S. Oda, Y. Funama, Toshinori Hirai", "url": "https://www.semanticscholar.org/paper/92e84f8ae3e04703b8ef1e04cfb1e61686b95ef7", "relevance": 1, "abstract": "", "citations": 0}
{"title": "Comparing Patient\u2019s Confidence in Clinical Capabilities in Urology: Large Language Models Versus Urologists", "year": 2024, "authors": "N. Carl, Lisa Nguyen, Sarah Haggenm\u00fcller, Martin Joachim Hetz, Jana Theres Winterstein, Friedrich Otto Hartung, Britta Gruene, Jakob Nikolas Kather, Tim Holland-Letz, Maurice Stephan Michel, F. Wessels, Titus Josef Brinker", "url": "https://www.semanticscholar.org/paper/69295aafaa144765b67997a6718189a7bd102580", "relevance": 1, "abstract": "", "citations": 11}
{"title": "Improving Retrieval-Augmented Generation in Medicine with Iterative Follow-up Questions", "year": 2024, "authors": "Guangzhi Xiong, Qiao Jin, Xiao Wang, Minjia Zhang, Zhiyong Lu, Aidong Zhang", "url": "https://www.semanticscholar.org/paper/2bb9a87bdfc8a35bc1813e5a88180f43615785a8", "relevance": 1, "abstract": "The emergent abilities of large language models (LLMs) have demonstrated great potential in solving medical questions. They can possess considerable medical knowledge, but may still hallucinate and are inflexible in the knowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed to enhance the medical question-answering capabilities of LLMs with external knowledge bases, it may still fail in complex cases where multiple rounds of information-seeking are required. To address such an issue, we propose iterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up queries based on previous information-seeking attempts. In each iteration of i-MedRAG, the follow-up queries will be answered by a vanilla RAG system and they will be further used to guide the query generation in the next iteration. Our experiments show the improved performance of various LLMs brought by i-MedRAG compared with vanilla RAG on complex questions from clinical vignettes in the United States Medical Licensing Examination (USMLE), as well as various knowledge tests in the Massive Multitask Language Understanding (MMLU) dataset. Notably, our zero-shot i-MedRAG outperforms all existing prompt engineering and fine-tuning methods on GPT-3.5, achieving an accuracy of 69.68% on the MedQA dataset. In addition, we characterize the scaling properties of i-MedRAG with different iterations of follow-up queries and different numbers of queries per iteration. Our case studies show that i-MedRAG can flexibly ask follow-up queries to form reasoning chains, providing an in-depth analysis of medical questions. To the best of our knowledge, this is the first-of-its-kind study on incorporating follow-up queries into medical RAG.", "citations": 79}
{"title": "Tiered Agentic Oversight: A Hierarchical Multi-Agent System for AI Safety in Healthcare", "year": 2025, "authors": "Y. Kim, H. Jeong, Chanwoo Park, Eugene Park, Haipeng Zhang, Xin Liu, Hyeonhoon Lee, D. McDuff, Marzyeh Ghassemi, Cynthia Breazeal, S. Tulebaev, Hae Won Park", "url": "https://www.semanticscholar.org/paper/3d343734acb39bf91b9fd88461484fd3d9433076", "relevance": 1, "abstract": "", "citations": 3}
{"title": "MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering", "year": 2025, "authors": "Yuexing Hao, Kumail Alhamoud, Hyewon Jeong, Haoran Zhang, Isha Puri, Philip Torr, Mike Schaekermann, Ariel D. Stern, Marzyeh Ghassemi", "url": "https://www.semanticscholar.org/paper/4d54b53cff4bde9d2c03ddbb16d2edf3e9117674", "relevance": 1, "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance on various medical question-answering (QA) benchmarks, including standardized medical exams. However, correct answers alone do not ensure correct logic, and models may reach accurate conclusions through flawed processes. In this study, we introduce the MedPAIR (Medical Dataset Comparing Physicians and AI Relevance Estimation and Question Answering) dataset to evaluate how physician trainees and LLMs prioritize relevant information when answering QA questions. We obtain annotations on 1,300 QA pairs from 36 physician trainees, labeling each sentence within the question components for relevance. We compare these relevance estimates to those for LLMs, and further evaluate the impact of these\"relevant\"subsets on downstream task performance for both physician trainees and LLMs. We find that LLMs are frequently not aligned with the content relevance estimates of physician trainees. After filtering out physician trainee-labeled irrelevant sentences, accuracy improves for both the trainees and the LLMs. All LLM and physician trainee-labeled data are available at: http://medpair.csail.mit.edu/.", "citations": 4}
{"title": "Large language models in healthcare and medical domain: A review", "year": 2023, "authors": "Zabir Al Nazi, Wei Peng", "url": "https://www.semanticscholar.org/paper/d9403061e3a3392652d037138532fab111c845c1", "relevance": 1, "abstract": "The deployment of large language models (LLMs) within the healthcare sector has sparked both enthusiasm and apprehension. These models exhibit the remarkable ability to provide proficient responses to free-text queries, demonstrating a nuanced understanding of professional medical knowledge. This comprehensive survey delves into the functionalities of existing LLMs designed for healthcare applications and elucidates the trajectory of their development, starting with traditional Pretrained Language Models (PLMs) and then moving to the present state of LLMs in the healthcare sector. First, we explore the potential of LLMs to amplify the efficiency and effectiveness of diverse healthcare applications, particularly focusing on clinical language understanding tasks. These tasks encompass a wide spectrum, ranging from named entity recognition and relation extraction to natural language inference, multimodal medical applications, document classification, and question-answering. Additionally, we conduct an extensive comparison of the most recent state-of-the-art LLMs in the healthcare domain, while also assessing the utilization of various open-source LLMs and highlighting their significance in healthcare applications. Furthermore, we present the essential performance metrics employed to evaluate LLMs in the biomedical domain, shedding light on their effectiveness and limitations. Finally, we summarize the prominent challenges and constraints faced by large language models in the healthcare sector by offering a holistic perspective on their potential benefits and shortcomings. This review provides a comprehensive exploration of the current landscape of LLMs in healthcare, addressing their role in transforming medical applications and the areas that warrant further research and development.", "citations": 344}
{"title": "Towards Expert-Level Medical Question Answering with Large Language Models", "year": 2023, "authors": "K. Singhal, Tao Tu, Juraj Gottweis, R. Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, S. Pfohl, H. Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, S. Lachgar, P. A. Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, B. A. Y. Arcas, Nenad Toma\u0161ev, Yun Liu, Renee C Wong, Christopher Semturs, S. S. Mahdavi, J. Barral, D. Webster, G. Corrado, Yossi Matias, Shekoofeh Azizi, A. Karthikesalingam, Vivek Natarajan", "url": "https://www.semanticscholar.org/paper/ea72fb2a0d340f9d14fbcf300cd5f5fbbe1050bb", "relevance": 1, "abstract": "Recent artificial intelligence (AI) systems have reached milestones in\"grand challenges\"ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge. Large language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a\"passing\"score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models' answers were compared to clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach. Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19% and setting a new state-of-the-art. We also observed performance approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU clinical topics datasets. We performed detailed human evaluations on long-form questions along multiple axes relevant to clinical applications. In pairwise comparative ranking of 1066 consumer medical questions, physicians preferred Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility (p<0.001). We also observed significant improvements compared to Med-PaLM on every evaluation axis (p<0.001) on newly introduced datasets of 240 long-form\"adversarial\"questions to probe LLM limitations. While further studies are necessary to validate the efficacy of these models in real-world settings, these results highlight rapid progress towards physician-level performance in medical question answering.", "citations": 704}
{"title": "Benchmarking Retrieval-Augmented Large Language Models in Biomedical NLP: Application, Robustness, and Self-Awareness", "year": 2024, "authors": "Mingchen Li, Zaifu Zhan, Han Yang, Yongkang Xiao, Jiatan Huang, Rui Zhang", "url": "https://www.semanticscholar.org/paper/6f2f6d61687ec13e6ecd7a73ab8567b7b6bce9e1", "relevance": 1, "abstract": "Large language models (LLM) have demonstrated remarkable capabilities in various biomedical natural language processing (NLP) tasks, leveraging the demonstration within the input context to adapt to new tasks. However, LLM is sensitive to the selection of demonstrations. To address the hallucination issue inherent in LLM, retrieval-augmented LLM (RAL) offers a solution by retrieving pertinent information from an established database. Nonetheless, existing research work lacks rigorous evaluation of the impact of retrieval-augmented large language models on different biomedical NLP tasks. This deficiency makes it challenging to ascertain the capabilities of RAL within the biomedical domain. Moreover, the outputs from RAL are affected by retrieving the unlabeled, counterfactual, or diverse knowledge that is not well studied in the biomedical domain. However, such knowledge is common in the real world. Finally, exploring the self-awareness ability is also crucial for the RAL system. So, in this paper, we systematically investigate the impact of RALs on 5 different biomedical tasks (triple extraction, link prediction, classification, question answering, and natural language inference). We analyze the performance of RALs in four fundamental abilities, including unlabeled robustness, counterfactual robustness, diverse robustness, and negative awareness. To this end, we proposed an evaluation framework to assess the RALs'performance on different biomedical NLP tasks and establish four different testbeds based on the aforementioned fundamental abilities. Then, we evaluate 3 representative LLMs with 3 different retrievers on 5 tasks over 9 datasets.", "citations": 7}
{"title": "Enhancing the performance of neurosurgery medical question-answering systems using a multi-task knowledge graph-augmented answer generation model", "year": 2025, "authors": "Ting Pan, Jiang Shen, Man Xu", "url": "https://www.semanticscholar.org/paper/a9be6596e47c35ad4f3195fb7b549431e61e2fae", "relevance": 1, "abstract": "Objective Neurosurgical intelligent question-answering (Q&A) systems offers a novel paradigm to enhance perceptual intelligence\u2014simulating human-like cognitive processing for contextual understanding and emotion interaction. While retrieval-based models lack perceptual adaptability to rare clinical scenarios, and generative LLMs, despite fluency, fail to ground outputs in domain-specific neurosurgical knowledge or doctor expertise. Hybrid frameworks struggle to emulate clinician perceptual workflows (e.g., contextual prioritization, empathy modulation). These present challenges for further improving the semantic understanding, memory integration, and trustworthiness of intelligent Q&A systems in neurosurgery. Approach To address these challenges, we propose a Multi-Task Knowledge Graph-Augmented Answer Generation model (MT-KGAG), designed to enhance perceptual fidelity. It uses a hybrid attention mechanism to introduce neurosurgical knowledge graph and doctor features in the answer generation model to prioritize clinically salient information akin to human perceptual workflows. Simultaneously, the model employs a multi-task learning framework, jointly optimizing answer generation, candidate answer ranking, and doctor recommendation tasks aligning machine outputs with clinician decision-making patterns while embedding safeguards against hallucination or inappropriate emotional mimicry. Experiments utilize real-world data from a Chinese online health platform, validated through perceptual coherence metrics and ethical robustness assessments. Results The MT-KGAG model outperformed all baselines. It achieved an Embedding Average of 0.9439, DISTINCT-2 of 0.2681, and a medical entity density of 0.2471. Medical experts rated patient safety at 4.02/5 and health outcomes at 3.89/5. Additionally, it attained MRR scores of 0.6155 for candidate answer ranking and 0.6169 for doctor recommendation, confirming its multi-task synergy. Discussion MT-KGAG pioneers perception-aware AI in neurosurgery, where LLMs transcend text generation to simulate clinician-like contextual reasoning and ethical judgment. By fusing LLM\u2019s generative adaptability with domain-specific knowledge graphs, the model navigates complex trade-offs between empathetic interaction and perceptual safety\u2014delivering responses that are both contextually nuanced and ethically constrained. This work highlights the transformative potential of perceptual intelligence in medical AI, enabling systems to \u201cinterpret\u201d patient needs, \u201crecall\u201d specialized knowledge, and \u201cprioritize\u201d clinical relevance while mitigating risks of anthropomorphic overreach.", "citations": 2}
{"title": "Towards evaluating and building versatile large language models for medicine", "year": 2024, "authors": "Chaoyi Wu, Pengcheng Qiu, Jinxin Liu, Hongfei Gu, Na Li, Ya Zhang, Yanfeng Wang, Weidi Xie", "url": "https://www.semanticscholar.org/paper/0143e2a2c122af4382d5846c8e31be1528415fff", "relevance": 1, "abstract": "In this study, we present MedS-Bench, a comprehensive benchmark to evaluate large language models (LLMs) in clinical contexts, MedS-Bench, spanning 11 high-level clinical tasks. We evaluate nine leading LLMs, e.g., MEDITRON, Llama 3, Mistral, GPT-4, Claude-3.5, etc. and found that most models struggle with these complex tasks. To address these limitations, we developed MedS-Ins, a large-scale instruction-tuning dataset for medicine. MedS-Ins comprises 58 medically oriented language corpora, totaling 5M instances with 19K instructions, across 122 tasks. To demonstrate the dataset\u2019s utility, we conducted a proof-of-concept experiment by performing instruction tuning on a lightweight, open-source medical language model. The resulting model, MMedIns-Llama 3, significantly outperformed existing models on various clinical tasks. To promote further advancements, we have made MedS-Ins fully accessible and invite the research community to contribute to its expansion. Additionally, we have launched a dynamic leaderboard for MedS-Bench, to track the development progress of medical LLMs.", "citations": 46}
{"title": "HuatuoGPT, towards Taming Language Model to Be a Doctor", "year": 2023, "authors": "Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Jianquan Li, Guimin Chen, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, Xiang Wan, Benyou Wang, Haizhou Li", "url": "https://www.semanticscholar.org/paper/5459cab5dcf3c65c6b4f63b3d9f1e376f722bbcb", "relevance": 1, "abstract": "In this paper, we present HuatuoGPT, a large language model (LLM) for medical consultation. The core recipe of HuatuoGPT is to leverage both \\textit{distilled data from ChatGPT} and \\textit{real-world data from doctors} in the supervised fine-tuned stage. The responses of ChatGPT are usually detailed, well-presented and informative while it cannot perform like a doctor in many aspects, e.g. for integrative diagnosis. We argue that real-world data from doctors would be complementary to distilled data in the sense the former could tame a distilled language model to perform like doctors. To better leverage the strengths of both data, we train a reward model to align the language model with the merits that both data bring, following an RLAIF (reinforced learning from AI feedback) fashion. To evaluate and benchmark the models, we propose a comprehensive evaluation scheme (including automatic and manual metrics). Experimental results demonstrate that HuatuoGPT achieves state-of-the-art results in performing medical consultation among open-source LLMs in GPT-4 evaluation, human evaluation, and medical benchmark datasets. It is worth noting that by using additional real-world data and RLAIF, the distilled language model (i.e., HuatuoGPT) outperforms its teacher model ChatGPT in most cases. Our code, data, and models are publicly available at \\url{https://github.com/FreedomIntelligence/HuatuoGPT}. The online demo is available at \\url{https://www.HuatuoGPT.cn/}.", "citations": 311}
{"title": "ICA-RAG: Information Completeness Guided Adaptive Retrieval-Augmented Generation for Disease Diagnosis", "year": 2025, "authors": "Jiawei He, Mingyi Jia, Zhihao Jia, Junwen Duan, Yan Song, Jianxin Wang", "url": "https://www.semanticscholar.org/paper/4ea2669f234c14eb6ac2834aa380b1137bd0996c", "relevance": 1, "abstract": "Retrieval-Augmented Large Language Models, which integrate external knowledge, have shown remarkable performance in medical domains, including clinical diagnosis. However, existing RAG methods often struggle to tailor retrieval strategies to diagnostic difficulty and input sample informativeness. This limitation leads to excessive and often unnecessary retrieval, impairing computational efficiency and increasing the risk of introducing noise that can degrade diagnostic accuracy. To address this, we propose ICA-RAG (Information Completeness Guided Adaptive Retrieval-Augmented Generation), a novel framework for enhancing RAG reliability in disease diagnosis. ICA-RAG utilizes an adaptive control module to assess the necessity of retrieval based on the input's information completeness. By optimizing retrieval and incorporating knowledge filtering, ICARAG better aligns retrieval operations with clinical requirements. Experiments on three Chinese electronic medical record datasets demonstrate that ICA-RAG significantly outperforms baseline methods, highlighting its effectiveness in clinical diagnosis.", "citations": 0}
{"title": "HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge", "year": 2023, "authors": "Hao Wang, Chi-Liang Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, Ting Liu", "url": "https://www.semanticscholar.org/paper/302ee27524a717ddc21f332ca634b9211c6ec6aa", "relevance": 1, "abstract": "Large Language Models (LLMs), such as the LLaMA model, have demonstrated their effectiveness in various general-domain natural language processing (NLP) tasks. Nevertheless, LLMs have not yet performed optimally in biomedical domain tasks due to the need for medical expertise in the responses. In response to this challenge, we propose HuaTuo, a LLaMA-based model that has been supervised-fine-tuned with generated QA (Question-Answer) instances. The experimental results demonstrate that HuaTuo generates responses that possess more reliable medical knowledge. Our proposed HuaTuo model is accessible at https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese.", "citations": 274}
{"title": "MKRAG: Medical Knowledge Retrieval Augmented Generation for Medical Question Answering.", "year": 2023, "authors": "Yucheng Shi, Shaochen Xu, Zheng Liu, Tianming Liu, Xiang Li, Ninghao Liu", "url": "https://www.semanticscholar.org/paper/5c54f1f38974c896ebbf6494a2ac0d9b1ba21b33", "relevance": 1, "abstract": "Large Language Models (LLMs), although powerful in general domains, often perform poorly on domain-specific tasks such as medical question answering (QA). In addition, LLMs tend to function as \"black-boxes\", making it challenging to modify their behavior. To address the problem, our work employs a transparent process of retrieval augmented generation (RAG), aiming to improve LLM responses without the need for fine-tuning or retraining. Specifically, we propose a comprehensive retrieval strategy to extract medical facts from an external knowledge base, and then inject them into the LLM's query prompt. Focusing on medical QA, we evaluate the impact of different retrieval models and the number of facts on LLM performance using the MedQA-SMILE dataset. Notably, our retrieval-augmented Vicuna-7B model exhibited an accuracy improvement from 44.46% to 48.54%. This work underscores the potential of RAG to enhance LLM performance, offering a practical approach to mitigate the challenges posed by black-box LLMs.", "citations": 37}
{"title": "Med-U1: Incentivizing Unified Medical Reasoning in LLMs via Large-scale Reinforcement Learning", "year": 2025, "authors": "Xiaotian Zhang, Yuan Wang, Zhaopeng Feng, Ruizhe Chen, Zhijie Zhou, Yan Zhang, Hongxia Xu, Jian Wu, Zuozhu Liu", "url": "https://www.semanticscholar.org/paper/d666cdb4ce3431efff5e3769e2cbe3e3dcb1ee5c", "relevance": 1, "abstract": "Medical Question-Answering (QA) encompasses a broad spectrum of tasks, including multiple choice questions (MCQ), open-ended text generation, and complex computational reasoning. Despite this variety, a unified framework for delivering high-quality medical QA has yet to emerge. Although recent progress in reasoning-augmented large language models (LLMs) has shown promise, their ability to achieve comprehensive medical understanding is still largely unexplored. In this paper, we present Med-U1, a unified framework for robust reasoning across medical QA tasks with diverse output formats, ranging from MCQs to complex generation and computation tasks. Med-U1 employs pure large-scale reinforcement learning with mixed rule-based binary reward functions, incorporating a length penalty to manage output verbosity. With multi-objective reward optimization, Med-U1 directs LLMs to produce concise and verifiable reasoning chains. Empirical results reveal that Med-U1 significantly improves performance across multiple challenging Med-QA benchmarks, surpassing even larger specialized and proprietary models. Furthermore, Med-U1 demonstrates robust generalization to out-of-distribution (OOD) tasks. Extensive analysis presents insights into training strategies, reasoning chain length control, and reward design for medical LLMs. Our code is available here.", "citations": 9}
{"title": "SearchRAG: Can Search Engines Be Helpful for LLM-Based Medical Question Answering?", "year": 2025, "authors": "Yucheng Shi, Tianze Yang, Canyu Chen, Quanzheng Li, Tianming Liu, Xiang Li, Ninghao Liu", "url": "https://www.semanticscholar.org/paper/7fe4310e6aa17671bded252d8dec873817e624b7", "relevance": 1, "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in general domains but often struggle with tasks requiring specialized knowledge. Conventional Retrieval-Augmented Generation (RAG) techniques typically retrieve external information from static knowledge bases, which can be outdated or incomplete, missing fine-grained clinical details essential for accurate medical question answering. In this work, we propose SearchRAG, a novel framework that overcomes these limitations by leveraging real-time search engines. Our method employs synthetic query generation to convert complex medical questions into search-engine-friendly queries and utilizes uncertainty-based knowledge selection to filter and incorporate the most relevant and informative medical knowledge into the LLM's input. Experimental results demonstrate that our method significantly improves response accuracy in medical question answering tasks, particularly for complex questions requiring detailed and up-to-date knowledge. We provide our code here11https://github.com/sycny/SearchRAG.", "citations": 10}
{"title": "Evaluating large language models on a highly-specialized topic, radiation oncology physics", "year": 2023, "authors": "J. Holmes, Zheng Liu, Lian-Cheng Zhang, Yuzhen Ding, Terence T. Sio, L. Mcgee, J. Ashman, Xiang Li, Tianming Liu, Jiajian Shen, W. Liu", "url": "https://www.semanticscholar.org/paper/9ec42d155e2014e86ab49adcf76fd40a41a867ea", "relevance": 1, "abstract": "Purpose We present the first study to investigate Large Language Models (LLMs) in answering radiation oncology physics questions. Because popular exams like AP Physics, LSAT, and GRE have large test-taker populations and ample test preparation resources in circulation, they may not allow for accurately assessing the true potential of LLMs. This paper proposes evaluating LLMs on a highly-specialized topic, radiation oncology physics, which may be more pertinent to scientific and medical communities in addition to being a valuable benchmark of LLMs. Methods We developed an exam consisting of 100 radiation oncology physics questions based on our expertise. Four LLMs, ChatGPT (GPT-3.5), ChatGPT (GPT-4), Bard (LaMDA), and BLOOMZ, were evaluated against medical physicists and non-experts. The performance of ChatGPT (GPT-4) was further explored by being asked to explain first, then answer. The deductive reasoning capability of ChatGPT (GPT-4) was evaluated using a novel approach (substituting the correct answer with \u201cNone of the above choices is the correct answer.\u201d). A majority vote analysis was used to approximate how well each group could score when working together. Results ChatGPT GPT-4 outperformed all other LLMs and medical physicists, on average, with improved accuracy when prompted to explain before answering. ChatGPT (GPT-3.5 and GPT-4) showed a high level of consistency in its answer choices across a number of trials, whether correct or incorrect, a characteristic that was not observed in the human test groups or Bard (LaMDA). In evaluating deductive reasoning ability, ChatGPT (GPT-4) demonstrated surprising accuracy, suggesting the potential presence of an emergent ability. Finally, although ChatGPT (GPT-4) performed well overall, its intrinsic properties did not allow for further improvement when scoring based on a majority vote across trials. In contrast, a team of medical physicists were able to greatly outperform ChatGPT (GPT-4) using a majority vote. Conclusion This study suggests a great potential for LLMs to work alongside radiation oncology experts as highly knowledgeable assistants.", "citations": 144}
{"title": "MedMKEB: A Comprehensive Knowledge Editing Benchmark for Medical Multimodal Large Language Models", "year": 2025, "authors": "Dexuan Xu, Jieyi Wang, Zhongyan Chai, Yongzhi Cao, Hanping Wang, Huaming Zhang, Yu Huang", "url": "https://api.semanticscholar.org/CorpusId:280546596", "relevance": 1, "abstract": "Recent advances in multimodal large language models (MLLMs) have significantly improved medical AI, enabling it to unify the understanding of visual and textual information. However, as medical knowledge continues to evolve, it is critical to allow these models to efficiently update outdated or incorrect information without retraining from scratch. Although textual knowledge editing has been widely studied, there is still a lack of systematic benchmarks for multimodal medical knowledge editing involving image and text modalities. To fill this gap, we present MedMKEB, the first comprehensive benchmark designed to evaluate the reliability, generality, locality, portability, and robustness of knowledge editing in medical multimodal large language models. MedMKEB is built on a high-quality medical visual question-answering dataset and enriched with carefully constructed editing tasks, including counterfactual correction, semantic generalization, knowledge transfer, and adversarial robustness. We incorporate human expert validation to ensure the accuracy and reliability of the benchmark. Extensive single editing and sequential editing experiments on state-of-the-art general and medical MLLMs demonstrate the limitations of existing knowledge-based editing approaches in medicine, highlighting the need to develop specialized editing strategies. MedMKEB will serve as a standard benchmark to promote the development of trustworthy and efficient medical knowledge editing algorithms.", "citations": 1}
{"title": "An Empirical Evaluation of Large Language Models on Consumer Health Questions", "year": 2024, "authors": "Moaiz Abrar, Y. Sermet, Ibrahim Demir", "url": "https://www.semanticscholar.org/paper/4a49393d4faea757ebb64cc51242aa26fe4fd27a", "relevance": 1, "abstract": "Background: Large Language Models (LLMs) have demonstrated strong performances in clinical question-answering (QA) benchmarks, yet their effectiveness in addressing real-world consumer medical queries remains underexplored. This study evaluates the capabilities and limitations of LLMs in answering consumer health questions using the MedRedQA dataset, which consists of medical questions and answers by verified experts from the AskDocs subreddit. Methods: Five LLMs-GPT-4o mini, Llama 3.1-70B, Mistral-123B, Mistral-7B, and Gemini-Flash were assessed using a cross-evaluation framework. Each model generated responses to consumer queries and their outputs were evaluated by every model by comparing them with expert responses. Human evaluation was used to assess the reliability of models as evaluators. Results: GPT-4o mini achieved the highest alignment with expert responses according to four out of the five models\u2019 judges, while Mistral-7B scored the lowest according to three out of five models\u2019 judges. Overall, model responses show low alignment with expert responses. Conclusions: Current small or medium sized LLMs struggle to provide accurate answers to consumer health questions and must be significantly improved.", "citations": 5}
{"title": "A Survey on Large Language Models from General Purpose to Medical Applications: Datasets, Methodologies, and Evaluations", "year": 2024, "authors": "Jinqiang Wang, Huansheng Ning, Yi Peng, Qikai Wei, Daniel Tesfai, Wenwei Mao, Tao Zhu, Runhe Huang", "url": "https://www.semanticscholar.org/paper/9f261d49ef7e9a792761aab565a4c6fa28aded00", "relevance": 1, "abstract": "Large Language Models (LLMs) have demonstrated surprising performance across various natural language processing tasks. Recently, medical LLMs enhanced with domain-specific knowledge have exhibited excellent capabilities in medical consultation and diagnosis. These models can smoothly simulate doctor-patient dialogues and provide professional medical advice. Most medical LLMs are developed through continued training of open-source general LLMs, which require significantly fewer computational resources than training LLMs from scratch. Additionally, this approach offers better patient privacy protection than API-based solutions. Given the above advantages, this survey systematically summarizes how to train medical LLMs based on open-source general LLMs from a more fine-grained perspective. It covers (a) how to acquire training corpus and construct customized medical training sets, (b) how to choose an appropriate training paradigm, (c) how to choose a suitable evaluation benchmark, and (d) existing challenges and promising research directions are discussed. This survey can provide guidance for the development of LLMs focused on various medical applications, such as medical education, diagnostic planning, and clinical assistants. Related resources and supplemental information can be found on the GitHub repository.", "citations": 15}
{"title": "MEG: Medical Knowledge-Augmented Large Language Models for Question Answering", "year": 2024, "authors": "Laura Cabello, Carmen Martin-Turrero, Uchenna Akujuobi, Anders Sogaard, Carlos Bobed", "url": "https://www.semanticscholar.org/paper/4a77983c48cbb10a1fb2696ce4bfe8a260759f94", "relevance": 1, "abstract": "Question answering is a natural language understanding task that involves reasoning over both explicit context, and unstated relevant domain knowledge. Despite the high cost of training, large language models (LLMs) -- the backbone of most modern question-answering systems -- still struggle to reliably capture the nuanced relationships between concepts that are crucial for reasoning in specialized fields like medicine. In this work, we present MEG, a parameter-efficient approach for medical knowledge-augmented LLMs. MEG uses a lightweight mapping network to incorporate knowledge graph embeddings into the LLM, enabling it to leverage external knowledge in a cost-effective way. We evaluate our method on four popular medical multiple-choice datasets and show that LLMs i) can effectively interpret knowledge graph embeddings and ii) gain significant advantages from the factual grounding these embeddings provide. MEG attains an average of +6.7% and +9.9% accuracy over specialized models like BioMistral-7B and MediTron-7B, respectively. Finally, we show that MEG's performance remains robust to the choice of graph encoder.", "citations": 5}
{"title": "Large Language Models in Healthcare: A Comprehensive Benchmark", "year": 2024, "authors": "Andrew Liu, Hongjian Zhou, Yining Hua, Omid Rohanian, Lei A. Clifton, David A. Clifton", "url": "https://www.semanticscholar.org/paper/3d5b18c93649faffe538bda523e92e51f86f8c7f", "relevance": 1, "abstract": "The adoption of large language models (LLMs) to assist clinicians has attracted remarkable attention. Existing works mainly adopt the close-ended question-answering task with answer options for evaluation. However, in real clinical settings, many clinical decisions, such as treatment recommendations, involve answering open-ended questions without pre-set options. Meanwhile, existing studies mainly use accuracy to assess model performance. In this paper, we comprehensively benchmark diverse LLMs in healthcare, to clearly understand their strengths and weaknesses. Our benchmark contains seven tasks and thirteen datasets across medical language generation, understanding, and reasoning. We conduct a detailed evaluation of existing sixteen LLMs in healthcare under both zero-shot and few-shot (i.e., 1,3,5-shot) learning settings. We report the results on five metrics (i.e. matching, faithfulness, comprehensiveness, generalizability, and robustness) that are critical in achieving trust from clinical users. We further invite medical experts to conduct human evaluation.", "citations": 10}
{"title": "PediaBench: a comprehensive Chinese pediatric dataset for benchmarking large language models", "year": 2024, "authors": "Qian Zhang, Panfeng Chen, Jiali Li, Linkun Feng, Shuyu Liu, Heng Zhao, Mei Chen, Hui Li, Yanhao Wang", "url": "https://www.semanticscholar.org/paper/52bd5615794e1149481f610cd60dd7c06d427297", "relevance": 1, "abstract": "The emergence of Large Language Models (LLMs) in the medical domain has stressed a compelling need for standard datasets to evaluate their question-answering (QA) performance. Although there have been several benchmark datasets for medical QA, they either cover common knowledge across different departments or are specific to another department rather than pediatrics. Moreover, some of them are limited to objective questions and do not measure the generation capacity of LLMs. Therefore, they cannot comprehensively assess the QA ability of LLMs in pediatrics. To fill this gap, we construct PediaBench, the first Chinese pediatric dataset for LLM evaluation. Specifically, it contains 4,117 objective questions and 1,632 subjective questions spanning 12 pediatric disease groups. It adopts an integrated scoring criterion based on different difficulty levels to thoroughly assess the proficiency of an LLM in instruction following, knowledge understanding, clinical case analysis, etc. Finally, we validate the effectiveness of PediaBench with extensive experiments on 20 open-source and commercial LLMs. Through an in-depth analysis of experimental results, we offer insights into the ability of LLMs to answer pediatric questions in the Chinese context, highlighting their limitations for further improvements. Our code and data are published at https://github.com/ACMISLab/PediaBench.", "citations": 4}
{"title": "BanglaMedQA and BanglaMMedBench: Evaluating Retrieval-Augmented Generation Strategies for Bangla Biomedical Question Answering", "year": 2025, "authors": "Sadia Sultana, Saiyma Sittul Muna, Mosammat Zannatul Samarukh, Ajwad Abrar, T. Chowdhury", "url": "https://www.semanticscholar.org/paper/6eb540f5c8ed20b8759a567681d7f1918c3675a2", "relevance": 1, "abstract": "Developing accurate biomedical Question Answering (QA) systems in low-resource languages remains a major challenge, limiting equitable access to reliable medical knowledge. This paper introduces BanglaMedQA and BanglaMMedBench, the first large-scale Bangla biomedical Multiple Choice Question (MCQ) datasets designed to evaluate reasoning and retrieval in medical artificial intelligence (AI). The study applies and benchmarks several Retrieval-Augmented Generation (RAG) strategies, including Traditional, Zero-Shot Fallback, Agentic, Iterative Feedback, and Aggregate RAG, combining textbook-based and web retrieval with generative reasoning to improve factual accuracy. A key novelty lies in integrating a Bangla medical textbook corpus through Optical Character Recognition (OCR) and implementing an Agentic RAG pipeline that dynamically selects between retrieval and reasoning strategies. Experimental results show that the Agentic RAG achieved the highest accuracy 89.54% with openai/gpt-oss-120b, outperforming other configurations and demonstrating superior rationale quality. These findings highlight the potential of RAG-based methods to enhance the reliability and accessibility of Bangla medical QA, establishing a foundation for future research in multilingual medical artificial intelligence.", "citations": 1}
{"title": "M-QALM: A Benchmark to Assess Clinical Reading Comprehension and Knowledge Recall in Large Language Models via Question Answering", "year": 2024, "authors": "A. Subramanian, Viktor Schlegel, Abhinav Ramesh Kashyap, Thanh-Tung Nguyen, Vijay Prakash Dwivedi, Stefan Winkler", "url": "https://www.semanticscholar.org/paper/7009263033f6fdbad688c1cb010a24be3356ba25", "relevance": 1, "abstract": "There is vivid research on adapting Large Language Models (LLMs) to perform a variety of tasks in high-stakes domains such as healthcare. Despite their popularity, there is a lack of understanding of the extent and contributing factors that allow LLMs to recall relevant knowledge and combine it with presented information in the clinical and biomedical domain: a fundamental pre-requisite for success on down-stream tasks. Addressing this gap, we use Multiple Choice and Abstractive Question Answering to conduct a large-scale empirical study on 22 datasets in three generalist and three specialist biomedical sub-domains. Our multifaceted analysis of the performance of 15 LLMs, further broken down by sub-domain, source of knowledge and model architecture, uncovers success factors such as instruction tuning that lead to improved recall and comprehension. We further show that while recently proposed domain-adapted models may lack adequate knowledge, directly fine-tuning on our collected medical knowledge datasets shows encouraging results, even generalising to unseen specialist sub-domains. We complement the quantitative results with a skill-oriented manual error analysis, which reveals a significant gap between the models' capabilities to simply recall necessary knowledge and to integrate it with the presented context. To foster research and collaboration in this field we share M-QALM, our resources, standardised methodology, and evaluation results, with the research community to facilitate further advancements in clinical knowledge representation learning within language models.", "citations": 5}
{"title": "Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)", "year": 2025, "authors": "Mansi Garg, Lee-Chi Wang, Bhavesh Ghanchi, Sanjana Dumpala, Shreyash Kakde, Yen Chih Chen", "url": "https://www.semanticscholar.org/paper/b76847c6a56e7ada6d9f953418d191c5204f708b", "relevance": 1, "abstract": "This work presents a Biomedical Literature Question Answering (Q&A) system based on a Retrieval-Augmented Generation (RAG) architecture, designed to improve access to accurate, evidence-based medical information. Addressing the shortcomings of conventional health search engines and the lag in public access to biomedical research, the system integrates diverse sources, including PubMed articles, curated Q&A datasets, and medical encyclopedias ,to retrieve relevant information and generate concise, context-aware responses. The retrieval pipeline uses MiniLM-based semantic embeddings and FAISS vector search, while answer generation is performed by a fine-tuned Mistral-7B-v0.3 language model optimized using QLoRA for efficient, low-resource training. The system supports both general medical queries and domain-specific tasks, with a focused evaluation on breast cancer literature demonstrating the value of domain-aligned retrieval. Empirical results, measured using BERTScore (F1), show substantial improvements in factual consistency and semantic relevance compared to baseline models. The findings underscore the potential of RAG-enhanced language models to bridge the gap between complex biomedical literature and accessible public health knowledge, paving the way for future work on multilingual adaptation, privacy-preserving inference, and personalized medical AI systems.", "citations": 1}
{"title": "Performance of ChatGPT on American Board of Surgery In-Training Examination Preparation Questions.", "year": 2024, "authors": "C. G. Tran, Jeremy Chang, S. Sherman, James P. De Andrade", "url": "https://www.semanticscholar.org/paper/5885107d060e8cb3e4018685c6ce44b23c166972", "relevance": 1, "abstract": "", "citations": 19}
{"title": "Integrating UMLS Knowledge into Large Language Models for Medical Question Answering", "year": 2023, "authors": "Rui Yang, Edison Marrese-Taylor, Yuhe Ke, Lechao Cheng, Qingyu Chen, Irene Li", "url": "https://www.semanticscholar.org/paper/e6e8302d7d62685c7eb4fe8ed7528dafed63c46b", "relevance": 1, "abstract": "Large language models (LLMs) have demonstrated powerful text generation capabilities, bringing unprecedented innovation to the healthcare field. While LLMs hold immense promise for applications in healthcare, applying them to real clinical scenarios presents significant challenges, as these models may generate content that deviates from established medical facts and even exhibit potential biases. In our research, we develop an augmented LLM framework based on the Unified Medical Language System (UMLS), aiming to better serve the healthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as our benchmark models, and conduct automatic evaluations using the ROUGE Score and BERTScore on 104 questions from the LiveQA test set. Additionally, we establish criteria for physician-evaluation based on four dimensions: Factuality, Completeness, Readability and Relevancy. ChatGPT-3.5 is used for physician evaluation with 20 questions on the LiveQA test set. Multiple resident physicians conducted blind reviews to evaluate the generated content, and the results indicate that this framework effectively enhances the factuality, completeness, and relevance of generated content. Our research demonstrates the effectiveness of using UMLS-augmented LLMs and highlights the potential application value of LLMs in in medical question-answering.", "citations": 18}
{"title": "Agentic Medical Knowledge Graphs Enhance Medical Question Answering: Bridging the Gap Between LLMs and Evolving Medical Knowledge", "year": 2025, "authors": "Mohammad R. Rezaei, Reza Saadati Fard, Jayson L. Parker, Rahul G. Krishnan, M. Lankarany", "url": "https://www.semanticscholar.org/paper/cd63455be2671968842016fecfbfbb5371331ddc", "relevance": 1, "abstract": "Large Language Models (LLMs) have significantly advanced medical question-answering by leveraging extensive clinical data and medical literature. However, the rapid evolution of medical knowledge and the labor-intensive process of manually updating domain-specific resources pose challenges to the reliability of these systems. To address this, we introduce Agentic Medical Graph-RAG (AMG-RAG), a comprehensive framework that automates the construction and continuous updating of medical knowledge graphs, integrates reasoning, and retrieves current external evidence, such as PubMed and WikiSearch. By dynamically linking new findings and complex medical concepts, AMG-RAG not only improves accuracy but also enhances interpretability in medical queries. Evaluations on the MEDQA and MEDMCQA benchmarks demonstrate the effectiveness of AMG-RAG, achieving an F1 score of 74.1 percent on MEDQA and an accuracy of 66.34 percent on MEDMCQA, outperforming both comparable models and those 10 to 100 times larger. Notably, these improvements are achieved without increasing computational overhead, highlighting the critical role of automated knowledge graph generation and external evidence retrieval in delivering up-to-date, trustworthy medical insights.", "citations": 7}
{"title": "RGAR: Recurrence Generation-augmented Retrieval for Factual-aware Medical Question Answering", "year": 2025, "authors": "Sichu Liang, Linhai Zhang, Hongyu Zhu, Wenwen Wang, Yulan He, Deyu Zhou", "url": "https://www.semanticscholar.org/paper/40252c9440ca7450f5a15156d2116fc3981127f2", "relevance": 1, "abstract": "Medical question answering requires extensive access to specialized conceptual knowledge. The current paradigm, Retrieval-Augmented Generation (RAG), acquires expertise medical knowledge through large-scale corpus retrieval and uses this knowledge to guide a general-purpose large language model (LLM) for generating answers. However, existing retrieval approaches often overlook the importance of factual knowledge, which limits the relevance of retrieved conceptual knowledge and restricts its applicability in real-world scenarios, such as clinical decision-making based on Electronic Health Records (EHRs). This paper introduces RGAR, a recurrence generation-augmented retrieval framework that retrieves both relevant factual and conceptual knowledge from dual sources (i.e., EHRs and the corpus), allowing them to interact and refine each another. Through extensive evaluation across three factual-aware medical question answering benchmarks, RGAR establishes a new state-of-the-art performance among medical RAG systems. Notably, the Llama-3.1-8B-Instruct model with RGAR surpasses the considerably larger, RAG-enhanced GPT-3.5. Our findings demonstrate the benefit of extracting factual knowledge for retrieval, which consistently yields improved generation quality.", "citations": 8}
{"title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions", "year": 2023, "authors": "Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting Liu", "url": "https://www.semanticscholar.org/paper/1e909e2a8cdacdcdff125ebcc566f37cb869a1c8", "relevance": 1, "abstract": "The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.", "citations": 2081}
{"title": "Chatbot Based on Large Language Model to Improve Adherence to Exercise-Based Treatment in People with Knee Osteoarthritis: System Development", "year": 2025, "authors": "Humberto Far\u00edas, Joaqu\u00edn Gonz\u00e1lez Aroca, Daniel Ortiz", "url": "https://www.semanticscholar.org/paper/0965ed7216a26d9b5f39c913276a949caa1f2b77", "relevance": 1, "abstract": "Knee osteoarthritis (KOA) is a prevalent condition globally, leading to significant pain and disability, particularly in individuals over the age of 40. While exercise has been shown to reduce symptoms and improve physical function and quality of life in patients with KOA, long-term adherence to exercise programs remains a challenge due to the lack of ongoing support. To address this, a chatbot was developed using large language models (LLMs) to provide evidence-based guidance and promote adherence to treatment. A systematic review conducted under the PRISMA framework identified relevant clinical guidelines that served as the foundational knowledge base for the chatbot. The Mistral 7B model, optimized with Parameter-Efficient Fine-Tuning (PEFT) and Mixture-of-Experts (MoE) techniques, was integrated to ensure computational efficiency and mitigate hallucinations, a critical concern in medical applications. Additionally, the chatbot employs Self-Reflective Retrieval-Augmented Generation (SELF-RAG) combined with Chain of Thought (CoT) reasoning, enabling dynamic query reformulation and the generation of accurate, evidence-based responses tailored to patient needs. The chatbot was evaluated by comparing pre- and post-improvement versions and against a reference model (ChatGPT), using metrics of accuracy, relevance, and consistency. The results demonstrated significant improvements in response quality and conversational coherence, emphasizing the potential of integrating advanced LLMs with retrieval and reasoning methods to address critical challenges in healthcare. This approach not only enhances treatment adherence but also strengthens patient\u2013provider interactions in managing chronic conditions like KOA.", "citations": 2}
{"title": "A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine", "year": 2024, "authors": "Hanguang Xiao, Feizhong Zhou, X. Liu, Tianqi Liu, Zhipeng Li, Xin Liu, Xiaoxuan Huang", "url": "https://www.semanticscholar.org/paper/e06bfe725a3763500358d8f2c2f014810ea740ab", "relevance": 1, "abstract": "", "citations": 88}
{"title": "Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations Across Modalities", "year": 2025, "authors": "Anindya Bijoy Das, Shahnewaz Karim Sakib, Shibbir Ahmed", "url": "https://www.semanticscholar.org/paper/c297062d593771aa68893abbb9024b2bea8bb497", "relevance": 1, "abstract": "Large Language Models (LLMs) are increasingly applied to medical imaging tasks, including image interpretation and synthetic image generation. However, these models often produce hallucinations, which are confident but incorrect outputs that can mislead clinical decisions. This study examines hallucinations in two directions: image to text, where LLMs generate reports from X-ray, CT, or MRI scans, and text to image, where models create medical images from clinical prompts. We analyze errors such as factual inconsistencies and anatomical inaccuracies, evaluating outputs using expert informed criteria across imaging modalities. Our findings reveal common patterns of hallucination in both interpretive and generative tasks, with implications for clinical reliability. We also discuss factors contributing to these failures, including model architecture and training data. By systematically studying both image understanding and generation, this work provides insights into improving the safety and trustworthiness of LLM driven medical imaging systems.", "citations": 6}
{"title": "Trust Me, I\u2019m Wrong: LLMs Hallucinate with Certainty Despite Knowing the Answer", "year": 2025, "authors": "Adi Simhi, Itay Itzhak, Fazl Barez, Gabriel Stanovsky, Yonatan Belinkov", "url": "https://www.semanticscholar.org/paper/3fa613c2ba433871ef0322f2801f10525a10dd51", "relevance": 1, "abstract": "Prior work on large language model (LLM) hallucinations has associated them with model uncertainty or inaccurate knowledge. In this work, we define and investigate a distinct type of hallucination, where a model can consistently answer a question correctly, but a seemingly trivial perturbation, which can happen in real-world settings, causes it to produce a hallucinated response with high certainty. This phenomenon, which we dub CHOKE (Certain Hallucinations Overriding Known Evidence), is particularly concerning in high-stakes domains such as medicine or law, where model certainty is often used as a proxy for reliability. We show that CHOKE examples are consistent across prompts, occur in different models and datasets, and are fundamentally distinct from other hallucinations. This difference leads existing mitigation methods to perform worse on CHOKE examples than on general hallucinations. Finally, we introduce a probing-based mitigation that outperforms existing methods on CHOKE hallucinations. These findings reveal an overlooked aspect of hallucinations, emphasizing the need to understand their origins and improve mitigation strategies to enhance LLM safety. The code is available at https://github.com/technion-cs-nlp/Trust_me_Im_wrong .", "citations": 6}
{"title": "Detecting hallucinations in large language models using semantic entropy", "year": 2024, "authors": "Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, Yarin Gal", "url": "https://www.semanticscholar.org/paper/f82f49c20c6acc69f884f05e3a9f1ceea91061ce", "relevance": 1, "abstract": "Large language model (LLM) systems, such as ChatGPT1 or Gemini2, can show impressive reasoning and question-answering capabilities but often \u2018hallucinate\u2019 false outputs and unsubstantiated answers3,4. Answering unreliably or without the necessary information prevents adoption in diverse fields, with problems including fabrication of legal precedents5 or untrue facts in news articles6 and even posing a risk to human life in medical domains such as radiology7. Encouraging truthfulness through supervision or reinforcement has been only partially successful8. Researchers need a general method for detecting hallucinations in LLMs that works even with new and unseen questions to which humans might not know the answer. Here we develop new methods grounded in statistics, proposing entropy-based uncertainty estimators for LLMs to detect a subset of hallucinations\u2014confabulations\u2014which are arbitrary and incorrect generations. Our method addresses the fact that one idea can be expressed in many ways by computing uncertainty at the level of meaning rather than specific sequences of words. Our method works across datasets and tasks without a priori knowledge of the task, requires no task-specific data and robustly generalizes to new tasks not seen before. By detecting when a prompt is likely to produce a confabulation, our method helps users understand when they must take extra care with LLMs and opens up new possibilities for using LLMs that are otherwise prevented by their unreliability. Hallucinations (confabulations) in large language model systems can be tackled by measuring uncertainty about the meanings of generated responses rather than the text itself to improve question-answering accuracy.", "citations": 889}
{"title": "Fine-Tuning Large Language Models to Appropriately Abstain with Semantic Entropy", "year": 2024, "authors": "Benedict Aaron Tjandra, Muhammed Razzak, Jannik Kossen, Kunal Handa, Yarin Gal", "url": "https://www.semanticscholar.org/paper/3bb6f6a4cf672616bd49d8f4eb15d1b4df19972b", "relevance": 1, "abstract": "Large Language Models (LLMs) are known to hallucinate, whereby they generate plausible but inaccurate text. This phenomenon poses significant risks in critical applications, such as medicine or law, necessitating robust hallucination mitigation strategies. While recent works have proposed fine-tuning methods to teach LLMs to abstain from answering questions beyond their knowledge or capabilities, these methods rely on the existence of ground-truth labels or are limited to short-form responses. To address these limitations, we propose fine-tuning using semantic entropy, an uncertainty measure derived from introspection into the model which does not require external labels. We demonstrate that our approach matches or outperforms models fine-tuned using prior work and achieves strong performance for both short and long-form generations on a range of datasets.", "citations": 6}
{"title": "Polaris: A Safety-focused LLM Constellation Architecture for Healthcare", "year": 2024, "authors": "Subhabrata Mukherjee, Paul Gamble, Markel Sanz Ausin, Neel Kant, Kriti Aggarwal, Neha Manjunath, Debajyoti Datta, Zhengliang Liu, Jiayuan Ding, Sophia Busacca, Cezanne Bianco, Swapnil Sharma, Rae Lasko, Michelle Voisard, Sanchay Harneja, Darya Filippova, Gerry Meixiong, Kevin Cha, Amir Youssefi, Meyhaa Buvanesh, Howard Weingram, Sebastian Bierman-Lytle, Harpreet Singh Mangat, Kim Parikh, Saad Godil, Alex Miller", "url": "https://www.semanticscholar.org/paper/af0a4be045364969970c428fdc417474d7bc6ed5", "relevance": 1, "abstract": "We develop Polaris, the first safety-focused LLM constellation for real-time patient-AI healthcare conversations. Unlike prior LLM works in healthcare focusing on tasks like question answering, our work specifically focuses on long multi-turn voice conversations. Our one-trillion parameter constellation system is composed of several multibillion parameter LLMs as co-operative agents: a stateful primary agent that focuses on driving an engaging conversation and several specialist support agents focused on healthcare tasks performed by nurses to increase safety and reduce hallucinations. We develop a sophisticated training protocol for iterative co-training of the agents that optimize for diverse objectives. We train our models on proprietary data, clinical care plans, healthcare regulatory documents, medical manuals, and other medical reasoning documents. We align our models to speak like medical professionals, using organic healthcare conversations and simulated ones between patient actors and experienced nurses. This allows our system to express unique capabilities such as rapport building, trust building, empathy and bedside manner. Finally, we present the first comprehensive clinician evaluation of an LLM system for healthcare. We recruited over 1100 U.S. licensed nurses and over 130 U.S. licensed physicians to perform end-to-end conversational evaluations of our system by posing as patients and rating the system on several measures. We demonstrate Polaris performs on par with human nurses on aggregate across dimensions such as medical safety, clinical readiness, conversational quality, and bedside manner. Additionally, we conduct a challenging task-based evaluation of the individual specialist support agents, where we demonstrate our LLM agents significantly outperform a much larger general-purpose LLM (GPT-4) as well as from its own medium-size class (LLaMA-2 70B).", "citations": 42}
{"title": "Classification of Hallucinations in Large Language Models Using a Novel Weighted Metric", "year": 2024, "authors": "Saaketh Raghava", "url": "https://www.semanticscholar.org/paper/9c734f07f2c3511d1abbb0dfa3ba04646fa6f2bb", "relevance": 1, "abstract": "As Large Language Models (LLMs) find increasing use in important fields such as healthcare, finance, and law, ensuring their accuracy and reliability is critical. One significant challenge is the occurrence of \u201challucinations,\u201d where these models produce nonsensical or incorrect information. This paper introduces a new framework designed to identify and categorize hallucinations in the outputs of LLMs, particularly in safety-sensitive applications. We present a detailed system that classifies hallucinations into four categories: Factual Errors, Speculative Responses, Logical Fallacies, and Improbable Scenarios. Our methodology employs a scoring system that combines metrics to offer a clearer picture of the model performance. Using the TruthfulQA dataset, and the Falcon 7B model, we analyze different types of hallucinations and their potential to compromise decision making in safety critical domains. By focusing on clarity and accuracy, this framework aims to improve the safety and reliability of LLMs in high stakes situations and sets the stage for more effective validation methods in artificial intelligence.", "citations": 0}
{"title": "MKA: Leveraging Cross-Lingual Consensus for Model Abstention", "year": 2025, "authors": "Sharad Duwal", "url": "https://www.semanticscholar.org/paper/0dceac64363ca79ca8b84c7313d09b6fcd520486", "relevance": 1, "abstract": "Reliability of LLMs is questionable even as they get better at more tasks. A wider adoption of LLMs is contingent on whether they are usably factual. And if they are not, on whether they can properly calibrate their confidence in their responses. This work focuses on utilizing the multilingual knowledge of an LLM to inform its decision to abstain or answer when prompted. We develop a multilingual pipeline to calibrate the model's confidence and let it abstain when uncertain. We run several multilingual models through the pipeline to profile them across different languages. We find that the performance of the pipeline varies by model and language, but that in general they benefit from it. This is evidenced by the accuracy improvement of $71.2\\%$ for Bengali over a baseline performance without the pipeline. Even a high-resource language like English sees a $15.5\\%$ improvement. These results hint at possible further improvements.", "citations": 1}
{"title": "DiagGPT: An LLM-based and Multi-agent Dialogue System with Automatic Topic Management for Flexible Task-Oriented Dialogue", "year": 2023, "authors": "Lang Cao", "url": "https://www.semanticscholar.org/paper/f8518829bfd945e7023646329e0836ffd7b0ed39", "relevance": 1, "abstract": "A significant application of Large Language Models (LLMs), like ChatGPT, is their deployment as chat agents, which respond to human inquiries across a variety of domains. While current LLMs proficiently answer general questions, they often fall short in complex diagnostic scenarios such as legal, medical, or other specialized consultations. These scenarios typically require Task-Oriented Dialogue (TOD), where an AI chat agent must proactively pose questions and guide users toward specific goals or task completion. Previous fine-tuning models have underperformed in TOD and the full potential of conversational capability in current LLMs has not yet been fully explored. In this paper, we introduce DiagGPT (Dialogue in Diagnosis GPT), an innovative approach that extends LLMs to more TOD scenarios. In addition to guiding users to complete tasks, DiagGPT can effectively manage the status of all topics throughout the dialogue development. This feature enhances user experience and offers a more flexible interaction in TOD. Our experiments demonstrate that DiagGPT exhibits outstanding performance in conducting TOD with users, showing its potential for practical applications in various fields.", "citations": 6}
{"title": "Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI", "year": 2023, "authors": "Muhammad Aurangzeb Ahmad, Ilker Yaramis, Taposh Dutta Roy", "url": "https://www.semanticscholar.org/paper/be2b0125f3161739c685e9f86d9fd49f9f6d99c8", "relevance": 1, "abstract": "Large language models have proliferated across multiple domains in as short period of time. There is however hesitation in the medical and healthcare domain towards their adoption because of issues like factuality, coherence, and hallucinations. Give the high stakes nature of healthcare, many researchers have even cautioned against its usage until these issues are resolved. The key to the implementation and deployment of LLMs in healthcare is to make these models trustworthy, transparent (as much possible) and explainable. In this paper we describe the key elements in creating reliable, trustworthy, and unbiased models as a necessary condition for their adoption in healthcare. Specifically we focus on the quantification, validation, and mitigation of hallucinations in the context in healthcare. Lastly, we discuss how the future of LLMs in healthcare may look like.", "citations": 65}
{"title": "Shadows in the Attention: Contextual Perturbation and Representation Drift in the Dynamics of Hallucination in LLMs", "year": 2025, "authors": "Zeyu Wei, Shuo Wang, Xiaohui Rong, Xuemin Liu, He Li", "url": "https://www.semanticscholar.org/paper/279eeb6f1aeb1b6754018b84f752317329266756", "relevance": 1, "abstract": "Hallucinations -- plausible yet erroneous outputs -- remain a critical barrier to reliable deployment of large language models (LLMs). We present the first systematic study linking hallucination incidence to internal-state drift induced by incremental context injection. Using TruthfulQA, we construct two 16-round\"titration\"tracks per question: one appends relevant but partially flawed snippets, the other injects deliberately misleading content. Across six open-source LLMs, we track overt hallucination rates with a tri-perspective detector and covert dynamics via cosine, entropy, JS and Spearman drifts of hidden states and attention maps. Results reveal (1) monotonic growth of hallucination frequency and representation drift that plateaus after 5--7 rounds; (2) relevant context drives deeper semantic assimilation, producing high-confidence\"self-consistent\"hallucinations, whereas irrelevant context induces topic-drift errors anchored by attention re-routing; and (3) convergence of JS-Drift ($\\sim0.69$) and Spearman-Drift ($\\sim0$) marks an\"attention-locking\"threshold beyond which hallucinations solidify and become resistant to correction. Correlation analyses expose a seesaw between assimilation capacity and attention diffusion, clarifying size-dependent error modes. These findings supply empirical foundations for intrinsic hallucination prediction and context-aware mitigation mechanisms.", "citations": 9}
{"title": "Public Health Risk Management, Policy, and Ethical Imperatives in the Use of AI Tools for Mental Health Therapy", "year": 2025, "authors": "Francis C Ohu, D. Burrell, Laura A Jones", "url": "https://www.semanticscholar.org/paper/523540abcc0159bfc2d833c4d11ae8210e9f1ad4", "relevance": 1, "abstract": "Background: The deployment of large language models (LLMs) in mental health therapy presents a compelling yet deeply fraught opportunity to address widespread disparities in access to psychological care. Recent empirical evidence reveals that these AI systems exhibit substantial shortcomings when confronted with complex clinical contexts. Methods: This paper synthesizes key findings from a critical analysis of LLMs operating in therapeutic roles and argues for the urgent establishment of comprehensive risk management frameworks, policy interventions, and ethical protocols governing their use. Results: LLMs tested in simulated therapeutic settings frequently exhibited stigmatizing attitudes toward mental health conditions and responded inappropriately to acute clinical symptoms such as suicidal ideation, psychosis, and delusions. Real-world evaluations reinforce these concerns. Some studies found that therapy and companion bots endorsed unsafe or harmful suggestions in adolescent crisis vignettes, while others reported inadequate chatbot responses to self-harm and sexual assault queries, prompting concern from clinicians, disappointment from patients, and calls for stronger oversight from policymakers. These failures contravene fundamental principles of safe clinical practice, including non-maleficence, therapeutic alliance, and evidence-based care. Moreover, LLMs lack the emotional intelligence, contextual grounding, and ethical accountability that underpin the professional responsibilities of human therapists. Their propensity for sycophantic or non-directive responses, driven by alignment objectives rather than clinical efficacy, further undermines their therapeutic utility. Conclusions: This analysis highlights barriers to the replacement of human therapists with autonomous AI systems. It also calls attention to the regulatory vacuum surrounding LLM-based wellness and therapy applications, many of which are widely accessible and unvetted. Recommendations include professional standards, transparency in training and deployment, robust privacy protections, and clinician oversight. The findings underscore the need to redefine AI as supportive, not substitutive.", "citations": 2}
{"title": "Listening to Patients: A Framework of Detecting and Mitigating Patient Misreport for Medical Dialogue Generation", "year": 2024, "authors": "Lang Qin, Yao Zhang, Hongru Liang, Adam Jatowt, Zhenglu Yang", "url": "https://www.semanticscholar.org/paper/d9a09efe7acb3bf509c66283332b935a02163dd9", "relevance": 1, "abstract": "Medical Dialogue Systems aim to provide automated healthcare support through patient-agent conversations. Previous efforts typically regard patients as ideal users -- one who accurately and consistently reports their health conditions. However, in reality, patients often misreport their symptoms, leading to discrepancies between their reports and actual health conditions. Overlooking patient misreport will affect the quality of healthcare consultations provided by MDS. To address this issue, we argue that MDS should ''listen to patients'' and tackle two key challenges: how to detect and mitigate patient misreport effectively. In this work, we propose PaMis, a framework of detecting and mitigating Patient Misreport for medical dialogue generation. PaMis first constructs dialogue entity graphs, then detects patient misreport based on graph entropy, and mitigates patient misreport by formulating clarifying questions. Experiments indicate that PaMis effectively enhances medical response generation, enabling models like GPT-4 to detect and mitigate patient misreports, and provide high-quality healthcare assistance.", "citations": 2}
{"title": "Loki's Dance of Illusions: A Comprehensive Survey of Hallucination in Large Language Models", "year": 2025, "authors": "Chaozhuo Li, Pengbo Wang, Chenxu Wang, Litian Zhang, Zheng Liu, Qiwei Ye, Yuanbo Xu, Feiran Huang, Xi Zhang, Philip S. Yu", "url": "https://www.semanticscholar.org/paper/4d608203639087e0fe3c5d2b7a374941dd182cb7", "relevance": 1, "abstract": "Edgar Allan Poe noted,\"Truth often lurks in the shadow of error,\"highlighting the deep complexity intrinsic to the interplay between truth and falsehood, notably under conditions of cognitive and informational asymmetry. This dynamic is strikingly evident in large language models (LLMs). Despite their impressive linguistic generation capabilities, LLMs sometimes produce information that appears factually accurate but is, in reality, fabricated, an issue often referred to as'hallucinations'. The prevalence of these hallucinations can mislead users, affecting their judgments and decisions. In sectors such as finance, law, and healthcare, such misinformation risks causing substantial economic losses, legal disputes, and health risks, with wide-ranging consequences.In our research, we have methodically categorized, analyzed the causes, detection methods, and solutions related to LLM hallucinations. Our efforts have particularly focused on understanding the roots of hallucinations and evaluating the efficacy of current strategies in revealing the underlying logic, thereby paving the way for the development of innovative and potent approaches. By examining why certain measures are effective against hallucinations, our study aims to foster a comprehensive approach to tackling this issue within the domain of LLMs.", "citations": 13}
{"title": "LLM-MedQA: Enhancing Medical Question Answering through Case Studies in Large Language Models", "year": 2024, "authors": "Hang Yang, Hao Chen, Hui Guo, Yineng Chen, Chingsheng Lin, Shuangyan Hu, Jinrong Hu, Xi Wu, Xin Wang", "url": "https://www.semanticscholar.org/paper/5154b92199ddafdbbb23abcbc5c42fc6a342bb88", "relevance": 1, "abstract": "Accurate and efficient question-answering systems are essential for high-quality patient care in the medical field. While Large Language Models (LLMs) have made remarkable strides across various domains, they still face challenges in medical question answering, particularly in understanding domain-specific terminology and performing complex reasoning, limiting their effectiveness in critical applications. To address this, we propose a multi-agent medical question-answering (MedQA) system incorporating similar case generation. We leverage the Llama3.1:70B model in a multi-agent architecture to enhance enhance zero-shot classification on the MedQA dataset, utilizing the model\u2019s inherent medical knowledge and reasoning capabilities without additional training data. Experimental results show substantial gains over existing benchmark models, with improvements of 7% in both accuracy and F1-score across various medical QA tasks. Furthermore, we examine the model\u2019s interpretability and reliability in addressing complex medical queries. This research not only offers a robust solution for medical question answering but also establishes a foundation for broader applications of LLMs in the medical domain.", "citations": 27}
{"title": "MedHallTune: An Instruction-Tuning Benchmark for Mitigating Medical Hallucination in Vision-Language Models", "year": 2025, "authors": "Qiao Yan, Yuchen Yuan, Xiaowei Hu, Yihan Wang, Jiaqi Xu, Jinpeng Li, Chi-Wing Fu, Pheng-Ann Heng", "url": "https://www.semanticscholar.org/paper/57eaef15e8858260189ee3fa050ea2677e0eb9ed", "relevance": 1, "abstract": "The increasing use of vision-language models (VLMs) in healthcare applications presents great challenges related to hallucinations, in which the models may generate seemingly plausible results that are in fact incorrect. Such hallucinations can jeopardize clinical decision making, potentially harming the diagnosis and treatments. In this work, we propose MedHallTune, a large-scale benchmark designed specifically to evaluate and mitigate hallucinations in medical VLMs. Comprising over 100,000 images and 1,000,000 instruction pairs, MedHallTune includes both hallucination and non-hallucination samples, each with ground-truth annotations. We conduct a comprehensive evaluation of current medical and general VLMs using MedHallTune, assessing their performance across key metrics, including clinical accuracy, relevance, detail level, and risk level. The experimental results show that fine-tuning with MedHallTune successfully improves the ability of several existing models to manage hallucinations and boost their zero-shot performance on downstream visual-question-answering (VQA) tasks, making them more reliable for practical medical applications. Our work contributes to the development of more trustworthy VLMs. Codes and dataset will be available at \\href{https://github.com/russellyq/MedHallTune}{MedHallTune}.", "citations": 2}
{"title": "Between reality and delusion: challenges of applying large language models to companion robots for open-domain dialogues with older adults", "year": 2025, "authors": "Bahar Irfan, Sanna Kuoppam\u00e4ki, Aida Hosseini, Gabriel Skantze", "url": "https://www.semanticscholar.org/paper/49e88f9671a3d4601acaaee920b345602ee581d3", "relevance": 1, "abstract": "Throughout our lives, we interact daily in conversations with our friends and family, covering a wide range of topics, known as open-domain dialogue. As we age, these interactions may diminish due to changes in social and personal relationships, leading to loneliness in older adults. Conversational companion robots can alleviate this issue by providing daily social support. Large language models (LLMs) offer flexibility for enabling open-domain dialogue in these robots. However, LLMs are typically trained and evaluated on textual data, while robots introduce additional complexity through multi-modal interactions, which has not been explored in prior studies. Moreover, it is crucial to involve older adults in the development of robots to ensure alignment with their needs and expectations. Correspondingly, using iterative participatory design approaches, this paper exposes the challenges of integrating LLMs into conversational robots, deriving from 34 Swedish-speaking older adults\u2019 (one-to-one) interactions with a personalized companion robot, built on Furhat robot with GPT-\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$-$$\\end{document}3.5. These challenges encompass disruptions in conversations, including frequent interruptions, slow, repetitive, superficial, incoherent, and disengaging responses, language barriers, hallucinations, and outdated information, leading to frustration, confusion, and worry among older adults. Drawing on insights from these challenges, we offer recommendations to enhance the integration of LLMs into conversational robots, encompassing both general suggestions and those tailored to companion robots for older adults.", "citations": 51}
{"title": "Enhancing Healthcare Through Large Language Models: A Study on Medical Question Answering", "year": 2024, "authors": "Haoran Yu, Chang Yu, Zihan Wang, Dongxian Zou, Hao Qin", "url": "https://www.semanticscholar.org/paper/e50798817acb71c91b63e5f49769cf4dcb7786ca", "relevance": 1, "abstract": "In recent years, the application of Large Language Models (LLMs) in healthcare has shown significant promise in improving the accessibility and dissemination of medical knowledge. This paper presents a detailed study of various LLMs trained on the MedQuAD medical question-answering dataset, with a focus on identifying the most effective model for providing accurate medical information. Among the models tested, the Sentence-t5 combined with Mistral 7B demonstrated superior performance, achieving a precision score of 0.762. This model's enhanced capabilities are attributed to its advanced pretraining techniques, robust architecture, and effective prompt construction methodologies. By leveraging these strengths, the Sentence-t5 + Mistral 7B model excels in understanding and generating precise medical answers. Our findings highlight the potential of integrating sophisticated LLMs in medical contexts to facilitate efficient and accurate medical knowledge retrieval, thus significantly enhancing patient education and support.", "citations": 42}
{"title": "Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation", "year": 2025, "authors": "Jiahao Cheng, Tiancheng Su, Jia Yuan, Guoxiu He, Jiawei Liu, Xinqi Tao, Jingwen Xie, Huaxia Li", "url": "https://www.semanticscholar.org/paper/163ce5ee3ee159c1e2e5c39d64f36ca7f9a528ea", "relevance": 1, "abstract": "Large Language Models (LLMs) often exhibit \\textit{hallucinations}, generating factually incorrect or semantically irrelevant content in response to prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by encouraging step-by-step reasoning, but its impact on hallucination detection remains underexplored. To bridge this gap, we conduct a systematic empirical evaluation. We begin with a pilot experiment, revealing that CoT reasoning significantly affects the LLM's internal states and token probability distributions. Building on this, we evaluate the impact of various CoT prompting methods on mainstream hallucination detection methods across both instruction-tuned and reasoning-oriented LLMs. Specifically, we examine three key dimensions: changes in hallucination score distributions, variations in detection accuracy, and shifts in detection confidence. Our findings show that while CoT prompting helps reduce hallucination frequency, it also tends to obscure critical signals used for detection, impairing the effectiveness of various detection methods. Our study highlights an overlooked trade-off in the use of reasoning. Code is publicly available at: https://github.com/ECNU-Text-Computing/cot-hallu-detect .", "citations": 11}
{"title": "Med42 - Evaluating Fine-Tuning Strategies for Medical LLMs: Full-Parameter vs. Parameter-Efficient Approaches", "year": 2024, "authors": "Cl'ement Christophe, P. Kanithi, Prateek Munjal, Tathagata Raha, Nasir Hayat, Ronnie Rajan, Ahmed Al-Mahrooqi, Avani Gupta, Muhammad Umar Salman, Gurpreet Gosal, Bhargav Kanakiya, Charles Chen, N. Vassilieva, B. Amor, Marco A. F. Pimentel, Shadab Khan", "url": "https://api.semanticscholar.org/CorpusId:269302939", "relevance": 1, "abstract": "This study presents a comprehensive analysis and comparison of two predominant fine-tuning methodologies - full-parameter fine-tuning and parameter-efficient tuning - within the context of medical Large Language Models (LLMs). We developed and refined a series of LLMs, based on the Llama-2 architecture, specifically designed to enhance medical knowledge retrieval, reasoning, and question-answering capabilities. Our experiments systematically evaluate the effectiveness of these tuning strategies across various well-known medical benchmarks. Notably, our medical LLM Med42 showed an accuracy level of 72% on the US Medical Licensing Examination (USMLE) datasets, setting a new standard in performance for openly available medical LLMs. Through this comparative analysis, we aim to identify the most effective and efficient method for fine-tuning LLMs in the medical domain, thereby contributing significantly to the advancement of AI-driven healthcare applications.", "citations": 66}
{"title": "MedExQA: Medical Question Answering Benchmark with Multiple Explanations", "year": 2024, "authors": "Yunsoo Kim, Jinge Wu, Yusuf Abdulle, Honghan Wu", "url": "https://www.semanticscholar.org/paper/2cd41895b6baa722af1f27b80b12a7b373ec5bf9", "relevance": 1, "abstract": "This paper introduces MedExQA, a novel benchmark in medical question-answering, to evaluate large language models\u2019 (LLMs) understanding of medical knowledge through explanations. By constructing datasets across five distinct medical specialties that are underrepresented in current datasets and further incorporating multiple explanations for each question-answer pair, we address a major gap in current medical QA benchmarks which is the absence of comprehensive assessments of LLMs\u2019 ability to generate nuanced medical explanations. Our work highlights the importance of explainability in medical LLMs, proposes an effective methodology for evaluating models beyond classification accuracy, and sheds light on one specific domain, speech language pathology, where current LLMs including GPT4 lack good understanding. Our results show generation evaluation with multiple explanations aligns better with human assessment, highlighting an opportunity for a more robust automated comprehension assessment for LLMs. To diversify open-source medical LLMs (currently mostly based on Llama2), this work also proposes a new medical model, MedPhi-2, based on Phi-2 (2.7B). The model outperformed medical LLMs based on Llama2-70B in generating explanations, showing its effectiveness in the resource-constrained medical domain. We will share our benchmark datasets and the trained model.", "citations": 58}
{"title": "Fine-Tuning LLMs for Reliable Medical Question-Answering Services", "year": 2024, "authors": "Ali Anaissi, Ali Braytee, Junaid Akram", "url": "https://www.semanticscholar.org/paper/dabcffe4b6eabd55d7a8228611f1ff1b8d6f5b63", "relevance": 1, "abstract": "We present an advanced approach to medical question-answering (QA) services, using fine-tuned Large Language Models (LLMs) to improve the accuracy and reliability of healthcare information. Our study focuses on optimizing models like LLaMA-2 and Mistral, which have shown great promise in delivering precise, reliable medical answers. By leveraging comprehensive datasets, we applied fine-tuning techniques such as rsDoRA+ and ReRAG. rsDoRA+ enhances model performance through a combination of decomposed model weights, varied learning rates for low-rank matrices, and rank stabilization, leading to improved efficiency. ReRAG, which integrates retrieval on demand and question rewriting, further refines the accuracy of the responses. This approach enables healthcare providers to access fast, dependable information, aiding in more efficient decision-making and fostering greater patient trust. Our work highlights the potential of fine-tuned LLMs to significantly improve the quality and accessibility of medical information services, ultimately contributing to better healthcare outcomes for all.", "citations": 7}
{"title": "BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains", "year": 2024, "authors": "Yanis Labrak, Adrien Bazoge, Emmanuel Morin, P. Gourraud, Mickael Rouvier, Richard Dufour", "url": "https://www.semanticscholar.org/paper/13b8934468665ecb586f491d7f9f6c460cb095e5", "relevance": 1, "abstract": "Large Language Models (LLMs) have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source LLMs tailored for health contexts, adapting general-purpose LLMs to the medical domain presents significant challenges. In this paper, we introduce BioMistral, an open-source LLM tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central. We conduct a comprehensive evaluation of BioMistral on a benchmark comprising 10 established medical question-answering (QA) tasks in English. We also explore lightweight models obtained through quantization and model merging approaches. Our results demonstrate BioMistral's superior performance compared to existing open-source medical models and its competitive edge against proprietary counterparts. Finally, to address the limited availability of data beyond English and to assess the multilingual generalization of medical LLMs, we automatically translated and evaluated this benchmark into 7 other languages. This marks the first large-scale multilingual evaluation of LLMs in the medical domain. Datasets, multilingual evaluation benchmarks, scripts, and all the models obtained during our experiments are freely released.", "citations": 390}
{"title": "LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day", "year": 2023, "authors": "Chunyuan Li, Cliff Wong, Sheng Zhang, N. Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, Jianfeng Gao", "url": "https://www.semanticscholar.org/paper/f22d71c7ce9720ba1f717a4f1181488200e78198", "relevance": 1, "abstract": "Conversational generative AI has demonstrated remarkable promise for empowering biomedical practitioners, but current investigations focus on unimodal text. Multimodal conversational AI has seen rapid progress by leveraging billions of image-text pairs from the public web, but such general-domain vision-language models still lack sophistication in understanding and conversing about biomedical images. In this paper, we propose a cost-efficient approach for training a vision-language conversational assistant that can answer open-ended research questions of biomedical images. The key idea is to leverage a large-scale, broad-coverage biomedical figure-caption dataset extracted from PubMed Central, use GPT-4 to self-instruct open-ended instruction-following data from the captions, and then fine-tune a large general-domain vision-language model using a novel curriculum learning method. Specifically, the model first learns to align biomedical vocabulary using the figure-caption pairs as is, then learns to master open-ended conversational semantics using GPT-4 generated instruction-following data, broadly mimicking how a layperson gradually acquires biomedical knowledge. This enables us to train a Large Language and Vision Assistant for BioMedicine (LLaVA-Med) in less than 15 hours (with eight A100s). LLaVA-Med exhibits excellent multimodal conversational capability and can follow open-ended instruction to assist with inquiries about a biomedical image. On three standard biomedical visual question answering datasets, LLaVA-Med outperforms previous supervised state-of-the-art on certain metrics. To facilitate biomedical multimodal research, we will release our instruction-following data and the LLaVA-Med model.", "citations": 1382}
{"title": "MedAlpaca -- An Open-Source Collection of Medical Conversational AI Models and Training Data", "year": 2023, "authors": "T. Han, L. Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander L\u00f6ser, D. Truhn, K. Bressem", "url": "https://www.semanticscholar.org/paper/90e41626b8c78600da70c4350c67c3a10525cb37", "relevance": 1, "abstract": "As large language models (LLMs) like OpenAI's GPT series continue to make strides, we witness the emergence of artificial intelligence applications in an ever-expanding range of fields. In medicine, these LLMs hold considerable promise for improving medical workflows, diagnostics, patient care, and education. Yet, there is an urgent need for open-source models that can be deployed on-premises to safeguard patient privacy. In our work, we present an innovative dataset consisting of over 160,000 entries, specifically crafted to fine-tune LLMs for effective medical applications. We investigate the impact of fine-tuning these datasets on publicly accessible pre-trained LLMs, and subsequently, we juxtapose the performance of pre-trained-only models against the fine-tuned models concerning the examinations that future medical doctors must pass to achieve certification.", "citations": 405}
{"title": "One LLM is not Enough: Harnessing the Power of Ensemble Learning for Medical Question Answering", "year": 2023, "authors": "Han Yang, Mingchen Li, Huixue Zhou, Yongkang Xiao, Qian Fang, Rui Zhang", "url": "https://www.semanticscholar.org/paper/c7d9b2c5759b1206320f58cc13311c5da05981bd", "relevance": 1, "abstract": "Objective: To enhance the accuracy and reliability of diverse medical question-answering (QA) tasks and investigate efficient approaches deploying the Large Language Models (LLM) technologies, We developed a novel ensemble learning pipeline by utilizing state-of-the-art LLMs, focusing on improving performance on diverse medical QA datasets. Materials and Methods: Our study employs three medical QA datasets: PubMedQA, MedQA-USMLE, and MedMCQA, each presenting unique challenges in biomedical question-answering. The proposed LLM-Synergy framework, focusing exclusively on zero-shot cases using LLMs, incorporates two primary ensemble methods. The first is a Boosting-based weighted majority vote ensemble, where decision-making is expedited and refined by assigning variable weights to different LLMs through a boosting algorithm. The second method is Cluster-based Dynamic Model Selection, which dynamically selects the most suitable LLM votes for each query, based on the characteristics of question contexts, using a clustering approach. Results: The Majority Weighted Vote and Dynamic Model Selection methods demonstrate superior performance compared to individual LLMs across three medical QA datasets. Specifically, the accuracies are 35.84%, 96.21%, and 37.26% for MedMCQA, PubMedQA, and MedQA-USMLE, respectively, with the Majority Weighted Vote. Correspondingly, the Dynamic Model Selection yields slightly higher accuracies of 38.01%, 96.36%, and 38.13%. Conclusion: The LLM-Synergy framework with two ensemble methods, represents a significant advancement in leveraging LLMs for medical QA tasks and provides an innovative way of efficiently utilizing the development with LLM Technologies, customing for both existing and potentially future challenge tasks in biomedical and health informatics research.", "citations": 51}
{"title": "Benchmarking the Confidence of Large Language Models in Answering Clinical Questions: Cross-Sectional Evaluation Study", "year": 2025, "authors": "Mahmud Omar, R. Agbareia, Benjamin S. Glicksberg, G. Nadkarni, Eyal Klang", "url": "https://www.semanticscholar.org/paper/166389fd2516759366cb4b8c6b23571ec5632685", "relevance": 1, "abstract": "Abstract Background The capabilities of large language models (LLMs) to self-assess their own confidence in answering questions within the biomedical realm remain underexplored. Objective This study evaluates the confidence levels of 12 LLMs across 5 medical specialties to assess LLMs\u2019 ability to accurately judge their own responses. Methods We used 1965 multiple-choice questions that assessed clinical knowledge in the following areas: internal medicine, obstetrics and gynecology, psychiatry, pediatrics, and general surgery. Models were prompted to provide answers and to also provide their confidence for the correct answers (score: range 0%\u2010100%). We calculated the correlation between each model\u2019s mean confidence score for correct answers and the overall accuracy of each model across all questions. The confidence scores for correct and incorrect answers were also analyzed to determine the mean difference in confidence, using 2-sample, 2-tailed t tests. Results The correlation between the mean confidence scores for correct answers and model accuracy was inverse and statistically significant (r=\u22120.40; P=.001), indicating that worse-performing models exhibited paradoxically higher confidence. For instance, a top-performing model\u2014GPT-4o\u2014had a mean accuracy of 74% (SD 9.4%), with a mean confidence of 63% (SD 8.3%), whereas a low-performing model\u2014Qwen2-7B\u2014showed a mean accuracy of 46% (SD 10.5%) but a mean confidence of 76% (SD 11.7%). The mean difference in confidence between correct and incorrect responses was low for all models, ranging from 0.6% to 5.4%, with GPT-4o having the highest mean difference (5.4%, SD 2.3%; P=.003). Conclusions Better-performing LLMs show more aligned overall confidence levels. However, even the most accurate models still show minimal variation in confidence between right and wrong answers. This may limit their safe use in clinical settings. Addressing overconfidence could involve refining calibration methods, performing domain-specific fine-tuning, and involving human oversight when decisions carry high risks. Further research is needed to improve these strategies before broader clinical adoption of LLMs.", "citations": 19}
{"title": "Collaboration among Multiple Large Language Models for Medical Question Answering", "year": 2025, "authors": "Kexin Shang, Chia-Hsuan Chang, Christopher C. Yang", "url": "https://www.semanticscholar.org/paper/ab4b03eb4e4e62c10e0ea967cbddbe8802401917", "relevance": 1, "abstract": "Empowered by vast internal knowledge reservoir, the new generation of large language models (LLMs) demonstrate untapped potential to tackle medical tasks. However, there is insufficient effort made towards summoning up a synergic effect from multiple LLMs\u2019 expertise and background. In this study, we propose a multi-LLM collaboration framework tailored on a medical multiple-choice questions dataset. Through post-hoc analysis on 3 pre-trained LLM participants, our framework is proved to boost all LLMs reasoning ability as well as alleviate their divergence among questions. We also measure an LLM\u2019s confidence when it confronts with adversary opinions from other LLMs and observe a concurrence between LLM\u2019s confidence and prediction accuracy.", "citations": 1}
{"title": "LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation", "year": 2025, "authors": "Ming Zhang, Yujiong Shen, Zelin Li, Huayu Sha, Binze Hu, Yuhui Wang, Chenhao Huang, Shichun Liu, Jingqi Tong, Changhao Jiang, Mingxu Chai, Zhiheng Xi, Shihan Dou, Tao Gui, Qi Zhang, Xuanjing Huang", "url": "https://www.semanticscholar.org/paper/799dcc2efd788f80770b6e829202642c5ca7e742", "relevance": 1, "abstract": "Evaluating large language models (LLMs) in medicine is crucial because medical applications require high accuracy with little room for error. Current medical benchmarks have three main types: medical exam-based, comprehensive medical, and specialized assessments. However, these benchmarks have limitations in question design (mostly multiple-choice), data sources (often not derived from real clinical scenarios), and evaluation methods (poor assessment of complex reasoning). To address these issues, we present LLMEval-Med, a new benchmark covering five core medical areas, including 2,996 questions created from real-world electronic health records and expert-designed clinical scenarios. We also design an automated evaluation pipeline, incorporating expert-developed checklists into our LLM-as-Judge framework. Furthermore, our methodology validates machine scoring through human-machine agreement analysis, dynamically refining checklists and prompts based on expert feedback to ensure reliability. We evaluate 13 LLMs across three categories (specialized medical models, open-source models, and closed-source models) on LLMEval-Med, providing valuable insights for the safe and effective deployment of LLMs in medical domains. The dataset is released in https://github.com/llmeval/LLMEval-Med.", "citations": 7}
{"title": "Large Language Models for Multi-Choice Question Classification of Medical Subjects", "year": 2024, "authors": "V'ictor Ponce-L'opez", "url": "https://www.semanticscholar.org/paper/417e3392836203defd172a20ab47a21310789b85", "relevance": 1, "abstract": "The aim of this paper is to evaluate whether large language models trained on multi-choice question data can be used to discriminate between medical subjects. This is an important and challenging task for automatic question answering. To achieve this goal, we train deep neural networks for multi-class classification of questions into the inferred medical subjects. Using our Multi-Question (MQ) Sequence-BERT method, we outperform the state-of-the-art results on the MedMCQA dataset with an accuracy of 0.68 and 0.60 on their development and test sets, respectively. In this sense, we show the capability of AI and LLMs in particular for multi-classification tasks in the Healthcare domain.", "citations": 1}
{"title": "Different Questions, Different Models: Fine-Grained Evaluation of Uncertainty and Calibration in Clinical QA with LLMs", "year": 2025, "authors": "A. Testoni, Iacer Calixto", "url": "https://www.semanticscholar.org/paper/3b2d7bbb293a53182d776df94b3599e47ba5d5c5", "relevance": 1, "abstract": "Accurate and well-calibrated uncertainty estimates are essential for deploying large language models (LLMs) in high-stakes domains such as clinical decision support. We present a fine-grained evaluation of uncertainty estimation methods for clinical multiple-choice question answering, covering ten open-source LLMs (general-purpose, biomedical, and reasoning models) across two datasets, eleven medical specialties, and six question types. We compare standard single-generation and sampling-based methods, and present a case study exploring simple, single-pass estimators based on behavioral signals in reasoning traces. These lightweight methods approach the performance of Semantic Entropy while requiring only one generation. Our results reveal substantial variation across specialties and question types, underscoring the importance of selecting models based on both the nature of the question and model-specific strengths.", "citations": 0}
{"title": "Larger and more instructable language models become less reliable", "year": 2024, "authors": "Lexin Zhou, Wout Schellaert, Fernando Mart\u00ednez-Plumed, Yael Moros-Daval, C\u00e9sar Ferri, J. Hern\u00e1ndez-Orallo", "url": "https://www.semanticscholar.org/paper/673324daac9505064d82b57e620418a50e548f8a", "relevance": 1, "abstract": "The prevailing methods to make large language models more powerful and amenable have been based on continuous scaling up (that is, increasing their size, data volume and computational resources1) and bespoke shaping up (including post-filtering2,3, fine tuning or use of human feedback4,5). However, larger and more instructable large language models may have become less reliable. By studying the relationship between difficulty concordance, task avoidance and prompting stability of several language model families, here we show that easy instances for human participants are also easy for the models, but scaled-up, shaped-up models do not secure areas of low difficulty in which either the model does not err or human supervision can spot the errors. We also find that early models often avoid user questions but scaled-up, shaped-up models tend to give an apparently sensible yet wrong answer much more often, including errors on difficult questions that human supervisors frequently overlook. Moreover, we observe that stability to different natural phrasings of the same question is improved by scaling-up and shaping-up interventions, but pockets of variability persist across difficulty levels. These findings highlight the need for a fundamental shift in the design and development of general-purpose artificial intelligence, particularly in high-stakes areas for which a predictable distribution of errors is paramount. Scaling up and shaping up large language models increased their tendency to provide sensible yet incorrect answers at difficulty levels humans cannot supervise, highlighting the need for a fundamental shift in artificial intelligence design towards reliability.", "citations": 168}
{"title": "BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text", "year": 2024, "authors": "Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Leo Wright Hall, Betty Xiong, Tony Lee, Roxana Daneshjou, Jonathan Frankle, Percy Liang, Michael Carbin, Christopher D. Manning", "url": "https://www.semanticscholar.org/paper/1804ea0c26e9b038547ff7a7e3eedf8d9de9717d", "relevance": 1, "abstract": "Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance on a wide variety of biomedical NLP tasks. However, these models have hundreds of billions of parameters, are computationally expensive to run, require users to send their input data over the internet, and are trained on unknown data sources. Can smaller, more targeted models compete? To address this question, we build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive model trained exclusively on PubMed abstracts and full articles. When fine-tuned, BioMedLM can produce strong multiple-choice biomedical question-answering results competitive with much larger models, such as achieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical Genetics exam. BioMedLM can also be fine-tuned to produce useful answers to patient questions on medical topics. This demonstrates that smaller models can potentially serve as transparent, privacy-preserving, economical and environmentally friendly foundations for particular NLP applications, such as in biomedicine. The model is available on the Hugging Face Hub: https://huggingface.co/stanford-crfm/BioMedLM.", "citations": 115}
{"title": "SciFive: a text-to-text transformer model for biomedical literature", "year": 2021, "authors": "Long Phan, J. Anibal, H. Tran, Shaurya Chanana, Erol Bahadroglu, Alec Peltekian, G. Altan-Bonnet", "url": "https://www.semanticscholar.org/paper/6003d268e9b5230dbc3e320497b50329d6186816", "relevance": 1, "abstract": "In this report, we introduce SciFive, a domain-specific T5 model that has been pre-trained on large biomedical corpora. Our model outperforms the current SOTA methods (i.e. BERT, BioBERT, Base T5) on tasks in named entity relation, relation extraction, natural language inference, and question-answering. We show that text-generation methods have significant potential in a broad array of biomedical NLP tasks, particularly those requiring longer, more complex outputs. Our results support the exploration of more difficult text generation tasks and the development of new methods in this area", "citations": 177}
{"title": "TCMEval-PA: a question-answering benchmark dataset for the prescription audit of Traditional Chinese Medicine", "year": 2025, "authors": "Baifeng Wang, Yiwei Lu, Zhe Wang, Peixin Ge, Guanjie Wang, Keyu Yao, Suyuan Peng, Yan Zhu", "url": "https://www.semanticscholar.org/paper/e1c298c7dc856b9f6204ca2c8fd03023ea48ed0c", "relevance": 1, "abstract": "Although large language models (LLMs) have witnessed rapid development in medical applications, their capacities to support rational medication use and guarantee prescription safety remain insufficiently investigated\u2014especially in tasks such as prescription audit, which plays a critical role in safeguarding both. This paper presents TCMEval-PA, a benchmark dataset for assessing the capabilities of LLMs in prescription audit of Chinese herbal medicines. The dataset comprises 328 choice questions, including 297 single-choice and 31 multiple-choice. All questions were designed and compiled through rule extraction from official documents and reviewed by licensed TCM physicians. TCMEval-PA comprehensively encompasses the key dimensions of prescription safety, including normativity (e.g., dispensing, decoction requirements, and regulations for special medicines) and appropriateness (e.g., contraindicated combinations and excessive dosages). The present study employed TCMEval-PA to assess several prevalent Chinese and English LLMs. This dataset can be utilized for the evaluation of LLMs and other artificial intelligence (AI) systems in TCM prescription safety scenarios and promotes research in intelligent auditing and decision support.", "citations": 0}
{"title": "AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering Benchmark Dataset", "year": 2024, "authors": "Tobi Olatunji, Charles Nimo, A. Owodunni, Tassallah Abdullahi, Emmanuel Ayodele, Mardhiyah Sanni, Chinemelu Aka, Folafunmi Omofoye, Foutse Yuehgoh, Timothy Faniran, Bonaventure F. P. Dossou, Moshood Yekini, Jonas Kemp, Katherine A. Heller, Jude Chidubem Omeke, Chidi Asuzu, Naome A. Etori, A. Ndiaye, Ifeoma Okoh, E. Ocansey, Wendy Kinara, Michael L. Best, Irfan Essa, Stephen Edward Moore, Chris Fourie, M. Asiedu", "url": "https://www.semanticscholar.org/paper/24353a793b7bc5ff36728b422c073d2b96e84174", "relevance": 1, "abstract": "Recent advancements in large language model(LLM) performance on medical multiple choice question (MCQ) benchmarks have stimulated interest from healthcare providers and patients globally. Particularly in low-and middle-income countries (LMICs) facing acute physician shortages and lack of specialists, LLMs offer a potentially scalable pathway to enhance healthcare access and reduce costs. However, their effectiveness in the Global South, especially across the African continent, remains to be established. In this work, we introduce AfriMed-QA, the first large scale Pan-African English multi-specialty medical Question-Answering (QA) dataset, 15,000 questions (open and closed-ended) sourced from over 60 medical schools across 16 countries, covering 32 medical specialties. We further evaluate 30 LLMs across multiple axes including correctness and demographic bias. Our findings show significant performance variation across specialties and geographies, MCQ performance clearly lags USMLE (MedQA). We find that biomedical LLMs underperform general models and smaller edge-friendly LLMs struggle to achieve a passing score. Interestingly, human evaluations show a consistent consumer preference for LLM answers and explanations when compared with clinician answers.", "citations": 21}
{"title": "OpenMedLM: prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models", "year": 2024, "authors": "Jenish Maharjan, A. Garikipati, N. Singh, Leo Cyrus, Mayank Sharma, M. Ciobanu, G. Barnes, R. Thapa, Q. Mao, R. Das", "url": "https://api.semanticscholar.org/CorpusId:268091253", "relevance": 1, "abstract": "LLMs can accomplish specialized medical knowledge tasks, however, equitable access is hindered by the extensive fine-tuning, specialized medical data requirement, and limited access to proprietary models. Open-source (OS) medical LLMs show performance improvements and provide the transparency and compliance required in healthcare. We present OpenMedLM, a prompting platform delivering state-of-the-art (SOTA) performance for OS LLMs on medical benchmarks. We evaluated OS foundation LLMs (7B-70B) on medical benchmarks (MedQA, MedMCQA, PubMedQA, MMLU medical-subset) and selected Yi34B for developing OpenMedLM. Prompting strategies included zero-shot, few-shot, chain-of-thought, and ensemble/self-consistency voting. OpenMedLM delivered OS SOTA results on three medical LLM benchmarks, surpassing previous best-performing OS models that leveraged costly and extensive fine-tuning. OpenMedLM displays the first results to date demonstrating the ability of OS foundation models to optimize performance, absent specialized fine-tuning. The model achieved 72.6% accuracy on MedQA, outperforming the previous SOTA by 2.4%, and 81.7% accuracy on MMLU medical-subset, establishing itself as the first OS LLM to surpass 80% accuracy on this benchmark. Our results highlight medical-specific emergent properties in OS LLMs not documented elsewhere to date and validate the ability of OS models to accomplish healthcare tasks, highlighting the benefits of prompt engineering to improve performance of accessible LLMs for medical applications.", "citations": 76}
{"title": "Large Language Model Benchmarks in Medical Tasks", "year": 2024, "authors": "Lawrence K.Q. Yan, Ming Li, Yichao Zhang, Caitlyn Heqi Yin, Cheng Fei, Benji Peng, Ziqian Bi, Pohsun Feng, Keyu Chen, Junyu Liu, Qian Niu", "url": "https://www.semanticscholar.org/paper/b24c88572e748baebb598ed61be713d56689e3b9", "relevance": 1, "abstract": "With the increasing application of large language models (LLMs) in the medical domain, evaluating these models'performance using benchmark datasets has become crucial. This paper presents a comprehensive survey of various benchmark datasets employed in medical LLM tasks. These datasets span multiple modalities including text, image, and multimodal benchmarks, focusing on different aspects of medical knowledge such as electronic health records (EHRs), doctor-patient dialogues, medical question-answering, and medical image captioning. The survey categorizes the datasets by modality, discussing their significance, data structure, and impact on the development of LLMs for clinical tasks such as diagnosis, report generation, and predictive decision support. Key benchmarks include MIMIC-III, MIMIC-IV, BioASQ, PubMedQA, and CheXpert, which have facilitated advancements in tasks like medical report generation, clinical summarization, and synthetic data generation. The paper summarizes the challenges and opportunities in leveraging these benchmarks for advancing multimodal medical intelligence, emphasizing the need for datasets with a greater degree of language diversity, structured omics data, and innovative approaches to synthesis. This work also provides a foundation for future research in the application of LLMs in medicine, contributing to the evolving field of medical artificial intelligence.", "citations": 21}
{"title": "PerMedCQA: Benchmarking Large Language Models on Medical Consumer Question Answering in Persian Language", "year": 2025, "authors": "Naghme Jamali, Milad Mohammadi, Danial Baledi, Zahra Rezvani, H. Faili", "url": "https://www.semanticscholar.org/paper/e0aa296cdb6c9fc7f2cbfde9ff9fc21f94cd860e", "relevance": 1, "abstract": "Medical consumer question answering (CQA) is crucial for empowering patients by providing personalized and reliable health information. Despite recent advances in large language models (LLMs) for medical QA, consumer-oriented and multilingual resources, particularly in low-resource languages like Persian, remain sparse. To bridge this gap, we present PerMedCQA, the first Persian-language benchmark for evaluating LLMs on real-world, consumer-generated medical questions. Curated from a large medical QA forum, PerMedCQA contains 68,138 question-answer pairs, refined through careful data cleaning from an initial set of 87,780 raw entries. We evaluate several state-of-the-art multilingual and instruction-tuned LLMs, utilizing MedJudge, a novel rubric-based evaluation framework driven by an LLM grader, validated against expert human annotators. Our results highlight key challenges in multilingual medical QA and provide valuable insights for developing more accurate and context-aware medical assistance systems. The data is publicly available on https://huggingface.co/datasets/NaghmehAI/PerMedCQA", "citations": 1}
{"title": "Capabilities of GPT-4 in ophthalmology: an analysis of model entropy and progress towards human-level medical question answering", "year": 2023, "authors": "F. Antaki, D. Milad, Mark A. Chia, Charles-\u00c9douard Gigu\u00e8re, Samir Touma, J. El-Khoury, P. Keane, R. Duval", "url": "https://www.semanticscholar.org/paper/2cb4f403104831abb7bbea43067ed7420e4b4b4c", "relevance": 1, "abstract": "Background Evidence on the performance of Generative Pre-trained Transformer 4 (GPT-4), a large language model (LLM), in the ophthalmology question-answering domain is needed. Methods We tested GPT-4 on two 260-question multiple choice question sets from the Basic and Clinical Science Course (BCSC) Self-Assessment Program and the OphthoQuestions question banks. We compared the accuracy of GPT-4 models with varying temperatures (creativity setting) and evaluated their responses in a subset of questions. We also compared the best-performing GPT-4 model to GPT-3.5 and to historical human performance. Results GPT-4\u20130.3 (GPT-4 with a temperature of 0.3) achieved the highest accuracy among GPT-4 models, with 75.8% on the BCSC set and 70.0% on the OphthoQuestions set. The combined accuracy was 72.9%, which represents an 18.3% raw improvement in accuracy compared with GPT-3.5 (p<0.001). Human graders preferred responses from models with a temperature higher than 0 (more creative). Exam section, question difficulty and cognitive level were all predictive of GPT-4-0.3 answer accuracy. GPT-4-0.3\u2019s performance was numerically superior to human performance on the BCSC (75.8% vs 73.3%) and OphthoQuestions (70.0% vs 63.0%), but the difference was not statistically significant (p=0.55\u2009and p=0.09). Conclusion GPT-4, an LLM trained on non-ophthalmology-specific data, performs significantly better than its predecessor on simulated ophthalmology board-style exams. Remarkably, its performance tended to be superior to historical human performance, but that difference was not statistically significant in our study.", "citations": 76}
{"title": "Large Language Model Synergy for Ensemble Learning in Medical Question Answering: Design and Evaluation Study", "year": 2024, "authors": "Han Yang, Mingchen Li, Huixue Zhou, Yongkang Xiao, Qian Fang, Shuang Zhou, Rui Zhang", "url": "https://www.semanticscholar.org/paper/be73061392b3c91de3d555bea01b87707099afca", "relevance": 1, "abstract": "Abstract Background Large language models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks, including medical question-answering (QA). However, individual LLMs often exhibit varying performance across different medical QA datasets. We benchmarked individual zero-shot LLMs (GPT-4, Llama2-13B, Vicuna-13B, MedLlama-13B, and MedAlpaca-13B) to assess their baseline performance. Within the benchmark, GPT-4 achieves the best 71% on MedMCQA (medical multiple-choice question answering dataset), Vicuna-13B achieves 89.5% on PubMedQA (a dataset for biomedical question answering), and MedAlpaca-13B achieves the best 70% among all, showing the potential for better performance across different tasks and highlighting the need for strategies that can harness their collective strengths. Ensemble learning methods, combining multiple models to improve overall accuracy and reliability, offer a promising approach to address this challenge. Objective To develop and evaluate efficient ensemble learning approaches, we focus on improving performance across 3 medical QA datasets through our proposed two ensemble strategies. Methods Our study uses 3 medical QA datasets: PubMedQA (1000 manually labeled and 11,269 test, with yes, no, or maybe answered for each question), MedQA-USMLE (Medical Question Answering dataset based on the United States Medical Licensing Examination; 12,724 English board-style questions; 1272 test, 5 options), and MedMCQA (182,822 training/4183 test questions, 4-option multiple choice). We introduced the LLM-Synergy framework, consisting of two ensemble methods: (1) a Boosting-based Weighted Majority Vote ensemble, refining decision-making by adaptively weighting each LLM and (2) a Cluster-based Dynamic Model Selection ensemble, dynamically selecting optimal LLMs for each query based on question-context embeddings and clustering. Results Both ensemble methods outperformed individual LLMs across all 3 datasets. Specifically comparing the best individual LLM, the Boosting-based Majority Weighted Vote achieved accuracies of 35.84% on MedMCQA (+3.81%), 96.21% on PubMedQA (+0.64%), and 37.26% (tie) on MedQA-USMLE. The Cluster-based Dynamic Model Selection yields even higher accuracies of 38.01% (+5.98%) for MedMCQA, 96.36% (+1.09%) for PubMedQA, and 38.13% (+0.87%) for MedQA-USMLE. Conclusions The LLM-Synergy framework, using 2 ensemble methods, represents a significant advancement in leveraging LLMs for medical QA tasks. Through effectively combining the strengths of diverse LLMs, this framework provides a flexible and efficient strategy adaptable to current and future challenges in biomedical informatics.", "citations": 11}
{"title": "Parameter-Efficient Domain Knowledge Integration from Multiple Sources for Biomedical Pre-trained Language Models", "year": 2021, "authors": "Qiuhao Lu, D. Dou, Thien Huu Nguyen", "url": "https://www.semanticscholar.org/paper/fb3dc58cb17c54b997e6301cbde7773f77427833", "relevance": 1, "abstract": "Domain-speci\ufb01c pre-trained language models (PLMs) have achieved great success over various downstream tasks in different domains. However, existing domain-speci\ufb01c PLMs mostly rely on self-supervised learning over large amounts of domain text, without explicitly integrating domain-speci\ufb01c knowledge, which can be essential in many domains. Moreover, in knowledge-sensitive ar-eas such as the biomedical domain, knowledge is stored in multiple sources and formats, and existing biomedical PLMs either neglect them or utilize them in a limited manner. In this work, we introduce an architecture to integrate domain knowledge from diverse sources into PLMs in a parameter-ef\ufb01cient way. More speci\ufb01cally, we propose to encode domain knowledge via adapters , which are small bottleneck feed-forward networks inserted between intermediate transformer layers in PLMs. These knowledge adapters are pre-trained for individual domain knowledge sources and integrated via an attention-based knowledge controller to enrich PLMs. Taking the biomedical domain as a case study, we explore three knowledge-speci\ufb01c adapters for PLMs based on the UMLS Metathesaurus graph, the Wikipedia articles for diseases, and the semantic grouping information for biomedical concepts. Extensive experiments on different biomedical NLP tasks and datasets demonstrate the bene\ufb01ts of the proposed architecture and the knowledge-speci\ufb01c adapters across multiple PLMs.", "citations": 28}
{"title": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for Multilingual Medical Question Answering", "year": 2025, "authors": "Feiyang Li, Yingjian Chen, Haoran Liu, Rui Yang, Han Yuan, Yuang Jiang, Tianxiao Li, Edison Marrese-Taylor, H. Rouhizadeh, Yusuke Iwasawa, Douglas Teodoro, Yutaka Matsuo, I. Li", "url": "https://www.semanticscholar.org/paper/cc3b19d8e5ff95b8a29ef0caa8d8ed16dc57f0e2", "relevance": 1, "abstract": "Large Language Models (LLMs) have shown remarkable progress in medical question answering (QA), yet their effectiveness remains predominantly limited to English due to imbalanced multilingual training data and scarce medical resources for low-resource languages. To address this critical language gap in medical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking (MKG-Rank), a knowledge graph-enhanced framework that enables English-centric LLMs to perform multilingual medical QA. Through a word-level translation mechanism, our framework efficiently integrates comprehensive English-centric medical knowledge graphs into LLM reasoning at a low cost, mitigating cross-lingual semantic distortion and achieving precise medical QA across language barriers. To enhance efficiency, we introduce caching and multi-angle ranking strategies to optimize the retrieval process, significantly reducing response times and prioritizing relevant medical knowledge. Extensive evaluations on multilingual medical QA benchmarks across Chinese, Japanese, Korean, and Swahili demonstrate that MKG-Rank consistently outperforms zero-shot LLMs, achieving maximum 35.03% increase in accuracy, while maintaining an average retrieval time of only 0.0009 seconds.", "citations": 2}
{"title": "70B-parameter large language models in Japanese medical question-answering", "year": 2024, "authors": "Issey Sukeda, Risa Kishikawa, Satoshi Kodera", "url": "https://www.semanticscholar.org/paper/8f218600de3fb379b2b3a988ea4bc6cc69b3bea4", "relevance": 1, "abstract": "Since the rise of large language models (LLMs), the domain adaptation has been one of the hot topics in various domains. Many medical LLMs trained with English medical dataset have made public recently. However, Japanese LLMs in medical domain still lack its research. Here we utilize multiple 70B-parameter LLMs for the first time and show that instruction tuning using Japanese medical question-answering dataset significantly improves the ability of Japanese LLMs to solve Japanese medical license exams, surpassing 50\\% in accuracy. In particular, the Japanese-centric models exhibit a more significant leap in improvement through instruction tuning compared to their English-centric counterparts. This underscores the importance of continual pretraining and the adjustment of the tokenizer in our local language. We also examine two slightly different prompt formats, resulting in non-negligible performance improvement.", "citations": 5}
{"title": "Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature", "year": 2023, "authors": "A. Lozano, Scott L. Fleming, Chia-Chun Chiang, Nigam H. Shah", "url": "https://www.semanticscholar.org/paper/3d297441afbc2124bc805f2a9a4d3db3e21ae217", "relevance": 1, "abstract": "The quickly-expanding nature of published medical literature makes it challenging for clinicians and researchers to keep up with and summarize recent, relevant findings in a timely manner. While several closed-source summarization tools based on large language models (LLMs) now exist, rigorous and systematic evaluations of their outputs are lacking. Furthermore, there is a paucity of high-quality datasets and appropriate benchmark tasks with which to evaluate these tools. We address these issues with four contributions: we release Clinfo.ai, an open-source WebApp that answers clinical questions based on dynamically retrieved scientific literature; we specify an information retrieval and abstractive summarization task to evaluate the performance of such retrieval-augmented LLM systems; we release a dataset of 200 questions and corresponding answers derived from published systematic reviews, which we name PubMed Retrieval and Synthesis (PubMedRS-200); and report benchmark results for Clinfo.ai and other publicly available OpenQA systems on PubMedRS-200.", "citations": 61}
{"title": "FairMedQA: Benchmarking Bias in Large Language Models for Medical Question Answering", "year": 2025, "authors": "Ying Xiao, Jie Huang, Ruijuan He, Jing Xiao, Mohammad Reza Mousavi, Yepang Liu, Kezhi Li, Zhenpeng Chen, Jie M. Zhang", "url": "https://api.semanticscholar.org/CorpusId:278905746", "relevance": 1, "abstract": "Large language models (LLMs) are approaching expert-level performance in medical question answering (QA), demonstrating strong potential to improve public healthcare. However, underlying biases related to sensitive attributes such as sex and race pose life-critical risks. The extent to which such sensitive attributes affect diagnosis remains an open question and requires comprehensive empirical investigation. Additionally, even the latest Counterfactual Patient Variations (CPV) benchmark can hardly distinguish the bias levels of different LLMs. To further explore these dynamics, we propose a new benchmark, FairMedQA, and benchmark 12 representative LLMs. FairMedQA contains 4,806 counterfactual question pairs constructed from 801 clinical vignettes. Our results reveal substantial accuracy disparity ranging from 3 to 19 percentage points across sensitive demographic groups. Notably, FairMedQA exposes biases that are at least 12 percentage points larger than those identified by the latest CPV benchmark, presenting superior benchmarking sensitivity. Our results underscore an urgent need for targeted debiasing techniques and more rigorous, identity-aware validation protocols before LLMs can be safely integrated into practical clinical decision-support systems.", "citations": 4}
{"title": "PersianMedQA: Evaluating Large Language Models on a Persian-English Bilingual Medical Question Answering Benchmark", "year": 2025, "authors": "Mohammad Javad Ranjbar Kalahroodi, Amirhossein Sheikholselami, Sepehr Karimi, Sepideh Ranjbar Kalahroodi, Heshaam Faili, A. Shakery", "url": "https://www.semanticscholar.org/paper/d81dc296f857338e44e6aff1486377c90c1b0c83", "relevance": 1, "abstract": "Large Language Models (LLMs) have achieved remarkable performance on a wide range of Natural Language Processing (NLP) benchmarks, often surpassing human-level accuracy. However, their reliability in high-stakes domains such as medicine, particularly in low-resource languages, remains underexplored. In this work, we introduce PersianMedQA, a large-scale dataset of 20,785 expert-validated multiple-choice Persian medical questions from 14 years of Iranian national medical exams, spanning 23 medical specialties and designed to evaluate LLMs in both Persian and English. We benchmark 40 state-of-the-art models, including general-purpose, Persian fine-tuned, and medical LLMs, in zero-shot and chain-of-thought (CoT) settings. Our results show that closed-source general models (e.g., GPT-4.1) consistently outperform all other categories, achieving 83.09% accuracy in Persian and 80.7% in English, while Persian fine-tuned models such as Dorna underperform significantly (e.g., 34.9% in Persian), often struggling with both instruction-following and domain reasoning. We also analyze the impact of translation, showing that while English performance is generally higher, 3-10% of questions can only be answered correctly in Persian due to cultural and clinical contextual cues that are lost in translation. Finally, we demonstrate that model size alone is insufficient for robust performance without strong domain or language adaptation. PersianMedQA provides a foundation for evaluating bilingual and culturally grounded medical reasoning in LLMs. The PersianMedQA dataset is available: https://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA .", "citations": 1}
{"title": "MedQARo: A Large-Scale Benchmark for Evaluating Large Language Models on Medical Question Answering in Romanian", "year": 2025, "authors": "Ana-Cristina Rogoz, R. Ionescu, Alexandra-Valentina Anghel, Ionu\u021b-Lucian Antone-Iordache, S. Coniac, Andreea-Iuliana Ionescu", "url": "https://www.semanticscholar.org/paper/f1fa442ae6a1c3c34650a182590caf672b218fcf", "relevance": 1, "abstract": "Question answering (QA) is an actively studied topic, being a core natural language processing (NLP) task that needs to be addressed before achieving Artificial General Intelligence (AGI). However, the lack of QA datasets in specific domains and languages hinders the development of robust AI models able to generalize across various domains and languages. To this end, we introduce MedQARo, the first large-scale medical QA benchmark in Romanian, alongside a comprehensive evaluation of state-of-the-art (SOTA) large language models (LLMs). We construct a high-quality and large-scale dataset comprising 105,880 QA pairs related to cancer patients from two medical centers. The questions regard medical case summaries of 1,242 patients, requiring either keyword extraction or reasoning to be answered correctly. MedQARo is the result of a time-consuming manual annotation process carried out by seven physicians specialized in oncology or radiotherapy, who spent a total of about 3,000 work hours to generate the QA pairs. Our benchmark contains both in-domain and cross-domain (cross-center and cross-cancer) test collections, enabling a precise assessment of generalization capabilities. We experiment with four open-source LLMs from distinct families of models on MedQARo. Each model is employed in two scenarios, namely one based on zero-shot prompting and one based on supervised fine-tuning. We also evaluate two state-of-the-art LLMs exposed only through APIs, namely GPT-5.2 and Gemini 3 Flash. Our results show that fine-tuned models significantly outperform zero-shot models, clearly indicating that pretrained models fail to generalize on MedQARo. Our findings demonstrate the importance of both domain-specific and language-specific fine-tuning for reliable clinical QA in Romanian. We publicly release our dataset and code at https://github.com/ana-rogoz/MedQARo.", "citations": 0}
{"title": "A Survey of Large Language Model for Drug Research and Development", "year": 2025, "authors": "Huijie Guo, Xudong Xing, Yongjie Zhou, Wenjiao Jiang, Xiaoyi Chen, Ting Wang, Zixuan Jiang, Yibing Wang, Junyan Hou, Yukun Jiang, Jianzhen Xu", "url": "https://www.semanticscholar.org/paper/7fbca8b465fda722179c6379948504d71329b9f9", "relevance": 1, "abstract": "Drug research and development (drug R&D) is a sophisticated, cost-intensive, and time-consuming procedure with historically low success rates. The advent of Artificial Intelligence (AI) technologies has introduced innovative methods into drug R&D, particularly by leveraging AI capabilities. Large language models (LLMs), a breakthrough in generative AI, have revolutionized drug discovery. With their extensive datasets, numerous parameters, and strong multitasking abilities, LLMs have significantly improved efficiency across various related domains, providing unparalleled support to drug R&D. These models have facilitated a deeper understanding of intricate disease mechanisms and the identification of novel therapeutic strategies, ushering in a new era in drug development and clinical applications. As a result, the advancement of LLMs is poised to drive significant transformations in drug R&D, emphasizing the importance of effectively leveraging this technology. This review provides insights into the architecture and characteristics of LLMs, explores their applications in drug R&D, and highlights their research implications in bioinformatics data, including proteins, genes, and chemical compounds. Furthermore, it investigates the practical strategies of LLMs in drug discovery, drug repositioning, and clinical inquiries, presenting an innovative approach to research and future advancements in this field.", "citations": 6}
{"title": "Evaluating the Accuracy of Responses by Large Language Models for Information on Disease Epidemiology", "year": 2025, "authors": "Kexin Zhu, Jiajie Zhang, Anton Klishin, Mario Esser, William A. Blumentals, J. Juhaeri, Corinne Jouquelet-Royer, Sarah-Jo Sinnott", "url": "https://www.semanticscholar.org/paper/206c0264d10c2500dd3df42ea58d73cf7147682b", "relevance": 1, "abstract": "Accurate background epidemiology of diseases are required in pharmacoepidemiologic research. We evaluated the performance of large language models (LLMs), including ChatGPT\u20103.5, ChatGPT\u20104, and Google Bard, when prompted with questions on disease frequency.", "citations": 4}
{"title": "Overconfident AI? Benchmarking LLM Self-Assessment in Clinical Scenarios", "year": 2024, "authors": "M. Omar, Benjamin S. Glicksberg, G. Nadkarni, E. Klang", "url": "https://www.semanticscholar.org/paper/6751cf76ea2ec85aef374faa496a5d3b5829e371", "relevance": 1, "abstract": "Background and Aim: Large language models (LLMs) show promise in healthcare, but their self-assessment capabilities remain unclear. This study evaluates the confidence levels and performance of 12 LLMs across five medical specialties to assess their ability to accurately judge their responses. Methods: We used 1965 multiple-choice questions from internal medicine, obstetrics and gynecology, psychiatry, pediatrics, and general surgery. Models were prompted to provide answers and confidence scores. Performance and confidence were analyzed using chi-square tests and t-tests. Consistency across question versions was also evaluated. Results: All models displayed high confidence regardless of answer correctness. Higher-tier models showed slightly better calibration, with a mean confidence of 72.5% for correct answers versus 69.4% for incorrect ones, compared to lower-tier models (79.6% vs 79.5%). The mean confidence difference between correct and incorrect responses ranged from 0.6% to 5.4% across all models. Four models showed significantly higher confidence when correct (p<0.01), but the difference remained small. Most models demonstrated consistency across question versions. Conclusion: While newer LLMs show improved performance and consistency in medical knowledge tasks, their confidence levels remain poorly calibrated. The gap between performance and self-assessment poses risks in clinical applications. Until these models can reliably gauge their certainty, their use in healthcare should be limited and supervised by experts. Further research on human-AI collaboration and ensemble methods is needed for responsible implementation. Keywords: Large Language Models (LLMs), Safe AI, AI Reliability, Clinical knowledge.", "citations": 6}
{"title": "Small Language Models for Emergency Departments Decision Support: A Benchmark Study", "year": 2025, "authors": "Zirui Wang, Jiajun Wu, Braden D. Teitge, Jessalyn K Holodinsky, Steve Drew", "url": "https://www.semanticscholar.org/paper/970fad23d98a25e57f0f25ab0fd911a73e80793e", "relevance": 1, "abstract": "Large language models (LLMs) have become increasingly popular in medical domains to assist physicians with a variety of clinical and operational tasks. Given the fast-paced and high-stakes environment of emergency departments (EDs), small language models (SLMs), characterized by a reduction in parameter count compared to LLMs, offer significant potential due to their inherent reasoning capability and efficient performance. This enables SLMs to support physicians by providing timely and accurate information synthesis, thereby improving clinical decision-making and workflow efficiency. In this paper, we present a comprehensive benchmark designed to identify SLMs suited for ED decision support, taking into account both specialized medical expertise and broad general problem-solving capabilities. In our evaluations, we focus on SLMs that have been trained on a mixture of general-domain and medical corpora. A key motivation for emphasizing SLMs is the practical hardware limitations, operational cost constraints, and privacy concerns in the typical real-world deployments. Our benchmark datasets include MedMCQA, MedQA-4Options, and PubMedQA, with the medical abstracts dataset emulating tasks aligned with real ED physicians'daily tasks. Experimental results reveal that general-domain SLMs surprisingly outperform their medically fine-tuned counterparts across these diverse benchmarks for ED. This indicates that for ED, specialized medical fine-tuning of the model may not be required.", "citations": 0}
{"title": "Building Models of Neurological Language", "year": 2025, "authors": "Henry Watkins", "url": "https://www.semanticscholar.org/paper/ef278c43b9250ec86f8b2dfe1f652f94348b402d", "relevance": 1, "abstract": "This report documents the development and evaluation of domain-specific language models for neurology. Initially focused on building a bespoke model, the project adapted to rapid advances in open-source and commercial medical LLMs, shifting toward leveraging retrieval-augmented generation (RAG) and representational models for secure, local deployment. Key contributions include the creation of neurology-specific datasets (case reports, QA sets, textbook-derived data), tools for multi-word expression extraction, and graph-based analyses of medical terminology. The project also produced scripts and Docker containers for local hosting. Performance metrics and graph community results are reported, with future possible work open for multimodal models using open-source architectures like phi-4.", "citations": 0}
{"title": "Performance of Large Language Models in Answering Critical Care Medicine Questions", "year": 2025, "authors": "Mahmoud Alwakeel, Aditya Nagori, An-Kwok Ian Wong, Neal Chaisson, Vijay Krishnamoorthy, Rishikesan Kamaleswaran", "url": "https://www.semanticscholar.org/paper/27a32876107151848d97dae7e0c50757efe4af6c", "relevance": 1, "abstract": "Large Language Models have been tested on medical student-level questions, but their performance in specialized fields like Critical Care Medicine (CCM) is less explored. This study evaluated Meta-Llama 3.1 models (8B and 70B parameters) on 871 CCM questions. Llama3.1:70B outperformed 8B by 30%, with 60% average accuracy. Performance varied across domains, highest in Research (68.4%) and lowest in Renal (47.9%), highlighting the need for broader future work to improve models across various subspecialty domains.", "citations": 0}
{"title": "Reasoning with large language models for medical question answering", "year": 2024, "authors": "Mary M. Lucas, Justin Yang, Jon K. Pomeroy, Christopher C. Yang", "url": "https://www.semanticscholar.org/paper/26d8487f05024425e0a87292ba1332e6ee34ef03", "relevance": 1, "abstract": "OBJECTIVES\nTo investigate approaches of reasoning with large language models (LLMs) and to propose a new prompting approach, ensemble reasoning, to improve medical question answering performance with refined reasoning and reduced inconsistency.\n\n\nMATERIALS AND METHODS\nWe used multiple choice questions from the USMLE Sample Exam question files on 2 closed-source commercial and 1 open-source clinical LLM to evaluate our proposed approach ensemble reasoning.\n\n\nRESULTS\nOn GPT-3.5 turbo and Med42-70B, our proposed ensemble reasoning approach outperformed zero-shot chain-of-thought with self-consistency on Steps 1, 2, and 3 questions (+3.44%, +4.00%, and +2.54%) and (2.3%, 5.00%, and 4.15%), respectively. With GPT-4 turbo, there were mixed results with ensemble reasoning again outperforming zero-shot chain-of-thought with self-consistency on Step 1 questions (+1.15%). In all cases, the results demonstrated improved consistency of responses with our approach. A qualitative analysis of the reasoning from the model demonstrated that the ensemble reasoning approach produces correct and helpful reasoning.\n\n\nCONCLUSION\nThe proposed iterative ensemble reasoning has the potential to improve the performance of LLMs in medical question answering tasks, particularly with the less powerful LLMs like GPT-3.5 turbo and Med42-70B, which may suggest that this is a promising approach for LLMs with lower capabilities. Additionally, the findings show that our approach helps to refine the reasoning generated by the LLM and thereby improve consistency even with the more powerful GPT-4 turbo. We also identify the potential and need for human-artificial intelligence teaming to improve the reasoning beyond the limits of the model.", "citations": 50}
{"title": "Adapted large language models can outperform medical experts in clinical text summarization", "year": 2023, "authors": "Dave Van Veen, Cara Van Uden, L. Blankemeier, Jean-Benoit Delbrouck, Asad Aali, Christian Bl\u00fcthgen, A. Pareek, Malgorzata Polacin, William Collins, Neera Ahuja, C. Langlotz, Jason Hom, S. Gatidis, John M. Pauly, Akshay S. Chaudhari", "url": "https://www.semanticscholar.org/paper/007c3d9b8dab341d2c77c4ee764fd921f7f14956", "relevance": 1, "abstract": "Analyzing vast textual data and summarizing key information from electronic health records imposes a substantial burden on how clinicians allocate their time. Although large language models (LLMs) have shown promise in natural language processing (NLP) tasks, their effectiveness on a diverse range of clinical summarization tasks remains unproven. Here we applied adaptation methods to eight LLMs, spanning four distinct clinical summarization tasks: radiology reports, patient questions, progress notes and doctor\u2013patient dialogue. Quantitative assessments with syntactic, semantic and conceptual NLP metrics reveal trade-offs between models and adaptation methods. A clinical reader study with 10 physicians evaluated summary completeness, correctness and conciseness; in most cases, summaries from our best-adapted LLMs were deemed either equivalent (45%) or superior (36%) compared with summaries from medical experts. The ensuing safety analysis highlights challenges faced by both LLMs and medical experts, as we connect errors to potential medical harm and categorize types of fabricated information. Our research provides evidence of LLMs outperforming medical experts in clinical text summarization across multiple tasks. This suggests that integrating LLMs into clinical workflows could alleviate documentation burden, allowing clinicians to focus more on patient care. Comparative performance assessment of large language models identified ChatGPT-4 as the best-adapted model across a diverse set of clinical text summarization tasks, and it outperformed 10 medical experts in a reader study.", "citations": 611}
{"title": "GPT-4: a new era of artificial intelligence in medicine", "year": 2023, "authors": "E. Waisberg, J. Ong, M. Masalkhi, S. Kamran, Nasif Zaman, Prithul Sarker, Andrew G Lee, A. Tavakkoli", "url": "https://www.semanticscholar.org/paper/a0eec004b3cfe0d162d425f2730a286845bcedbc", "relevance": 1, "abstract": "", "citations": 168}
{"title": "Large language model AI chatbots require approval as medical devices", "year": 2023, "authors": "S. Gilbert, Hugh Harvey, T. Melvin, E. Vollebregt, Paul Wicks", "url": "https://www.semanticscholar.org/paper/e548942b23a54f3492d2ff566a6729d8f37278bd", "relevance": 1, "abstract": "", "citations": 166}
{"title": "Heterogeneous Knowledge Grounding for Medical Question Answering with Retrieval Augmented Large Language Model", "year": 2024, "authors": "Wenting Zhao, Zhongfen Deng, S. Yadav, Philip S. Yu", "url": "https://www.semanticscholar.org/paper/fe89037a6d5481d39e562a0458e91060e5cb86c0", "relevance": 1, "abstract": "The Large Language Model (LLM) is renowned for its ability to encode a vast amount of general domain knowledge, enabling it to excel in question-answering, dialogue systems, and summarization tasks. However, the medical domain presents a unique challenge to LLM due to the distribution of medical knowledge, which follows a long-tail pattern. Existing approaches address this challenge by injecting medical knowledge into LLM through single sources such as medical textbooks or medical knowledge bases. However, medical knowledge is distributed across multiple heterogeneous information sources. A medical question-answering system can enhance answer coverage and confidence by considering these diverse knowledge sources together. To bridge this gap, we propose a novel approach called Heterogeneous Knowledge Retrieval-Augmented LLM for medical domain question answering. Our experiments, conducted on the MedQA-USMLE dataset, demonstrate promising performance improvements. These results underscore the importance of harnessing heterogeneous knowledge sources in the medical domain.", "citations": 7}
{"title": "A Comprehensive Survey on the Trustworthiness of Large Language Models in Healthcare", "year": 2025, "authors": "Manar A. Aljohani, Jun Hou, Sindhura Kommu, Xuan Wang", "url": "https://www.semanticscholar.org/paper/2a8cf14e036d451f27df981a8b2b7e039b96f89a", "relevance": 1, "abstract": "The application of large language models (LLMs) in healthcare holds significant promise for enhancing clinical decision-making, medical research, and patient care. However, their integration into real-world clinical settings raises critical concerns around trustworthiness, particularly around dimensions of truthfulness, privacy, safety, robustness, fairness, and explainability. These dimensions are essential for ensuring that LLMs generate reliable, unbiased, and ethically sound outputs. While researchers have recently begun developing benchmarks and evaluation frameworks to assess LLM trustworthiness, the trustworthiness of LLMs in healthcare remains underexplored, lacking a systematic review that provides a comprehensive understanding and future insights. This survey addresses that gap by providing a comprehensive review of current methodologies and solutions aimed at mitigating risks across key trust dimensions. We analyze how each dimension affects the reliability and ethical deployment of healthcare LLMs, synthesize ongoing research efforts, and identify critical gaps in existing approaches. We also identify emerging challenges posed by evolving paradigms, such as multi-agent collaboration, multi-modal reasoning, and the development of small open-source medical models. Our goal is to guide future research toward more trustworthy, transparent, and clinically viable LLMs.", "citations": 22}
{"title": "Advancing medical question answering with a knowledge embedding transformer", "year": 2025, "authors": "Xiang Zhu, Mustaqeem Khan, Abdelmalik Taleb-Ahmed, Alice Othmani", "url": "https://www.semanticscholar.org/paper/62d3b52886fba9b4a3695e2245b3a4a1dc00d0a7", "relevance": 1, "abstract": "Efficient medical question answering is essential for better patient care. Despite progress since Eliza (1966), even advanced LLMs (e.g., GPT-4) struggle with medical data. This study presents a system combining knowledge embedding and transformers. It includes a knowledge understanding layer and an answer generation layer. Tested on the MedQA dataset, it achieved 82.92% accuracy, outperforming GPT-4\u2019s 71.07%. The results demonstrate the system\u2019s ability to deliver accurate and ethical answers. This integrated method improves response speed and quality. Future work will enhance precision, support patient interaction, and integrate multimodal data for improved healthcare query processing.", "citations": 4}
{"title": "Bias Evaluation and Mitigation in Retrieval-Augmented Medical Question-Answering Systems", "year": 2025, "authors": "Yuelyu Ji, Hang Zhang, Yanshan Wang", "url": "https://www.semanticscholar.org/paper/0e0a26772788aa53a808798ec04524ce1d5461f4", "relevance": 1, "abstract": "Medical Question Answering systems based on Retrieval Augmented Generation is promising for clinical decision support because they can integrate external knowledge, thus reducing inaccuracies inherent in standalone large language models (LLMs). However, these systems may unintentionally propagate or amplify biases associated with sensitive demographic attributes like race, gender, and socioeconomic factors. This study systematically evaluates demographic biases within medical RAG pipelines across multiple QA benchmarks, including MedQA, MedMCQA, MMLU, and EquityMedQA. We quantify disparities in retrieval consistency and answer correctness by generating and analyzing queries sensitive to demographic variations. We further implement and compare several bias mitigation strategies to address identified biases, including Chain of Thought reasoning, Counterfactual filtering, Adversarial prompt refinement, and Majority Vote aggregation. Experimental results reveal significant demographic disparities, highlighting that Majority Vote aggregation notably improves accuracy and fairness metrics. Our findings underscore the critical need for explicitly fairness-aware retrieval methods and prompt engineering strategies to develop truly equitable medical QA systems.", "citations": 4}
{"title": "Investigating LLM Capabilities on Long Context Comprehension for Medical Question Answering", "year": 2025, "authors": "Feras AlMannaa, Talia Tseriotou, Jenny Chim, M. Liakata", "url": "https://www.semanticscholar.org/paper/4644947c3de21615b229e9e227a2b353c439d928", "relevance": 1, "abstract": "This study is the first to investigate LLM comprehension capabilities over long-context (LC), clinically relevant medical Question Answering (QA) beyond MCQA. Our comprehensive approach considers a range of settings based on content inclusion of varying size and relevance, LLM models of different capabilities and a variety of datasets across task formulations. We reveal insights on model size effects and their limitations, underlying memorization issues and the benefits of reasoning models, while demonstrating the value and challenges of leveraging the full long patient's context. Importantly, we examine the effect of Retrieval Augmented Generation (RAG) on medical LC comprehension, showcasing best settings in single versus multi-document QA datasets. We shed light into some of the evaluation aspects using a multi-faceted approach uncovering common metric challenges. Our quantitative analysis reveals challenging cases where RAG excels while still showing limitations in cases requiring temporal reasoning.", "citations": 0}
{"title": "Language Model Uncertainty Quantification with Attention Chain", "year": 2025, "authors": "Yinghao Li, Rushi Qiang, Lama Moukheiber, Chao Zhang", "url": "https://www.semanticscholar.org/paper/03df740cc1e6399c37f860481d2b73f1d315eb71", "relevance": 1, "abstract": "Accurately quantifying a large language model's (LLM) predictive uncertainty is crucial for judging the reliability of its answers. While most existing research focuses on short, directly answerable questions with closed-form outputs (e.g., multiple-choice), involving intermediate reasoning steps in LLM responses is increasingly important. This added complexity complicates uncertainty quantification (UQ) because the probabilities assigned to answer tokens are conditioned on a vast space of preceding reasoning tokens. Direct marginalization is infeasible, and the dependency inflates probability estimates, causing overconfidence in UQ. To address this, we propose UQAC, an efficient method that narrows the reasoning space to a tractable size for marginalization. UQAC iteratively constructs an\"attention chain\"of tokens deemed\"semantically crucial\"to the final answer via a backtracking procedure. Starting from the answer tokens, it uses attention weights to identify the most influential predecessors, then iterates this process until reaching the input tokens. The resulting chain is further refined with similarity filtering and probability thresholding, which reduce the reasoning space, facilitating the approximation of the marginal answer token probabilities. We validate UQAC on multiple reasoning benchmarks with advanced open-source LLMs, demonstrating that it consistently delivers reliable UQ estimates with high computational efficiency.", "citations": 10}
{"title": "Evaluating the Pre-Consultation Ability of LLMs using Diagnostic Guidelines", "year": 2026, "authors": "Jean Seo, Gibaeg Kim, Kihun Shin, Seungseop Lim, Hyunkyung Lee, Wooseok Han, Jongwon Lee, Eunho Yang", "url": "https://www.semanticscholar.org/paper/e34c98042cfff1be09bad8fb4f676422145d7026", "relevance": 1, "abstract": "We introduce EPAG, a benchmark dataset and framework designed for Evaluating the Pre-consultation Ability of LLMs using diagnostic Guidelines. LLMs are evaluated directly through HPI-diagnostic guideline comparison and indirectly through disease diagnosis. In our experiments, we observe that small open-source models fine-tuned with a well-curated, task-specific dataset can outperform frontier LLMs in pre-consultation. Additionally, we find that increased amount of HPI (History of Present Illness) does not necessarily lead to improved diagnostic performance. Further experiments reveal that the language of pre-consultation influences the characteristics of the dialogue. By open-sourcing our dataset and evaluation pipeline on https://github.com/seemdog/EPAG, we aim to contribute to the evaluation and further development of LLM applications in real-world clinical settings.", "citations": 0}
{"title": "Performance of large language models on prosthodontics questions of the dentistry specialization examination: a comparative analysis (2014\u20132024)", "year": 2025, "authors": "Hayriye Yasemin Yay Ku\u015f\u00e7u, Zuhal G\u00f6r\u00fc\u015f", "url": "https://www.semanticscholar.org/paper/6b972d52fa5a10f802c08e81b6f753b8238f9fcc", "relevance": 1, "abstract": "Aims: This study aimed to comparatively evaluate the performance of five contemporary large language models (LLMs) on prosthodontics questions of the dentistry specialization examination (DUS) between 2014 and 2024. \nMethods: A total of 167 prosthodontics questions from the DUS were analyzed. The questions were administered to five different LLMs: ChatGPT-5 (OpenAI Inc., USA), Claude 4 (Anthropic, USA), Gemini 1.5 Pro (Google LLC, USA), DeepSeek-V2 (DeepSeek AI, China), and Perplexity Pro (Perplexity AI, USA). The models\u2019 responses were compared with the official answer keys provided by the Student Selection and Placement Center (OSYM), coded as correct or incorrect, and accuracy percentages were calculated. Statistical analyses included the Friedman test, correlation analysis, and frequency distributions. Subsection analyses were also performed to evaluate model performance across different content areas. \nResults: DeepSeek-V2 achieved the highest overall accuracy rate (70.06%). Perplexity Pro (53.89%) and Gemini 1.5 Pro (51.50%) demonstrated moderate performance, ChatGPT-5 (49.10%) performed close to human levels, while Claude 4 had the lowest accuracy (32.34%). Subsection analyses revealed high accuracy in standardized knowledge areas such as implantology and temporomandibular joint (TMJ) disorders (66.7-100%), whereas notable decreases were observed in occlusion and morphology questions (9.1-53.9%). Correlation analyses indicated significant relationships between certain models. \nConclusion: The findings demonstrate heterogeneous performance of LLMs on DUS prosthodontics questions. While these models may serve as supplementary tools for exam preparation and dental education, their variable accuracy and potential for generating misinformation suggest they should not be used independently. Under expert supervision, LLMs may enhance dental education.", "citations": 1}
{"title": "Blinded evaluation of GPT-4 and physician responses to patient inquiries across multiple specialties", "year": 2025, "authors": "Adrian Sallabi, Hrvoje Hrvoj, A. \u0160tajduhar", "url": "https://www.semanticscholar.org/paper/dad890bca626bd91eb60ee31211f6a59365602df", "relevance": 1, "abstract": "Objectives To evaluate the quality of responses generated by GPT-4 in comparison to those written by hospital specialists and general practice physicians across multiple medical specialties. The goal was to assess whether large language models (LLMs) can support patient communication by providing accurate, useful, complete, and empathetic responses to real-world patient inquiries. Methods We collected 100 anonymized patient questions from a public online health forum covering five specialties: cardiology, infectious diseases, neurology, gynecology, and gastroenterology. Responses were generated by GPT-4, 50 hospital-based specialists, and 50 general practice physicians. A group of 50 specialists, blinded to response source, evaluated each answer using four 7-point Likert scales: accuracy, usefulness, completeness, and empathy. The study design and reporting were informed by the CONSORT-AI Extension to promote transparency in AI evaluation. Results The model received significantly higher ratings across all four categories compared to both physician groups. It was ranked best in 67% of evaluations, particularly outperforming physicians in completeness and empathy. While response length correlated positively with quality for physicians, model's longer responses were less useful when overly detailed. The model consistently produced longer, more comprehensive replies than human groups. Discussion The model's strong performance in completeness and empathy highlights its potential role in enhancing patient communication. Although it matched or exceeded physicians in accuracy and usefulness, caution is warranted due to risks like hallucinations and lack of true understanding. Conclusion This blinded evaluation suggests that AI-generated responses may support clinical practice by delivering accurate, comprehensive, and empathetic information for patient communication.", "citations": 0}
{"title": "Clinical Assessment of Large Language Models: A Comprehensive Multi-domain Performance Study for Healthcare Applications", "year": 2025, "authors": "Harsh Hirani, Bharat Saboo, Alok Modi, Palak Modi, S. Samajdar, Banshi Saboo, Manoj Chawla, Anuj Maheshwari, Amit Gupta, Rakesh M. Parikh, S. Dariya, Ritu Johari, Rutul Gokhlani, Param Kadam", "url": "https://www.semanticscholar.org/paper/60f61ab69ca133d974deac06d0b8d0b93f8761bc", "relevance": 1, "abstract": "\n \n \n As artificial intelligence continues to reshape healthcare landscapes, large language models (LLMs) have emerged as powerful tools with potential applications spanning clinical documentation, patient communication, and diagnostic support. However, the clinical utility and safety profiles of these models remain inadequately characterized. This study evaluates four prominent LLMs across multiple healthcare-relevant domains to provide evidence-based guidance for clinical implementation.\n \n \n \n We developed a comprehensive evaluation framework assessing ChatGPT, Google Gemini, Perplexity, and Grok versions as of JULY 2025 across 15 clinical domains. Our methodology included standardized testing scenarios designed to mirror real-world healthcare applications, from emergency medicine protocols to evidence-based research synthesis. Performance metrics encompassed accuracy, speed, safety, and integration capabilities.\n \n \n \n Our analysis revealed significant performance variations among models. ChatGPT achieved the highest composite score (82/100), demonstrating particular strength in clinical documentation and patient communication. However, critical safety concerns emerged across all platforms, including dangerous product hallucination rates (23%\u201331% in Google Gemini and Grok) and universal patient memory limitations. Perplexity distinguished itself as the only model providing consistent source citations (94% accuracy), while Grok\u2019s rapid response times (average 12.3 s compared to ChatGPT 28.7 S, Gemini 31.2 S, and perplexity 24.8 s) showed promise for emergency applications despite translation accuracy concerns.\n \n \n \n While LLMs demonstrate substantial potential for enhancing clinical workflows, current models require careful implementation with robust safety protocols. No single model provides comprehensive clinical utility, suggesting multi-platform strategies may be necessary. Critical safety gaps, particularly in medical product recommendations and patient data continuity, demand immediate attention before widespread clinical deployment.\n", "citations": 0}
{"title": "Harm Reduction Strategies for Thoughtful Use of Large Language Models in the Medical Domain: Perspectives for Patients and Clinicians", "year": 2025, "authors": "Birger Mo\u00ebll, Fredrik Sand Aronsson", "url": "https://www.semanticscholar.org/paper/0c59358867b5707be57924eeea3bed5ba1a9129c", "relevance": 1, "abstract": "Abstract The integration of large language models (LLMs) into health care presents significant risks to patients and clinicians, inadequately addressed by current guidance. This paper adapts harm reduction principles from public health to medical LLMs, proposing a structured framework for mitigating these domain-specific risks while maximizing ethical utility. We outline tailored strategies for patients, emphasizing critical health literacy and output verification, and for clinicians, enforcing \u201chuman-in-the-loop\u201d validation and bias-aware workflows. Key innovations include developing thoughtful use protocols that position LLMs as assistive tools requiring mandatory verification, establishing actionable institutional policies with risk-stratified deployment guidelines and patient disclaimers, and critically analyzing underaddressed regulatory, equity, and safety challenges. This research moves beyond theory to offer a practical roadmap, enabling stakeholders to ethically harness LLMs, balance innovation with accountability, and preserve core medical values: patient safety, equity, and trust in high-stakes health care settings.", "citations": 4}
{"title": "Tiered Agentic Oversight: A Hierarchical Multi-Agent System for Healthcare Safety", "year": 2025, "authors": "Y. Kim, H. Jeong, Chanwoo Park, Eugene Park, Haipeng Zhang, Xin Liu, Hyeonhoon Lee, D. McDuff, Marzyeh Ghassemi, Cynthia Breazeal, S. Tulebaev, Hae Won Park", "url": "https://www.semanticscholar.org/paper/a233f3c034f6203a5a2853cf87c8538037e8eb6e", "relevance": 1, "abstract": "Large language models (LLMs) deployed as agents introduce significant safety risks in clinical settings due to their potential for error and single points of failure. We introduce Tiered Agentic Oversight (TAO), a hierarchical multi-agent system that enhances AI safety through layered, automated supervision. Inspired by clinical hierarchies (e.g., nurse-physician-specialist) in hospital, TAO routes tasks to specialized agents based on complexity, creating a robust safety framework through automated inter- and intra-tier communication and role-playing. Crucially, this hierarchical structure functions as an effective error-correction mechanism, absorbing up to 24% of individual agent errors before they can compound. Our experiments reveal TAO outperforms single-agent and other multi-agent systems on 4 out of 5 healthcare safety benchmarks, with up to an 8.2% improvement. Ablation studies confirm key design principles of the system: (i) its adaptive architecture is over 3% safer than static, single-tier configurations, and (ii) its lower tiers are indispensable, as their removal causes the most significant degradation in overall safety. Finally, we validated the system's synergy with human doctors in a user study where a physician, acting as the highest tier agent, provided corrective feedback that improved medical triage accuracy from 40% to 60%. Project Page: https://tiered-agentic-oversight.github.io/", "citations": 0}
