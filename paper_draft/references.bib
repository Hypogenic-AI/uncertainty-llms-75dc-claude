@inproceedings{kuhn2023semantic,
    title={Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation},
    author={Kuhn, Lorenz and Gal, Yarin and Farquhar, Sebastian},
    booktitle={Proceedings of the International Conference on Learning Representations (ICLR)},
    year={2023}
}

@article{farquhar2024detecting,
    title={Detecting Hallucinations in Large Language Models Using Semantic Entropy},
    author={Farquhar, Sebastian and Kossen, Jannik and Kuhn, Lorenz and Gal, Yarin},
    journal={Nature},
    volume={630},
    pages={625--630},
    year={2024}
}

@article{wu2024uncertainty,
    title={Uncertainty Estimation of Large Language Models in Medical Question Answering},
    author={Wu, Yijie and others},
    journal={arXiv preprint arXiv:2407.08662},
    year={2024}
}

@inproceedings{umapathi2023medhalt,
    title={{Med-HALT}: Medical Domain Hallucination Test for Large Language Models},
    author={Umapathi, Logesh Kumar and Pal, Ankit and Sankarasubbu, Malaikannan},
    booktitle={Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)},
    year={2023}
}

@article{ness2024medfuzz,
    title={{MedFuzz}: Exploring the Robustness of Large Language Models in Medical Question Answering},
    author={Ness, Robert and others},
    journal={Microsoft Research Technical Report},
    year={2024}
}

@inproceedings{zhao2025afice,
    title={{AFICE}: Aligning for Faithful Integrity with Confidence Estimation},
    author={Zhao, Liang and others},
    booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
    year={2025}
}

@article{wang2024wordsequence,
    title={Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form Medical Question Answering Applications and Beyond},
    author={Wang, Zhiyuan and others},
    journal={Engineering Applications of Artificial Intelligence},
    volume={150},
    year={2025}
}

@inproceedings{tian2023just,
    title={Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Including Large Language Models},
    author={Tian, Katherine and others},
    booktitle={Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    year={2023}
}

@article{xiong2023can,
    title={Can {LLM}s Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in {LLM}s},
    author={Xiong, Miao and others},
    journal={arXiv preprint arXiv:2306.13063},
    year={2023}
}

@article{kadavath2022language,
    title={Language Models (Mostly) Know What They Know},
    author={Kadavath, Saurav and others},
    journal={arXiv preprint arXiv:2207.05221},
    year={2022}
}

@inproceedings{lin2022teaching,
    title={Teaching Models to Express Their Uncertainty in Words},
    author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
    booktitle={Transactions on Machine Learning Research (TMLR)},
    year={2022}
}

@inproceedings{wang2024conformal,
    title={Don't Hallucinate, Abstain: Identifying {LLM} Knowledge Gaps via Multi-{LLM} Collaboration},
    author={Wang, Jinhao and others},
    booktitle={Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    year={2024}
}

@article{zeng2024fragile,
    title={The Uncertainty in {LLM}s is Fragile},
    author={Zeng, Zhisheng and others},
    journal={arXiv preprint arXiv:2407.15729},
    year={2024}
}

@article{jin2021medqa,
    title={What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams},
    author={Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
    journal={Applied Sciences},
    volume={11},
    number={14},
    pages={6421},
    year={2021}
}

@article{geng2023survey,
    title={A Survey of Language Model Confidence Estimation and Calibration},
    author={Geng, Jiahui and others},
    journal={arXiv preprint arXiv:2311.08298},
    year={2023}
}

@article{chen2024inside,
    title={{INSIDE}: {LLM}'s Internal States Retain the Power of Hallucination Detection},
    author={Chen, Yiwei and others},
    journal={arXiv preprint arXiv:2402.03744},
    year={2024}
}

@article{fadeeva2023lmpolygraph,
    title={{LM-Polygraph}: Uncertainty Estimation for Language Models},
    author={Fadeeva, Ekaterina and others},
    journal={arXiv preprint arXiv:2311.07383},
    year={2023}
}

@article{kapoor2024teaching,
    title={Models That Can Be Taught to Express Uncertainty},
    author={Kapoor, Sayash and others},
    journal={arXiv preprint arXiv:2410.09747},
    year={2024}
}
